import sys
import os
import re
import time
import random
import shutil
import pyodbc
import fnmatch
import datetime
import pythoncom
import pywintypes
import pandas as pd
import win32com.client as com
from win32com.client import gencache

from decimal import Decimal
from dotenv import load_dotenv
from contextlib import redirect_stdout









def set_dpi_awareness():
    try:
        from ctypes import windll
        windll.shcore.SetProcessDpiAwareness(1)
    except ImportError:
        print("ctypes ëª¨ë“ˆì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. Python í™˜ê²½ì„ í™•ì¸í•˜ì„¸ìš”.")
    except AttributeError:
        print("SetProcessDpiAwareness í•¨ìˆ˜ê°€ ì§€ì›ë˜ì§€ ì•ŠëŠ” í™˜ê²½ì…ë‹ˆë‹¤. Windows 8.1 ì´ìƒì—ì„œë§Œ ì§€ì›ë©ë‹ˆë‹¤.")
    except OSError as os_error:
        print(f"OS ê´€ë ¨ ì˜¤ë¥˜ ë°œìƒ: {os_error}")
    except Exception as e:
        print(f"ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")
set_dpi_awareness()
sys.path.append(os.path.dirname(os.path.abspath(__file__)))









# ============================================================================ [ í˜„ì¬ ì¼ì‹œ ì„¤ì • ë° ì „ë‚  ì‹œê°„ ê³„ì‚° ]

# í˜„ì¬ ì‹œê°„ì„ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜¤ëŠ” ë¶€ë¶„
current_datetime = datetime.datetime.now()

# >> ì•„ë˜ ë³€ìˆ˜ëŠ” í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ìˆ˜ë™ìœ¼ë¡œ í˜„ì¬ì‹œê°„ì„ ì§€ì •í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤. í˜„ì¬ ì‹œê°ì„ ìˆ˜ë™ ì§€ì •í•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.
current_datetime = datetime.datetime.strptime("2025070202", "%Y%m%d%H")

# ì „ë‚  ë‚ ì§œ ê³„ì‚°
target_date = (current_datetime - datetime.timedelta(days=1)).strftime("%Y%m%d")
peak_hours = ['08', '11', '14', '17']
target_stat_hours = [f"{target_date}{hour}" for hour in peak_hours]  # ["2025070108", "2025070111", "2025070114", "2025070117"]

# ì¡°íšŒëœ ë°ì´í„°ë¥¼ ë‹´ì„ ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
traffic_data_08 = []
traffic_data_11 = []
traffic_data_14 = []
traffic_data_17 = []









# ============================================================================ [ ì „ì—­ìƒíƒœì„¤ì • - DBì ‘ì†ì •ë³´ & ë„¤íŠ¸ì›Œí¬ ]

class Config:
    
    def __init__(self):
        load_dotenv(dotenv_path="C:/Digital Twin Simulation Program/.env")
        self.env = os.getenv("FLASK_ENV", "production")
        
        self.db_config = {
            "test": {
                "driver": "Tibero 5 ODBC Driver",
                "server": os.getenv("ENZERO_SERVER"),
                "port": os.getenv("ENZERO_PORT"),
                "db": os.getenv("ENZERO_DB"),
                "uid": os.getenv("ENZERO_UID"),
                "pwd": os.getenv("ENZERO_PWD")
            },
            "prod": {
                "dsn": os.getenv("DSNNAME"),
                "uid": os.getenv("DBUSER"),
                "pwd": os.getenv("DBPWD")
            }
        }

        self.vissim_paths = self._load_vissim_paths()

    # ê²½í¬, ì†¡ì •ë™, ë„ì‹¬, êµë™ ë„¤íŠ¸ì›Œí¬ ê²½ë¡œ
    
    def _load_vissim_paths(self):
        base_path = r"C:\Digital Twin Simulation Network\VISSIM"
        file_list = [
            "gyeongpo.inpx",
            "songjung.inpx",
            "downtown.inpx",
            "gyodong.inpx"
        ]
        return {
            os.path.splitext(name)[0]: os.path.join(base_path, name)
            for name in file_list
        }

# ============================================================================ [ DB ì—°ê²° - êµí†µëŸ‰ ì¡°íšŒ ]

FIFTEEN_MINUTES = ["00", "15", "30", "45"]
INTERVAL_LABEL = {  # í‚¤(mm) -> êµ¬ê°„ ì˜ë¯¸
    "00": "45~00ë¶„",  # hh00 ì€ ì§ì „ 45~ì •ê°
    "15": "00~15ë¶„",
    "30": "15~30ë¶„",
    "45": "30~45ë¶„",
}

class DatabaseManager:
    def __init__(self, config):
        self.config = config
        self.conn = self._connect()
        self.cursor = self.conn.cursor() if self.conn else None

        # ì‹œê°„ëŒ€ë³„ ì§‘ê³„(HOUR)
        self.traffic_data_by_hour = {"08": [], "11": [], "14": [], "17": []}

        # 15ë¶„ ì§‘ê³„(HH -> MM -> list)
        self.traffic_data_by_15min = {
            "08": {"00": [], "15": [], "30": [], "45": []},
            "11": {"00": [], "15": [], "30": [], "45": []},
            "14": {"00": [], "15": [], "30": [], "45": []},
            "17": {"00": [], "15": [], "30": [], "45": []},
        }

    def _connect(self):
        try:
            if self.config.env == "test":
                db = self.config.db_config["test"]
                print(">>>>> âœ… ì—”ì œë¡œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°")
                return pyodbc.connect(
                    f"DRIVER={db['driver']};"
                    f"SERVER={db['server']};"
                    f"PORT={db['port']};"
                    f"DB={db['db']};"
                    f"UID={db['uid']};"
                    f"PWD={db['pwd']};"
                )
            else:
                db = self.config.db_config["prod"]
                print(">>>>> âœ… ê°•ë¦‰ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°")
                return pyodbc.connect(
                    f"DSN={db['dsn']};"
                    f"UID={db['uid']};"
                    f"PWD={db['pwd']};"
                )
        except Exception as e:
            print("â›” DB ì—°ê²° ì‹¤íŒ¨:", e)
            return None

    # cursor.description ê¸°ë°˜ ì•ˆì „ ë§¤í•‘
    def _rows_to_dicts(self, rows):
        cols = [d[0] for d in self.cursor.description]
        out = []
        for r in rows:
            d = {}
            for c, v in zip(cols, r):
                d[c] = float(v) if isinstance(v, Decimal) else v
            out.append(d)
        return out

    @staticmethod
    def _expand_to_15min_timestamps(target_stat_hours):
        """
        input:  ['YYYYMMDD08','YYYYMMDD11',...]
        output: ['YYYYMMDD0800','YYYYMMDD0815','YYYYMMDD0830','YYYYMMDD0845', ...]
        """
        return [f"{hh}{mm}" for hh in target_stat_hours for mm in FIFTEEN_MINUTES]

    def fetch_peak_traffic_data(self, target_stat_hours):
        """
        target_stat_hours: ['YYYYMMDD08','YYYYMMDD11','YYYYMMDD14','YYYYMMDD17']
        - STAT_HOUR_CROSS  : STAT_HOUR, CROSS_ID, VOL, VOL_01..VOL_24
        - STAT_15MIN_CROSS : STAT_15MIN, CROSS_ID, VOL, VOL_01..VOL_24
        ì ì¬ ëŒ€ìƒ:
        - self.traffic_data_by_hour[HH]     # HH in {"08","11","14","17"}
        - self.traffic_data_by_15min[HH][MM]  # MM in {"00","15","30","45"}
        """
        if not self.cursor:
            print(">>> DB ì»¤ì„œê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        # ê³µí†µ ì»¬ëŸ¼ ì„¸íŠ¸
        vol_cols = ["VOL"] + [f"VOL_{i:02d}" for i in range(1, 25)]

        try:
            # ---------------- (A) ì‹œê°„ëŒ€(HH) ë°ì´í„° ----------------
            hour_cols = ["STAT_HOUR", "CROSS_ID"] + vol_cols
            hour_col_str = ", ".join(hour_cols)

            placeholders = ", ".join(["?"] * len(target_stat_hours))
            sql_hour = f"""
                SELECT {hour_col_str}
                FROM TOMMS.STAT_HOUR_CROSS
                WHERE STAT_HOUR IN ({placeholders})
                AND INFRA_TYPE = 'SMT'
            """
            self.cursor.execute(sql_hour, target_stat_hours)
            hour_rows = self.cursor.fetchall()
            hour_dicts = self._rows_to_dicts(hour_rows)

            # ì ì¬
            for row in hour_dicts:
                # í•„ìˆ˜ í‚¤ ì¡´ì¬ í™•ì¸
                if "STAT_HOUR" not in row or "CROSS_ID" not in row:
                    continue
                hh = str(row["STAT_HOUR"])[-2:]  # '08','11','14','17'
                if hh in self.traffic_data_by_hour:
                    self.traffic_data_by_hour[hh].append(row)

            # ---------------- (B) 15ë¶„(HHMM) ë°ì´í„° ----------------
            min_cols = ["STAT_15MIN", "CROSS_ID"] + vol_cols
            min_col_str = ", ".join(min_cols)

            target_stat_15mins = [f"{hh}{mm}" for hh in target_stat_hours for mm in FIFTEEN_MINUTES]
            placeholders_15 = ", ".join(["?"] * len(target_stat_15mins))
            sql_15 = f"""
                SELECT {min_col_str}
                FROM TOMMS.STAT_15MIN_CROSS
                WHERE STAT_15MIN IN ({placeholders_15})
                AND INFRA_TYPE = 'SMT'
            """
            self.cursor.execute(sql_15, target_stat_15mins)
            min_rows = self.cursor.fetchall()
            min_dicts = self._rows_to_dicts(min_rows)

            for row in min_dicts:
                if "STAT_15MIN" not in row or "CROSS_ID" not in row:
                    continue
                ts = str(row["STAT_15MIN"])   # 'YYYYMMDDHHMM'
                hh, mm = ts[8:10], ts[10:12]  # HH, MM
                if hh in self.traffic_data_by_15min and mm in self.traffic_data_by_15min[hh]:
                    self.traffic_data_by_15min[hh][mm].append(row)

            # ---------------- (C) ê²€ì¦ & ë¡œê·¸ ----------------
            print(f"âœ… [ì‹œê°„ëŒ€ ë°ì´í„°] ì´ {len(hour_dicts)}ê±´")
            for hh in ["08", "11", "14", "17"]:
                print(f"       {hh}ì‹œ: {len(self.traffic_data_by_hour[hh])}ê±´")

            print(f"âœ… [15ë¶„ ë°ì´í„°] ì´ {len(min_dicts)}ê±´")
            for hh in ["08", "11", "14", "17"]:
                counts = {mm: len(self.traffic_data_by_15min[hh][mm]) for mm in FIFTEEN_MINUTES}
                total = sum(counts.values())
                print(f"       {hh}ì‹œ ì´ {total}ê±´")
                print(f"            - {INTERVAL_LABEL['15']} (í‚¤=15): {counts['15']}ê±´")
                print(f"            - {INTERVAL_LABEL['30']} (í‚¤=30): {counts['30']}ê±´")
                print(f"            - {INTERVAL_LABEL['45']} (í‚¤=45): {counts['45']}ê±´")
                print(f"            - {INTERVAL_LABEL['00']} (í‚¤=00): {counts['00']}ê±´")

        except Exception as e:
            print("â›” êµí†µëŸ‰ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜:", e)

# ============================================================================ [ DB ì—°ê²° - êµì°¨ë¡œ ë°©í–¥ë³„ movement ì¡°íšŒ ]

class NodeDirectionManager:

    def __init__(self, config: Config):
        self.config = config
        self.conn = self._connect()
        self.cursor = self.conn.cursor() if self.conn else None

    def _connect(self):
        try:
            if self.config.env == "test":
                db = self.config.db_config["test"]
                return pyodbc.connect(
                    f"DRIVER={db['driver']};"
                    f"SERVER={db['server']};"
                    f"PORT={db['port']};"
                    f"DB={db['db']};"
                    f"UID={db['uid']};"
                    f"PWD={db['pwd']};"
                )
            else:
                db = self.config.db_config["prod"]
                return pyodbc.connect(
                    f"DSN={db['dsn']};"
                    f"UID={db['uid']};"
                    f"PWD={db['pwd']};"
                )
        except Exception as e:
            print("â›” DB ì—°ê²° ì‹¤íŒ¨:", e)
            return None

    def fetch_node_dir_info(self):
        if not self.cursor:
            print("â›” DB ì»¤ì„œê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return pd.DataFrame()

        try:
            query = """
                SELECT
                    NI.CROSS_ID,
                    DI.NODE_ID,
                    DI.APPR_ID,
                    DI.MOVEMENT,
                    DI.DIRECTION
                FROM TOMMS.TFA_NODE_DIR_INFO DI
                JOIN TOMMS.TFA_NODE_INFO     NI
                ON NI.NODE_ID = DI.NODE_ID
                -- í•„ìš” ì‹œ ì •ë ¬
                ORDER BY DI.NODE_ID, DI.APPR_ID, DI.DIRECTION
            """
            self.cursor.execute(query)
            rows = self.cursor.fetchall()

            cleaned_rows = []
            for row in rows:
                out = []
                for val in row:
                    # Tibero/pyodbc Decimal â†’ int ë³€í™˜ (ì •ìˆ˜ ì»¬ëŸ¼ë§Œ)
                    if isinstance(val, Decimal):
                        out.append(int(val))
                    else:
                        out.append(val)
                cleaned_rows.append(tuple(out))

            df = pd.DataFrame(
                cleaned_rows,
                columns=["CROSS_ID", "NODE_ID", "APPR_ID", "MOVEMENT", "DIRECTION"]
            )

            print(f"âœ… NODE_DIR_INFO ì¡°íšŒ ì™„ë£Œ - {len(df)}ê±´")
            return df

        except Exception as e:
            print("â›” NODE_DIR_INFO ì¡°íšŒ ì‹¤íŒ¨:", e)
            return pd.DataFrame()

# ============================================================================ [ DB ì—°ê²° - êµ¬ê°„ ì •ë³´ ì¡°íšŒ ]

class VTTMInfoManager:

    def __init__(self, config: Config):
        self.config = config
        self.conn = self._connect()
        self.cursor = self.conn.cursor() if self.conn else None

    def _connect(self):
        try:
            if self.config.env == "test":
                db = self.config.db_config["test"]
                return pyodbc.connect(
                    f"DRIVER={db['driver']};"
                    f"SERVER={db['server']};"
                    f"PORT={db['port']};"
                    f"DB={db['db']};"
                    f"UID={db['uid']};"
                    f"PWD={db['pwd']};"
                )
            else:
                db = self.config.db_config["prod"]
                return pyodbc.connect(
                    f"DSN={db['dsn']};"
                    f"UID={db['uid']};"
                    f"PWD={db['pwd']};"
                )
        except Exception as e:
            print("â›” DB ì—°ê²° ì‹¤íŒ¨:", e)
            return None

    def fetch_vttm_info(self):
        if not self.cursor:
            print("â›” DB ì»¤ì„œê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return pd.DataFrame()

        try:
            query = """
                SELECT VTTM_ID, FROM_NODE_NAME, TO_NODE_NAME
                FROM TOMMS.TFA_VTTM_INFO
            """
            self.cursor.execute(query)
            rows = self.cursor.fetchall()

            cleaned_rows = []
            for row in rows:
                cleaned_row = []
                for val in row:
                    if isinstance(val, Decimal):
                        cleaned_row.append(int(val))
                    else:
                        cleaned_row.append(val)
                cleaned_rows.append(tuple(cleaned_row))

            df = pd.DataFrame(cleaned_rows, columns=["VTTM_ID", "FROM_NODE_NAME", "TO_NODE_NAME"])
            print(f"âœ… VTTM_INFO ì¡°íšŒ ì™„ë£Œ - {len(df)}ê±´")
            return df

        except Exception as e:
            print("â›” VTTM_INFO ì¡°íšŒ ì‹¤íŒ¨:", e)
            return pd.DataFrame()









# ============================================================================ [ êµ¬ê°„ ê²°ê³¼ê°’ DB INSERT ]

def insert_vttm_results_to_db(df_vttm, db_manager):
    
    if db_manager.cursor is None:
        print("â›” DB ì»¤ì„œê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    insert_query = """
        INSERT INTO TOMMS.TFA_VTTM_HOUR_RESULT (
            STAT_HOUR, VTTM_ID, DISTANCE, VEHS, TRAVEL_TIME
        ) VALUES (?, ?, ?, ?, ?)
    """

    # NaNì„ Noneìœ¼ë¡œ ëŒ€ì²´, íƒ€ì… í˜•ë³€í™˜
    def clean_value(val, target_type):
        if pd.isna(val):
            return None
        try:
            if target_type == "int":
                return int(val)
            elif target_type == "float":
                return float(val)
            elif target_type == "str":
                return str(val)
        except:
            return None

    insert_data = []
    for _, row in df_vttm.iterrows():
        insert_data.append((
            clean_value(row.get("STAT_HOUR"), "str"),
            clean_value(row.get("VTTM_ID"), "str"),
            clean_value(row.get("DISTANCE"), "float"),
            clean_value(row.get("VEHS"), "int"),
            clean_value(row.get("TRAVEL_TIME"), "float")
        ))

    try:
        db_manager.cursor.executemany(insert_query, insert_data)
        db_manager.conn.commit()
        print(f"âœ… VTTM_RESULTì— {len(insert_data)}ê±´ ì‚½ì… ì™„ë£Œ")
    except Exception as e:
        print("â›” DB ì‚½ì… ì¤‘ ì˜¤ë¥˜:", e)
        db_manager.conn.rollback()

# ============================================================================ [ êµì°¨ë¡œ ê²°ê³¼ê°’ DB INSERT ]

def insert_node_results_to_db(df_node: pd.DataFrame, db_manager):
    if db_manager.cursor is None:
        print("â›” DB ì»¤ì„œê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    # --- 1) í´ë¦°ì—…: íƒ€ì…/ê³µë°± ì •ë¦¬ ---
    df = df_node.copy()
    for c in ["STAT_HOUR", "TIMEINT", "NODE_ID"]:
        if c in df.columns:
            df[c] = df[c].astype(str).str.strip()

    # í•„ìˆ˜ í‚¤ ëˆ„ë½ í–‰ ì œê±°
    df = df[df["NODE_ID"].notna() & (df["NODE_ID"] != "")]

    # ìˆ˜ì¹˜ ì»¬ëŸ¼ ì •ë¦¬
    def to_float(x):
        try:
            return float(x)
        except Exception:
            return None
    def to_int(x):
        try:
            return int(float(x))
        except Exception:
            return None

    if "QLEN"  in df.columns: df["QLEN"]  = df["QLEN"].map(to_float)
    if "DELAY" in df.columns: df["DELAY"] = df["DELAY"].map(to_float)
    if "STOPS" in df.columns: df["STOPS"] = df["STOPS"].map(to_float)
    if "VEHS"  in df.columns: df["VEHS"]  = df["VEHS"].map(to_int)

    # --- 2) ë¶€ëª¨í‚¤ ì¡´ì¬í•  ë•Œë§Œ INSERT (FK ì•ˆì „) ---
    insert_sql = """
        INSERT INTO TOMMS.TFA_NODE_15MIN_RESULT
            (STAT_HOUR, TIMEINT, NODE_ID, QLEN, VEHS, DELAY, STOPS)
        SELECT ?, ?, ?, ?, ?, ?, ?
        FROM DUAL
        WHERE EXISTS (
            SELECT 1 FROM TOMMS.TFA_NODE_INFO p
            WHERE p.NODE_ID = ?
        )
    """

    params = []
    for _, r in df.iterrows():
        params.append((
            r.get("STAT_HOUR"),
            r.get("TIMEINT"),
            r.get("NODE_ID"),
            r.get("QLEN"),
            r.get("VEHS"),
            r.get("DELAY"),
            r.get("STOPS"),
            r.get("NODE_ID"),  # EXISTS ê²€ì¦ìš©
        ))

    try:
        db_manager.cursor.fast_executemany = True  # pyodbc ì„±ëŠ¥ ì˜µì…˜
        db_manager.cursor.executemany(insert_sql, params)
        db_manager.conn.commit()
        print(f"âœ… NODE_RESULT ì‚½ì… ì‹œë„ {len(params)}ê±´ ì™„ë£Œ (ë¶€ëª¨í‚¤ ìˆëŠ” í–‰ë§Œ ì‹¤ì œ ì‚½ì…)")
    except Exception as e:
        print("â›” NODE_RESULT ì‚½ì… ì˜¤ë¥˜:", e)
        db_manager.conn.rollback()

# ============================================================================ [ êµì°¨ë¡œ ë°©í–¥ë³„ ê²°ê³¼ê°’ DB INSERT ]

def insert_node_dir_results_to_db(df_dir_node: pd.DataFrame, db_manager):
    
    if db_manager.cursor is None:
        print("â›” DB ì»¤ì„œê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    insert_query = """
        INSERT INTO TOMMS.TFA_NODE_DIR_15MIN_RESULT (
            STAT_HOUR, TIMEINT, NODE_ID, APPR_ID, DIRECTION, QLEN, VEHS, DELAY, STOPS
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
    """

    def clean_value(val, target_type):
        if pd.isna(val):
            return None
        try:
            if target_type == "int":
                return int(val)
            elif target_type == "float":
                return float(val)
            elif target_type == "str":
                return str(val)
        except:
            return None

    insert_data = []
    for _, row in df_dir_node.iterrows():
        insert_data.append((
            clean_value(row.get("STAT_HOUR"), "str"),
            clean_value(row.get("TIMEINT"), "str"),
            clean_value(row.get("NODE_ID"), "str"),
            clean_value(row.get("APPR_ID"), "int"),
            clean_value(row.get("DIRECTION"), "int"),
            clean_value(row.get("QLEN"), "float"),
            clean_value(row.get("VEHS"), "int"),
            clean_value(row.get("DELAY"), "float"),
            clean_value(row.get("STOPS"), "float")
        ))

    try:
        db_manager.cursor.executemany(insert_query, insert_data)
        db_manager.conn.commit()
        print(f"âœ… NODE_DIR_RESULTì— {len(insert_data)}ê±´ ì‚½ì… ì™„ë£Œ")
    except Exception as e:
        print("â›” NODE_DIR_RESULT ì‚½ì… ì˜¤ë¥˜:", e)
        db_manager.conn.rollback()

# ============================================================================ [ Data Collection ê²°ê³¼ê°’ DB INSERT - í†µí–‰ì‹œê°„ ì¦‰ì„ ê³„ì‚° ]

def insert_dc_to_db(dc: pd.DataFrame, db_manager):
    if db_manager.cursor is None:
        print("â›” DB ì»¤ì„œê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    insert_query = """
        INSERT INTO TOMMS.TFA_DC_HOUR_RESULT (
            STAT_HOUR, DC_ID, DISTANCE, VEHS, SPEED, TRAVEL_TIME
        ) VALUES (?, ?, ?, ?, ?, ?)
    """

    def clean_value(val, target_type):
        if pd.isna(val):
            return None
        try:
            if target_type == "int":
                return int(val)
            elif target_type == "float":
                return float(val)
            elif target_type == "str":
                return str(val)
        except:
            return None

    def compute_travel_time_seconds(row) -> float:
        """
        TRAVEL_TIME[s] = DISTANCE(m) * 3.6 / SPEED(km/h)
        SPEEDê°€ 0ì´ê±°ë‚˜ ê²°ì¸¡ì´ë©´ 0.0 ë°˜í™˜
        """
        # ì›ë˜ ê°’ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜ì˜
        tt = row.get("TRAVEL_TIME")
        if pd.notna(tt):
            try:
                return round(float(tt), 1)
            except:
                pass

        d = row.get("DISTANCE")
        v = row.get("SPEED")
        try:
            d = float(d) if pd.notna(d) else None
            v = float(v) if pd.notna(v) else None
            if d is None or v is None or v <= 0:
                return 0.0   # â† SPEED=0, ê²°ì¸¡ ì‹œ 0.0 ê°•ì œ
            return round(d * 3.6 / v, 1)
        except:
            return 0.0

    insert_data = []
    for _, row in dc.iterrows():
        insert_data.append((
            clean_value(row.get("STAT_HOUR"), "str"),
            clean_value(row.get("DC_ID"), "int"),
            clean_value(row.get("DISTANCE"), "float"),
            clean_value(row.get("VEHS"), "int"),
            clean_value(row.get("SPEED"), "float"),
            compute_travel_time_seconds(row)
        ))

    try:
        db_manager.cursor.executemany(insert_query, insert_data)
        db_manager.conn.commit()
        print(f"âœ… DC_RESULTì— {len(insert_data)}ê±´ ì‚½ì… ì™„ë£Œ")
    except Exception as e:
        print("â›” DC_RESULT ì‚½ì… ì˜¤ë¥˜:", e)
        db_manager.conn.rollback()

# ============================================================================ [ Network Performance ê²°ê³¼ê°’ DB INSERT ]

def insert_np_to_db(np: pd.DataFrame, db_manager):
    
    if db_manager.cursor is None:
        print("â›” DB ì»¤ì„œê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    insert_query = """
            INSERT INTO TOMMS.TFA_DISTRICT_HOUR_RESULT (
            DISTRICT_ID, STAT_HOUR, VEHS, COST
            ) VALUES (?, ?, ?, ?)
    """
    
    def clean_value(val, target_type):
        if pd.isna(val):
            return None
        try:
            if target_type == "int":
                return int(val)
            elif target_type == "float":
                return float(val)
            elif target_type == "str":
                return str(val)
        except:
            return None
    
    insert_data = []
    for _, row in np.iterrows():
        insert_data.append((
            clean_value(row.get("DISTRICT_ID"), "int"),
            clean_value(row.get("STAT_HOUR"), "str"),
            clean_value(row.get("VEHS"), "int"),
            clean_value(row.get("COST"), "float")
        ))

    try:
        db_manager.cursor.executemany(insert_query, insert_data)
        db_manager.conn.commit()
        print(f"âœ… NP_RESULTì— {len(insert_data)}ê±´ ì‚½ì… ì™„ë£Œ")
    except Exception as e:
        print("â›” NP_RESULT ì‚½ì… ì˜¤ë¥˜:", e)
        db_manager.conn.rollback()






# ============================================================================ [ ì‹œë®¬ë ˆì´ì…˜ ì»¨íŠ¸ë¡¤ëŸ¬ ]

class VissimSimulationManager:
    
    def __init__(self, config: Config, db_manager: DatabaseManager):
        self.vissim = None
        self.paths = config.vissim_paths
        self.db = db_manager
        self._com_initialized = False
    
    # --- VISSIM COM ê°ì²´ë¥¼ "ì„±ê³µí•  ë•Œê¹Œì§€" ìƒì„±í•˜ëŠ” ìœ í‹¸
    
    def _init_com(self):
        """STAë¡œ COM ì´ˆê¸°í™” (ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œí•´ë„ ì•ˆì „í•˜ë„ë¡ ê°€ë“œ)"""
        if not self._com_initialized:
            # COINIT_APARTMENTTHREADED = STA
            pythoncom.CoInitializeEx(pythoncom.COINIT_APARTMENTTHREADED)
            self._com_initialized = True
            print("ğŸ”§ COM initialized (STA)")

    def _uninit_com(self):
        """COM í•´ì œ (ì°¸ì¡° ì¹´ìš´íŠ¸ ë§ì¶”ê¸°)"""
        if self._com_initialized:
            pythoncom.CoUninitialize()
            self._com_initialized = False
            print("ğŸ§¹ COM uninitialized")
    
    def _ensure_vissim(self,
                    prog_ids=("Vissim.Vissim.22",),
                    max_attempts=8,
                    base_delay=1.5,
                    hard_timeout_sec=90) -> bool:
        self._init_com()
        start = time.time()
        last_err = None

        for attempt in range(1, max_attempts + 1):
            if time.time() - start > hard_timeout_sec:
                print(f"â›” í•˜ë“œ íƒ€ì„ì•„ì›ƒ ì´ˆê³¼({hard_timeout_sec}s)")
                break

            for prog in prog_ids:
                try:
                    # 1) Dispatch
                    self.vissim = com.Dispatch(prog)
                    print(f"ğŸ”µ VISSIM COM ìƒì„± ì„±ê³µ: {prog} (attempt={attempt})")
                    return True
                except pywintypes.com_error as e1:
                    last_err = e1
                    # 2) EnsureDispatch í´ë°±
                    try:
                        self.vissim = gencache.EnsureDispatch(prog)
                        print(f"ğŸ”µ VISSIM COM ìƒì„± ì„±ê³µ(EnsureDispatch): {prog} (attempt={attempt})")
                        return True
                    except Exception as e2:
                        last_err = e2

            sleep_s = min(base_delay * (2 ** (attempt - 1)), 10.0) + random.uniform(0.0, 0.5)
            print(f"ğŸ” ì¬ì‹œë„ ëŒ€ê¸°: {sleep_s:.1f}s (attempt={attempt}/{max_attempts})")
            time.sleep(sleep_s)

        print(f"â›” ìµœì¢… ì‹¤íŒ¨: {repr(last_err)}")
        self.vissim = None
        return False

    # ============================================================================ [ ì—°ê³„ - ì‹¤í–‰ - ì¶”ì¶œ - ì €ì¥ - ì¢…ë£Œ ]

    def run_full_simulation(self, area):
        global target_stat_hours

        print(f"ğŸ”µ vissim ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ê±´ë„¤ë°›ì€ ë¶„ì„ëŒ€ìƒ ì¼ì‹œ : {target_date}")
        print(f"ğŸ”µ vissim ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ê±´ë„¤ë°›ì€ ë¶„ì„ëŒ€ìƒ ì§€êµ¬ : {area}")
        path = self.paths.get(area)

        if not path or not os.path.isfile(path):
            print(f"â›” [ ê²½ê³  ] {area} íŒŒì¼ ì—†ìŒ: {path}")
            return

        # âœ… ë°˜ë“œì‹œ ìƒì„±ë  ë•Œê¹Œì§€ ì‹œë„
        if not self._ensure_vissim(prog_ids=("Vissim.Vissim.22",), max_attempts=8, base_delay=1.5, hard_timeout_sec=90):
            print("â›” VISSIM ê°ì²´ë¥¼ ìƒì„±í•˜ì§€ ëª»í•´ ì‹œë®¬ë ˆì´ì…˜ ìŠ¤í‚µ")
            self._uninit_com()
            return

        try:
            # ------------------------------------------------------------ ë°˜ë³µëœ ì‹œë®¬ë ˆì´ì…˜ ë£¨í”„
            for idx, (hour_key, traffic_hour_list) in enumerate(self.db.traffic_data_by_hour.items()):
                # hour_key ì˜ˆ: "08","11","14","17"
                try:
                    idx = peak_hours.index(hour_key)
                    full_stat_hour = target_stat_hours[idx]  # "YYYYMMDDHH"
                except ValueError:
                    print(f"â›” [ ì˜¤ë¥˜ ] ì‹œê°„ëŒ€ {hour_key}ëŠ” peak_hoursì— ì—†ìŠµë‹ˆë‹¤.")
                    continue

                # 15ë¶„ ë°ì´í„°(ì—†ìœ¼ë©´ ë¹ˆ êµ¬ì¡°ë¡œ ëŒ€ì²´)
                traffic_15min_list = self.db.traffic_data_by_15min.get(
                    hour_key,
                    {"00": [], "15": [], "30": [], "45": []}
                )

                # ë¡œê·¸(ìš”ì•½)
                hh_total = len(traffic_hour_list)
                mm_counts = {mm: len(traffic_15min_list.get(mm, [])) for mm in ["00", "15", "30", "45"]}
                mm_total = sum(mm_counts.values())
                print(f"ğŸ”µ [ {area} ] ( {full_stat_hour} ) ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘ ===")
                print(f"    â”œâ”€ ì‹œê°„ëŒ€ ë°ì´í„°: {hh_total}ê±´")
                print(f"    â””â”€ 15ë¶„ ë°ì´í„°: ì´ {mm_total}ê±´ / 00:{mm_counts['00']} 15:{mm_counts['15']} 30:{mm_counts['30']} 45:{mm_counts['45']}")

                # [1] ì´ì „ ê²°ê³¼ ì‚­ì œ
                self.cleanup_att_files(area)

                # [2] ë„¤íŠ¸ì›Œí¬ íŒŒì¼ ë‹¤ì‹œ ë¡œë“œ (ìµœëŒ€ 3íšŒ ì†Œí”„íŠ¸ ì¬ì‹œë„)
                for load_try in range(1, 4):
                    try:
                        self.vissim.LoadNet(path, False)
                        print(f"ğŸ” [ ë„¤íŠ¸ì›Œí¬ ì¬ë¡œë“œ ì™„ë£Œ ] {area} â†’ {path} (try={load_try})")
                        break
                    except pywintypes.com_error as e:
                        print(f"âš ï¸ ì¬ë¡œë“œ ì‹¤íŒ¨(try={load_try}): {repr(e)}")
                        time.sleep(1.0 * load_try)
                else:
                    print(f"â›” [ ì˜¤ë¥˜ ] ë„¤íŠ¸ì›Œí¬ ì¬ë¡œë“œ ë°˜ë³µ ì‹¤íŒ¨: {path}")
                    continue

                # [3] ì—°ê³„ â†’ ì‹¤í–‰ â†’ ì¶”ì¶œ
                # âœ… ë³€ê²½: ì‹œê°„ëŒ€/15ë¶„ ë°ì´í„°ë¥¼ í•¨ê»˜ ì „ë‹¬
                self.apply_traffic_data(traffic_hour_list, traffic_15min_list)
                self.run_simulation()
                df_node, df_dir_node, df_vttm, dc, np = self.extract_results(stat_hour=full_stat_hour, area_name=area)

                # [5] DB ì €ì¥
                self.save_results((df_dir_node, df_node, df_vttm, dc, np), area, hour_key)

                # [6] ê²°ê³¼ íŒŒì¼ ì‚­ì œ
                self.cleanup_att_files(area)

        finally:
            # ------------------------------------------------------------ ì¢…ë£Œ
            self.close_simulation()
            self._uninit_com()  # âœ… COM í•´ì œ

    # ============================================================================ [ ì—°ê³„ - vehicle input / static route ]

    def apply_traffic_data(self, traffic_hour_list, traffic_15min_list):
        """
        traffic_hour_list: ì‹œê°„ëŒ€(HH) ë°ì´í„° list[dict]
        traffic_15min_list: {"00": [...], "15": [...], "30": [...], "45": [...]}
            - "15" ë¦¬ìŠ¤íŠ¸ì˜ ê° rowëŠ” '00~15' êµ¬ê°„ì„ ì˜ë¯¸
            - "30" â†’ 15~30, "45" â†’ 30~45, "00" â†’ 45~00
        """

        # -----------------------------
        # 0) ì‚¬ì „ ì¸ë±ì‹± (ì„±ëŠ¥ & ë‹¨ìˆœí™”)
        # -----------------------------
        def _row_to_volume_map(row):
            # VOL_XX â†’ int ë¡œë§Œ êµ¬ì„± (None ì œì™¸)
            return {
                key.replace("VOL_", ""): int(val)
                for key, val in row.items()
                if key.startswith("VOL_") and val is not None
            }

        # ì‹œê°„ëŒ€: (cross_id, vol_key) â†’ volume
        hour_map = {}
        for row in traffic_hour_list:
            cross_id = str(row.get("CROSS_ID"))
            vol_map = _row_to_volume_map(row)
            for vol_key, vol in vol_map.items():
                hour_map[(cross_id, vol_key)] = vol

        # 15ë¶„: mmë³„ (cross_id, vol_key) â†’ volume
        mm_maps = {"00": {}, "15": {}, "30": {}, "45": {}}
        for mm in ["00", "15", "30", "45"]:
            rows = traffic_15min_list.get(mm, [])
            m = mm_maps[mm]
            for row in rows:
                cross_id = str(row.get("CROSS_ID"))
                vol_map = _row_to_volume_map(row)
                for vol_key, vol in vol_map.items():
                    m[(cross_id, vol_key)] = vol

        total_vi = 0
        total_route = 0

        print(f"ğŸ”µ [ êµí†µëŸ‰ ì…ë ¥ ì‹œì‘ ] VI={len(traffic_hour_list)}ê±´, 15ë¶„={'/'.join(f'{k}:{len(traffic_15min_list.get(k, []))}' for k in ['15','30','45','00'])}")

        # ------------------------------------------------------------ [ vehicle input êµí†µëŸ‰ ì…ë ¥ ]
        vehicle_input_nos = self.vissim.Net.VehicleInputs.GetMultiAttValues('No')
        vehicle_input_node_ids = self.vissim.Net.VehicleInputs.GetMultiAttValues('Node_ID')
        vehicle_input_link_ids = self.vissim.Net.VehicleInputs.GetMultiAttValues('Link_ID')

        for no, node_id, link_id in zip(vehicle_input_nos, vehicle_input_node_ids, vehicle_input_link_ids):
            if node_id[1] is None or link_id[1] is None:
                continue

            cross_id = str(node_id[1])              # VIëŠ” Node_ID ê¸°ì¤€
            vol_key = f"{int(link_id[1]):02d}"      # Link_ID â†’ "02","03"... ë§¤ì¹­

            vi_vol = hour_map.get((cross_id, vol_key))  # ì‹œê°„ëŒ€ êµí†µëŸ‰ë§Œ ì‚¬ìš©
            if vi_vol is None:
                continue

            vi = self.vissim.Net.VehicleInputs.ItemByKey(no[1])
            # ì‹œê°„ëŒ€ êµí†µëŸ‰ìœ¼ë¡œ 5 ìŠ¬ë¡¯ ë™ì¼ ì…ë ¥(ì› ì½”ë“œ ìœ ì§€)
            vi.SetAttValue('Volume(1)', vi_vol)
            vi.SetAttValue('Volume(2)', vi_vol)
            vi.SetAttValue('Volume(3)', vi_vol)
            vi.SetAttValue('Volume(4)', vi_vol)
            vi.SetAttValue('Volume(5)', vi_vol)
            total_vi += 1

        # ------------------------------------------------------------ [ static route êµí†µëŸ‰ ì…ë ¥ ]
        vrds = self.vissim.Net.VehicleRoutingDecisionsStatic
        num_decisions = self.vissim.Net.VehicleRoutingDecisionsStatic.Count

        # RelFlow ë§¤í•‘ ê·œì¹™
        # (1) = ì‹œê°„ëŒ€, (2)=00~15 â†’ "15", (3)=15~30 â†’ "30", (4)=30~45 â†’ "45", (5)=45~00 â†’ "00"
        mm_for_relflow = {2: "15", 3: "30", 4: "45", 5: "00"}

        for i in range(1, num_decisions + 1):
            if not vrds.ItemKeyExists(i):
                continue
            decision = vrds.ItemByKey(i)

            for route in decision.VehRoutSta.GetAll():
                sr_node_id = route.AttValue('VehRoutDec\\Node_ID')
                sr_turn_id = route.AttValue('Turn_ID')

                if sr_node_id is None or sr_turn_id is None:
                    continue

                cross_id = str(sr_node_id)
                vol_key = f"{int(sr_turn_id):02d}"

                # (1) ì‹œê°„ëŒ€ êµí†µëŸ‰ (RelFlow(1))
                sr_vol_hour = hour_map.get((cross_id, vol_key))
                if sr_vol_hour is None:
                    # ì‹œê°„ëŒ€ ë¶„ê¸° ì—†ìœ¼ë©´ ì „ì²´ ë¼ìš°íŠ¸ ì…ë ¥ ìŠ¤í‚µ
                    # (í•„ìš” ì‹œ 0 ì…ë ¥ìœ¼ë¡œ ìœ ì§€í•˜ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ continueë¥¼ ì œê±°í•˜ê³  0ìœ¼ë¡œ ì„¸íŒ…)
                    continue

                route.SetAttValue("RelFlow(1)", sr_vol_hour)

                # (2~5) 15ë¶„ êµí†µëŸ‰
                for rel_idx in [2, 3, 4, 5]:
                    mm = mm_for_relflow[rel_idx]
                    sr_vol_15m = mm_maps[mm].get((cross_id, vol_key), 0)
                    route.SetAttValue(f"RelFlow({rel_idx})", sr_vol_15m)

                total_route += 1

        print(f"âœ… [ ì…ë ¥ ì™„ë£Œ ] VehicleInputs: {total_vi}ê°œ, StaticRoutes: {total_route}ê°œ")
        
    # ============================================================================ [ ì‹¤í–‰ - simulation run ]

    def run_simulation(self):
        
        End_of_simulation = 4200
        self.vissim.Simulation.SetAttValue('SimPeriod', End_of_simulation) # ì‹œë®¬ë ˆì´ì…˜ 4200ì´ˆ
        self.vissim.Graphics.CurrentNetworkWindow.SetAttValue("QuickMode", 1)
        self.vissim.Simulation.SetAttValue('UseMaxSimSpeed', True)
        self.vissim.Simulation.RunContinuous()

    # ============================================================================ [ ì¶”ì¶œ - node results / vehicle input ]

    def extract_results(self, stat_hour: str, area_name: str):
        
        print(f"ğŸ”µ ë¶„ì„ëŒ€ìƒì¼ì‹œ [ {stat_hour} ] ë¶„ì„ëŒ€ìƒì§€ì—­ [ {area_name} ]") # 2025070108, 2025070111

        target_folder = r"C:\Digital Twin Simulation Network\VISSIM"
        results = {}

        # ------------------------------------------------------------ ì‚­ì œ ëŒ€ìƒ ì œê±° (.results, .err, .lock ë“±)
        
        for file in os.listdir(target_folder):
            full_path = os.path.join(target_folder, file)

            if file.endswith(".err") or file.endswith(".lock"):
                try:
                    os.remove(full_path)
                    print(f"âœ… [ íŒŒì¼ì‚­ì œ ì™„ë£Œ ] : {file}")
                except Exception as e:
                    print(f"â›” [ íŒŒì¼ì‚­ì œ ì‹¤íŒ¨ ]: {file} â†’ {e}")

            elif file.endswith(".results") and os.path.isdir(full_path):
                try:
                    shutil.rmtree(full_path)
                    print(f"âœ… [ í´ë”ì‚­ì œ ì™„ë£Œ ] : {file}")
                except Exception as e:
                    print(f"â›” [ í´ë”ì‚­ì œ ì‹¤íŒ¨ ]: {file} â†’ {e}")

        # ------------------------------------------------------------ íŒŒì¼ ì°¾ê¸°

        def find_latest_index(base_name, result_type):
            pattern = re.compile(rf"{re.escape(base_name)}_{re.escape(result_type)}_(\d+)\.att")
            max_idx = 0
            for file in os.listdir(target_folder):
                m = pattern.match(file)
                if m:
                    max_idx = max(max_idx, int(m.group(1)))
            return f"{max_idx:03d}" if max_idx > 0 else None

        # ------------------------------------------------------------ ê²°ê³¼ê°’ dfë¡œ í• ë‹¹í•˜ê¸°

        def read_att_file(path):
            if not os.path.exists(path):
                print(f"â›” íŒŒì¼ ì—†ìŒ: {path}")
                return pd.DataFrame()

            encodings = ["utf-8-sig", "utf-16", "cp949", "utf-8"]
            for enc in encodings:
                try:
                    with open(path, "r", encoding=enc, errors="strict") as f:
                        lines = f.read().splitlines()
                    break
                except (UnicodeDecodeError, UnicodeError):
                    continue
            else:
                print(f"â›” ì¸ì½”ë”© ì‹¤íŒ¨: {path}")
                return pd.DataFrame()

            # ë³´í†µ '$'ê°€ ë“¤ì–´ê°„ ë¼ì¸ì´ 2ë²ˆ ì´ìƒ ì¡´ì¬, ë‘ ë²ˆì§¸ê°€ í—¤ë”
            dollar_lines = [i for i, line in enumerate(lines) if "$" in line]
            if len(dollar_lines) < 2:
                print(f"â›” í¬ë§· ì´ìƒ(í—¤ë” íƒì§€ ì‹¤íŒ¨): {path}")
                return pd.DataFrame()

            header_idx = dollar_lines[1]
            header_line = lines[header_idx]

            # âœ… í•µì‹¬: '$'ë¶€í„° ì²« ':'ê¹Œì§€ ì œê±° â†’ ì–´ë–¤ í—¤ë” íƒ€ì…ì´ ì™€ë„ ë™ì‘
            # ì˜ˆ) $DATACOLLECTIONMEASUREMENTEVALUATION:SIMRUN;TIMEINT;... â†’ SIMRUN;TIMEINT;...
            header_line = re.sub(r"^\$[^:]*:", "", header_line).strip()

            columns = [c.strip() for c in header_line.split(';') if c.strip()]
            data_lines = lines[header_idx + 1:]

            # ë°ì´í„° ë¶€ë¶„ íŒŒì‹±
            rows = []
            for line in data_lines:
                if not line.strip():
                    continue
                values = [v.strip() for v in line.split(';')]
                # ì—´ ê°œìˆ˜ ë³´ì •
                if len(values) < len(columns):
                    values += [''] * (len(columns) - len(values))
                elif len(values) > len(columns):
                    values = values[:len(columns)]
                rows.append(dict(zip(columns, values)))

            df = pd.DataFrame(rows)
            return df

        # ------------------------------------------------------------ ê° êµ¬ì—­ ê²°ê³¼ê°’ ì²˜ë¦¬

        latest_index = find_latest_index(area_name, "Node Results")
        if not latest_index:
            print(f"â›” {area_name}: ì‹œë®¬ë ˆì´ì…˜ íŒŒì¼ ì—†ìŒ")
            return {}

        # íŒŒì¼ ê²½ë¡œ ì •ì˜
        node_file = os.path.join(target_folder, f"{area_name}_Node Results_{latest_index}.att")
        vttm_file = os.path.join(target_folder, f"{area_name}_Vehicle Travel Time Results_{latest_index}.att")
        dc_file = os.path.join(target_folder, f"{area_name}_Data Collection Results_{latest_index}.att")
        np_file = os.path.join(target_folder, f"{area_name}_Vehicle Network Performance Evaluation Results_{latest_index}.att")

        # íŒŒì¼ ì½ê¸°
        df_dir_node = read_att_file(node_file)
        df_vttm = read_att_file(vttm_file)
        dc = read_att_file(dc_file)
        np = read_att_file(np_file)

        print(f"âœ… {area_name} - Node Results ({df_dir_node.shape[0]}í–‰)")
        print(f"âœ… {area_name} - Travel Time Results ({df_vttm.shape[0]}í–‰)")
        print(f"âœ… {area_name} - Data Collection Results ({dc.shape[0]}í–‰)")
        print(f"âœ… {area_name} - Vehicle Network Performance Evaluation Results ({np.shape[0]}í–‰)")

        # ì»¬ëŸ¼ëª… ë§¤í•‘
        district_map = {
            "gyodong": 1,
            "songjung": 2,
            "downtown": 3,
            "gyeongpo": 4
        }
        timeint_map = {
            '600-1500': '00-15',
            '1500-2400': '15-30',
            '2400-3300': '30-45',
            '3300-4200': '45-00',
        }
        node_col_map = {
            "VEHS(ALL)": "VEHS",
            "VEHDELAY(ALL)": "DELAY",
            "STOPS(ALL)": "STOPS",
            "MOVEMENT\\NODE\\NODE_ID": "NODE_ID",
            "MOVEMENT\\NODE\\SA": "SA_NO"
        }
        vttm_col_map = {
            "VEHICLETRAVELTIMEMEASUREMENT\\NAME": "VTTM_ID",
            "VEHS(ALL)": "VEHS",
            "TRAVTM(ALL)": "TRAVEL_TIME",
            "DISTTRAV(ALL)": "DISTANCE",
            "VEHICLETRAVELTIMEMEASUREMENT\\LINKS_ID": "LINK_ID",
            "VEHICLETRAVELTIMEMEASUREMENT\\UPDOWN": "UPDOWN",
            "VEHICLETRAVELTIMEMEASUREMENT\\SA": "SA_NO",
            "VEHICLETRAVELTIMEMEASUREMENT\\ROAD_NAME": "ROAD_NAME",
            "VEHICLETRAVELTIMEMEASUREMENT\\ACTIVE": "ACTIVE"
        }
        
        data_col_map = {
            "DATACOLLECTIONMEASUREMENT": "DC_ID",
            "DIST(ALL)": "DISTANCE",
            "VEHS(ALL)": "VEHS",
            "SPEEDAVGARITH(ALL)": "SPEED",
        }
        np_col_map = {
            "VEHACT(ALL)": "VEHS",
            "TRAVELCOST": "COST"
        }

        # ì»¬ëŸ¼ëª… ë³€ê²½
        df_dir_node.rename(columns=node_col_map, inplace=True)
        df_vttm.rename(columns=vttm_col_map, inplace=True)
        dc.rename(columns=data_col_map, inplace=True)
        np.rename(columns=np_col_map, inplace=True)

        # ------------------------------------------------------------ êµì°¨ë¡œ & êµì°¨ë¡œ ë°©í–¥ë³„ ê²°ê³¼ê°’ ê°€ê³µ

        # 0) ë§¤í•‘ í…Œì´ë¸” ë¡œë“œ
        node_dir_manager = NodeDirectionManager(config)
        df_node_dir_info = node_dir_manager.fetch_node_dir_info()  # [CROSS_ID, NODE_ID, APPR_ID, MOVEMENT, DIRECTION]

        # 1) ë§¤í•‘ ì¡´ì¬ í™•ì¸
        if df_node_dir_info.empty:
            print("â›” ë°©í–¥ ê¸°ì¤€(ë§¤í•‘) ë°ì´í„°ê°€ ì—†ì–´ ë°©í–¥ë³„ ê²°ê³¼ ìƒì„± ë¶ˆê°€ â†’ ìŠ¤í‚µ")
            df_dir_node = pd.DataFrame()
        else:
            # 2) í‚¤/ì»¬ëŸ¼ ì •ê·œí™”
            df_node_dir_info = df_node_dir_info.copy()
            df_node_dir_info.columns = [c.upper() for c in df_node_dir_info.columns]

            # âœ… í•„ìš”í•œ ì»¬ëŸ¼ì„ **CROSS_ID í¬í•¨**í•´ì„œ ë³´ì¡´ (ì´ì „ ì½”ë“œì˜ ëˆ„ë½ ì§€ì )
            need_cols = ["MOVEMENT", "NODE_ID", "APPR_ID", "DIRECTION", "CROSS_ID"]
            df_node_dir_info = (
                df_node_dir_info[need_cols]
                .drop_duplicates(subset=["MOVEMENT"])  # movementë³„ ìœ ì¼ ë§¤í•‘ ê°€ì •
            )

            # íƒ€ì… ì •ê·œí™” (ë§¤ì¹­ ì‹¤íŒ¨ ë°©ì§€)
            df_node_dir_info["MOVEMENT"] = df_node_dir_info["MOVEMENT"].astype(str).str.strip()
            # NODE_ID: 10ìë¦¬ ë¬¸ìì—´
            df_node_dir_info["NODE_ID"] = (
                df_node_dir_info["NODE_ID"].astype(str).str.strip().str.zfill(10)
            )
            # CROSS_ID: ì •ìˆ˜ (NULL ìˆìœ¼ë©´ NaNâ†’drop ë˜ëŠ” 0 ì²˜ë¦¬ ì„ íƒ)
            df_node_dir_info["CROSS_ID"] = pd.to_numeric(df_node_dir_info["CROSS_ID"], errors="coerce")

            # ì›ë³¸ ê²°ê³¼ í”„ë ˆì„ ì •ê·œí™”
            df_dir_node = df_dir_node.copy()
            if "MOVEMENT" not in df_dir_node.columns:
                raise ValueError("ì›ë³¸ df_dir_nodeì— MOVEMENT ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            df_dir_node["MOVEMENT"] = df_dir_node["MOVEMENT"].astype(str).str.strip()

            # 3) MOVEMENT ê¸°ì¤€ ë³‘í•©  (ì¶©ëŒ íšŒí”¼ ìœ„í•´ suffix ì‚¬ìš©)
            df_dir_node = df_dir_node.merge(
                df_node_dir_info,
                on="MOVEMENT",
                how="left",
                suffixes=("", "_map"),
                validate="m:1"
            )
            print("âœ… MOVEMENT ê¸°ë°˜ìœ¼ë¡œ CROSS_ID, NODE_ID, APPR_ID, DIRECTION ë³‘í•© ì™„ë£Œ")

            # 4) ì¶©ëŒ ì»¬ëŸ¼ ì •ë¦¬(í†µí•©): NODE_ID/APPR_ID/DIRECTION/CROSS_ID
            #    - ì›ë³¸ì— ê°’ ì—†ìœ¼ë©´ *_map ê°’ìœ¼ë¡œ ì±„ì›€
            for col in ["NODE_ID", "APPR_ID", "DIRECTION", "CROSS_ID"]:
                map_col = f"{col}_map"
                if col in df_dir_node.columns and map_col in df_dir_node.columns:
                    df_dir_node[col] = df_dir_node[col].where(df_dir_node[col].notna(), df_dir_node[map_col])
                    df_dir_node.drop(columns=[map_col], inplace=True)
                elif map_col in df_dir_node.columns:
                    df_dir_node.rename(columns={map_col: col}, inplace=True)

            # íƒ€ì… ì¬ê³ ì • (ë³‘í•© í›„)
            df_dir_node["NODE_ID"] = df_dir_node["NODE_ID"].astype(str).str.strip().str.zfill(10)
            df_dir_node["CROSS_ID"] = pd.to_numeric(df_dir_node["CROSS_ID"], errors="coerce")

            # 5) í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ë³´ì¥ + ë§¤í•‘ ì‹¤íŒ¨ ì§„ë‹¨
            required_cols = {"NODE_ID", "APPR_ID", "DIRECTION", "CROSS_ID"}
            missing = required_cols - set(df_dir_node.columns)
            if missing:
                cols = ", ".join(df_dir_node.columns)
                raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}. í˜„ì¬ ì»¬ëŸ¼ë“¤: {cols}")

            null_node_rows = df_dir_node[df_dir_node["NODE_ID"].isna() | (df_dir_node["NODE_ID"].astype(str).str.len() == 0)]
            if not null_node_rows.empty:
                bad_movs = null_node_rows["MOVEMENT"].dropna().unique().tolist()
                print(f"âš ï¸ MOVEMENTâ†’NODE_ID ë§¤í•‘ ì‹¤íŒ¨ ê±´ìˆ˜={len(null_node_rows)} / ì˜ˆì‹œ={bad_movs[:5]}")

            null_cross_rows = df_dir_node[df_dir_node["CROSS_ID"].isna()]
            if not null_cross_rows.empty:
                bad_movs = null_cross_rows["MOVEMENT"].dropna().unique().tolist()
                print(f"âš ï¸ MOVEMENTâ†’CROSS_ID ë§¤í•‘ ì‹¤íŒ¨ ê±´ìˆ˜={len(null_cross_rows)} / ì˜ˆì‹œ={bad_movs[:5]}")

            # 6) ê³µí†µ ê°€ê³µ
            df_dir_node["STAT_HOUR"] = stat_hour
            df_dir_node["TIMEINT"] = df_dir_node["TIMEINT"].map(timeint_map).fillna(df_dir_node["TIMEINT"])
            df_dir_node.drop(columns=[c for c in ["SIMRUN"] if c in df_dir_node.columns], inplace=True)

            # NODE_ID ë¬´ê²°ì„± í™•ë³´
            df_dir_node = df_dir_node[
                df_dir_node["NODE_ID"].notna() &
                (df_dir_node["NODE_ID"].astype(str).str.len() > 0)
            ]

            # 7) êµì°¨ë¡œ / ë°©í–¥ë³„ ë¶„ë¦¬
            #    âœ… ê¸°ì¡´ì˜ "MOVEMENT â†’ CROSS_ID rename"ì€ **ì‚­ì œ**.
            #    ìš°ë¦¬ê°€ ë°©ê¸ˆ ë³‘í•©í•œ **ì§„ì§œ CROSS_ID(ìˆ«ìí˜•)** ë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤.
            df_node = df_dir_node[df_dir_node["MOVEMENT"].apply(lambda x: '@' not in str(x))].copy()
            df_dir_node = df_dir_node[df_dir_node["MOVEMENT"].apply(lambda x: '@' in str(x))].copy()

            # 8) ì»¬ëŸ¼ ì •ë ¬
            base_cols = ["STAT_HOUR", "TIMEINT", "NODE_ID", "SA_NO", "QLEN", "VEHS", "DELAY", "STOPS"]
            dir_extra_cols = ["APPR_ID", "DIRECTION"]
            keep_extra = ["CROSS_ID", "NODE_NAME"]

            if not df_node.empty:
                # node ì§‘ê³„ì—ëŠ” CROSS_ID/NODE_NAMEì´ í•„ìš” ì—†ìœ¼ë©´ ì œê±°
                cols_node = [c for c in base_cols if c in df_node.columns]
                df_node = df_node[cols_node]

            cols_dir = [c for c in (base_cols + dir_extra_cols + keep_extra) if c in df_dir_node.columns]
            df_dir_node = df_dir_node[cols_dir]

            # 9) ë°©í–¥ë³„ ì¬ê°€ê³µ
            df_dir_node = df_dir_node[df_dir_node["APPR_ID"].notna() & (df_dir_node["APPR_ID"] != "")]
            for c in ["QLEN", "DELAY", "STOPS", "VEHS"]:
                if c in df_dir_node.columns:
                    df_dir_node[c] = pd.to_numeric(df_dir_node[c], errors="coerce")

            group_cols = ["STAT_HOUR", "TIMEINT", "NODE_ID", "CROSS_ID", "NODE_NAME", "SA_NO", "APPR_ID", "DIRECTION"]
            have_cols = [c for c in group_cols if c in df_dir_node.columns]
            df_dir_node = (
                df_dir_node
                .groupby(have_cols, as_index=False)
                .agg({"QLEN": "mean", "DELAY": "mean", "STOPS": "mean", "VEHS": "sum"})
            )
            for c in ["QLEN", "DELAY", "STOPS"]:
                if c in df_dir_node.columns:
                    df_dir_node[c] = df_dir_node[c].round(2)

            # 10) INSERT ì§ì „ ìŠ¤í‚¤ë§ˆë¡œ ìŠ¬ë¼ì´ì‹± (CROSS_IDê°€ í•„ìš” ì—†ìœ¼ë©´ ì œì™¸)
            df_dir_node = df_dir_node[["STAT_HOUR","TIMEINT","NODE_ID","APPR_ID","DIRECTION","QLEN","VEHS","DELAY","STOPS"]]
        
        # ------------------------------------------------------------ êµ¬ê°„ ê²°ê³¼ê°’ ê°€ê³µ
        
        # êµ¬ê°„ë¶„ì„ì—ì„œ í™œì„±í™”ëœ êµ¬ê°„ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì‚­ì œ
        df_vttm = df_vttm[df_vttm["ACTIVE"] == str(1)].copy()
        
        df_vttm["STAT_HOUR"] = stat_hour
        
        # í•„ìš” ì—†ëŠ” ì»¬ëŸ¼ ì œê±°
        df_vttm.drop(columns=[col for col in ["SIMRUN", "VEHICLETRAVELTIMEMEASUREMENT", "TIMEINT"] if col in df_vttm.columns], inplace=True)

        # # VTTM_INFO ì¡°íšŒ ë° ë³‘í•©
        vttm_info_manager = VTTMInfoManager(config)
        df_vttm_info = vttm_info_manager.fetch_vttm_info()

        if not df_vttm_info.empty:
            df_vttm = df_vttm.merge(df_vttm_info, on="VTTM_ID", how="left")
            print("âœ… êµ¬ê°„ ë…¸ë“œ ì •ë³´ ë³‘í•© ì™„ë£Œ")
        else:
            print("ğŸ”µ êµ¬ê°„ ë…¸ë“œ ì •ë³´ ë³‘í•© ìŠ¤í‚µ (ë°ì´í„° ì—†ìŒ)")

        # ì»¬ëŸ¼ ì •ë ¬
        # ê¶Œì—­, ë¶„ì„ëŒ€ìƒì¼ì, ë¶„ì„ëŒ€ìƒì‹œê°„, êµ¬ê°„ì•„ì´ë””, ì‹œì êµì°¨ë¡œëª…, ì¢…ì êµì°¨ë¡œëª…, ìƒí•˜í–‰êµ¬ë¶„, ê±°ë¦¬(m), í†µí–‰ëŸ‰, ì‹œê°„(ì´ˆ), SAë²ˆí˜¸, ëŒ€ë¡œëª…, í™œì„±í™”ì—¬ë¶€
        desired_vttm_cols = ["STAT_HOUR", "TIMEINT", "VTTM_ID", "FROM_NODE_NAME", "TO_NODE_NAME", "UPDOWN", "DISTANCE", "VEHS", "TRAVEL_TIME", "SA_NO", "ROAD_NAME", "ACTIVE"]
        df_vttm = df_vttm[[col for col in desired_vttm_cols if col in df_vttm.columns]]
        
        # ------------------------------------------------------------ Data Collection ì»¬ëŸ¼ ì œê±°
        district_code = district_map.get(area)
        
        dc.drop(columns=[c for c in ["DATACOLLECTIONMEASUREMENTEVALUATION:SIMRUN", "TIMEINT"] if c in dc.columns], inplace=True)
        dc["STAT_HOUR"] = stat_hour
        dc["DISTRICT_ID"] = district_code
        
        # ------------------------------------------------------------ Network Performance ì»¬ëŸ¼ ì œê±°
        # DISTRICT, STAT_HOUR, VEHS, COST
        
        np.drop(columns=[c for c in ["VEHICLENETWORKPERFORMANCEMEASUREMENTEVALUATION:SIMRUN", "TIMEINT"] if c in np.columns], inplace=True)
        np["STAT_HOUR"] = stat_hour
        np["DISTRICT_ID"] = district_code
        np.drop(columns=["SIMRUN"], errors="ignore", inplace=True)
        
        # ------------------------------------------------------------ êµì°¨ë¡œ ë°©í–¥ë³„ ê²°ê³¼ê°’ / êµì°¨ë¡œ ê²°ê³¼ê°’ ì—‘ì…€ ì €ì¥
        
        output_dir = os.path.join(target_folder, "results_csv")
        os.makedirs(output_dir, exist_ok=True)

        df_node.to_csv(os.path.join(output_dir, f"{area_name}_{stat_hour}_node.csv"), index=False, encoding="utf-8-sig")
        df_dir_node.to_csv(os.path.join(output_dir, f"{area_name}_{stat_hour}_dir_node.csv"), index=False, encoding="utf-8-sig")
        df_vttm.to_csv(os.path.join(output_dir, f"{area_name}_{stat_hour}_vttm.csv"), index=False, encoding="utf-8-sig")
        dc.to_csv(os.path.join(output_dir, f"{area_name}_{stat_hour}_dc.csv"), index=False, encoding="utf-8-sig")
        np.to_csv(os.path.join(output_dir, f"{area_name}_{stat_hour}_np.csv"), index=False, encoding="utf-8-sig")

        print(f"âœ… CSV ì €ì¥ ì™„ë£Œ â†’ {output_dir}")

        return df_node.copy(), df_dir_node.copy(), df_vttm.copy(), dc.copy(), np.copy()

    # ============================================================================ [ ì €ì¥ ]

    def save_results(self, result, area_name, hour_key):

        df_dir_node, df_node, df_vttm, dc, np = result

        insert_vttm_results_to_db(df_vttm, self.db) # êµ¬ê°„ ê²°ê³¼ê°’ DB INSERT
        insert_node_results_to_db(df_node, self.db) # êµì°¨ë¡œ ê²°ê³¼ê°’ DB INSERT
        insert_node_dir_results_to_db(df_dir_node, self.db) # êµì°¨ë¡œ ë°©í–¥ë³„ ê²°ê³¼ê°’ DB INSERT
        insert_dc_to_db(dc, self.db) # êµì°¨ë¡œ ë°©í–¥ë³„ ê²°ê³¼ê°’ DB INSERT
        insert_np_to_db(np, self.db) # êµì°¨ë¡œ ë°©í–¥ë³„ ê²°ê³¼ê°’ DB INSERT
        
        print(f"âœ… [ DB ì €ì¥ ì™„ë£Œ ] {area_name}-{hour_key} ê²°ê³¼ DB ì €ì¥ ë˜ëŠ” íŒŒì¼ ê¸°ë¡")

    # ============================================================================ [ ì¢…ë£Œ ]

    def close_simulation(self):
        print("âœ… [ ì‹œë®¬ë ˆì´ì…˜ ì¢…ë£Œ ]")
        self.vissim = None

    # ============================================================================ [ ê²°ê³¼ê°’ íŒŒì¼ë“¤ ì „ë¶€ ì‚­ì œ ]

    def cleanup_att_files(self, area):
        import fnmatch
        import os

        target_folder = r"C:\Digital Twin Simulation Network\VISSIM"
        patterns = [
            f"{area}_Node Results_*.att",
            f"{area}_Vehicle Travel Time Results_*.att",
            f"{area}_Vehicle Network Performance Evaluation Results_*.att",
            f"{area}_Data Collection Results_*.att"
        ]

        deleted = 0
        for pattern in patterns:
            for file in os.listdir(target_folder):
                if fnmatch.fnmatch(file, pattern):
                    try:
                        os.remove(os.path.join(target_folder, file))
                        print(f"ğŸ§¹ [ ì‚­ì œ ì™„ë£Œ ] {file}")
                        deleted += 1
                    except Exception as e:
                        print(f"â›” [ ì‚­ì œ ì‹¤íŒ¨ ] {file} â†’ {e}")

        if deleted == 0:
            print("âš ï¸ ì‚­ì œí•  att íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")









# ============================================================================ [ main ì‹¤í–‰ ]

class Tee:
    """stdoutì„ íŒŒì¼ê³¼ ì½˜ì†”ì— ë™ì‹œì— ì¶œë ¥"""
    def __init__(self, *streams):
        self.streams = streams
    def write(self, data):
        for s in self.streams:
            s.write(data)
            s.flush()
    def flush(self):
        for s in self.streams:
            s.flush()

if __name__ == "__main__":
    
    # ------------------------------------------------------------ ë¡œê·¸ í´ë” ë° íŒŒì¼ëª… ì§€ì •
    log_folder = "logs"
    os.makedirs(log_folder, exist_ok=True)

    start_time = datetime.datetime.now()
    log_filename = start_time.strftime("%Y%m%d_%H%M%S_vissim_simulation.log")
    log_path = os.path.join(log_folder, log_filename)

    # ------------------------------------------------------------ ë¡œê·¸íŒŒì¼ + ì½˜ì†” ë™ì‹œì— ì¶œë ¥
    with open(log_path, "w", encoding="utf-8") as log_file:
        sys.stdout = Tee(sys.__stdout__, log_file)  # ì½˜ì†”(stdout) + ë¡œê·¸íŒŒì¼ ë™ì‹œ ì¶œë ¥

        print("ğŸŸ¢ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘")
        print(f"â–¶ï¸ ì‹œì‘ ì‹œê°„: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)

        # ------------------------------------------------------------ ì‹¤ì œ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ ì½”ë“œ
        config = Config()
        db = DatabaseManager(config)
        vissim_manager = VissimSimulationManager(config, db)
        db.fetch_peak_traffic_data(target_stat_hours)
        area_list = ["gyodong", "gyeongpo", "downtown", "songjung"]  # 1, 4, 3, 2

        for area in area_list:
            vissim_manager.run_full_simulation(area)

        end_time = datetime.datetime.now()
        duration = end_time - start_time
        print("=" * 60)
        print("ğŸ”´ ì‹œë®¬ë ˆì´ì…˜ ì¢…ë£Œ")
        print(f"â¹ï¸ ì¢…ë£Œ ì‹œê°„: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ•’ ì´ ì†Œìš” ì‹œê°„: {str(duration).split('.')[0]} (HH:MM:SS)")

    print(f"âœ… ë¡œê·¸ ì €ì¥ ì™„ë£Œ â†’ {log_path}")