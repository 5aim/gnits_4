import sys
import os
import re
import time
import shutil
import pyodbc
import fnmatch
import datetime
import pywintypes
import pandas as pd
import win32com.client as com
import numpy as np

from pprint import pprint
from decimal import Decimal
from dotenv import load_dotenv
from contextlib import redirect_stdout
from collections import defaultdict
from typing import Dict, List, Tuple, Optional








# ================================================== [ DPI ì„¤ì • ]

def set_dpi_awareness() -> None:
    try:
        from ctypes import windll
        windll.shcore.SetProcessDpiAwareness(1)
    except ImportError:
        print(" â›” ctypes ëª¨ë“ˆì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. Python í™˜ê²½ì„ í™•ì¸í•˜ì„¸ìš”.")
    except AttributeError:
        print(" â›” SetProcessDpiAwareness í•¨ìˆ˜ê°€ ì§€ì›ë˜ì§€ ì•ŠëŠ” í™˜ê²½ì…ë‹ˆë‹¤. Windows 8.1 ì´ìƒì—ì„œë§Œ ì§€ì›ë©ë‹ˆë‹¤.")
    except OSError as os_error:
        print(f" â›” OS ê´€ë ¨ ì˜¤ë¥˜ ë°œìƒ: {os_error}")
    except Exception as e:
        print(f" â›” ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")

set_dpi_awareness()
sys.path.append(os.path.dirname(os.path.abspath(__file__)))








# ================================================== [ ê³µìš© ìœ í‹¸ ]

# Decimal â†’ int/float ë³€í™˜, ë‚˜ë¨¸ì§€ëŠ” ê·¸ëŒ€ë¡œ.
def to_py(val):
    if isinstance(val, Decimal):
        return int(val) if val == int(val) else float(val)
    return val

# í˜•ì‹ ê²€ì‚¬: YYYYMMDDHH (10ìë¦¬ ìˆ«ì)
def is_valid_stat_hour(s: str) -> bool:
    return bool(re.fullmatch(r"\d{10}", s))

# VOL_00 ~ VOL_23 ê°™ì€ ì»¬ëŸ¼ì„ ì´ë¦„ ê¸°ì¤€ ì •ë ¬í•œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜. ë°˜í™˜ í˜•íƒœ: [(ì»¬ëŸ¼ëª…, ì»¬ëŸ¼ì¸ë±ìŠ¤), ...]
def extract_vol_columns(cols: List[str]) -> List[Tuple[str, int]]:
    # ì¸ë±ìŠ¤ëŠ” ì™¸ë¶€ì—ì„œ ë§¤í•‘ëœ dict ë¡œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë¯€ë¡œ, ì—¬ê¸°ì„  ì´ë¦„ë§Œ ì •ë ¬.
    vol_names = sorted([c for c in cols if re.fullmatch(r"VOL_\d{2}", c)])
    return vol_names

def to_db_py(v):
    # None/NaN ì²˜ë¦¬
    if v is None:
        return None
    # pandasì˜ NAë¥˜ë„ Noneìœ¼ë¡œ
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass

    # numpy ìŠ¤ì¹¼ë¼ â†’ íŒŒì´ì¬ ìŠ¤ì¹¼ë¼
    if isinstance(v, (np.integer,)):
        return int(v)
    if isinstance(v, (np.floating,)):
        return float(v)
    if isinstance(v, (np.bool_,)):
        return bool(v)

    # Decimal â†’ int/float
    if isinstance(v, Decimal):
        return int(v) if v == int(v) else float(v)

    # ê·¸ ì™¸ëŠ” ì›ë³¸ ìœ ì§€(ë¬¸ìì—´ ë“±)
    return v





# ================================================== [ í˜„ì¬ ì¼ì‹œ/íƒ€ê¹ƒ ì‹œê°„ ê³„ì‚° ]

def compute_target_hours(now: Optional[datetime.datetime] = None,
                        pick_hours: List[str] = None) -> Tuple[str, List[str]]:
    
    """
    ê¸°ì¤€ì‹œê°(now) ê¸°ì¤€ ì „ë‚  ë‚ ì§œ(YYYYMMDD)ì™€, ì „ë‚ ì˜ íŠ¹ì • HH ë¦¬ìŠ¤íŠ¸(YYYYMMDDHH)ë¥¼ ëŒë ¤ì¤Œ.
    """
    
    if now is None:
        now = datetime.datetime.now()
    if pick_hours is None:
        pick_hours = ["08", "11", "14", "17"]

    target_date = (now - datetime.timedelta(days=1)).strftime("%Y%m%d")
    target_stat_hours = [f"{target_date}{h}" for h in pick_hours]
    print(f">>>>> âœ… ë°ì´í„° ì¡°íšŒ ê¸°ì¤€ ì‹œê°„ : {target_date}")
    return target_date, target_stat_hours







# ================================================== [ í™˜ê²½ì„¤ì • ë° DB ì ‘ì†ì •ë³´ ë¡œë”© ]

class Config:

    def __init__(self):
        load_dotenv(dotenv_path="C:/Digital Twin Simulation Program/.env")
        raw_env = (os.getenv("FLASK_ENV", "prod") or "").lower()
        self.env = "prod" if raw_env in ("prod", "production") else "test"

        self.db_config = {
            "test": {
                "driver": "Tibero 5 ODBC Driver",
                "server": os.getenv("ENZERO_SERVER"),
                "port": os.getenv("ENZERO_PORT"),
                "db": os.getenv("ENZERO_DB"),
                "uid": os.getenv("ENZERO_UID"),
                "pwd": os.getenv("ENZERO_PWD"),
            },
            "prod": {
                "dsn": os.getenv("DSNNAME"),
                "uid": os.getenv("DBUSER"),
                "pwd": os.getenv("DBPWD"),
            },
        }








# ================================================== [ DB ]

class DatabaseManager:
    """
    Tibero ê¸°ë°˜ ì‹œê°„ëŒ€/ì¼ë³„ êµí†µëŸ‰ ì¡°íšŒ ë§¤ë‹ˆì €
    """
    def __init__(self, config: Config):
        self.config = config
        self.conn = self._connect()
        self.cursor = self.conn.cursor() if self.conn else None

        # ì‚¬ìš©í•  ì»¬ëŸ¼ë“¤(ì°¸ê³ ìš©)
        self.columns = [
            "STAT_HOUR", "CROSS_ID",
            "VOL_00","VOL_01","VOL_02","VOL_03","VOL_04","VOL_05",
            "VOL_06","VOL_07","VOL_08","VOL_09","VOL_10","VOL_11",
            "VOL_12","VOL_13","VOL_14","VOL_15","VOL_16","VOL_17",
            "VOL_18","VOL_19","VOL_20","VOL_21","VOL_22","VOL_23",
        ]

    def _connect(self):
        try:
            if self.config.env == "test":
                db = self.config.db_config["test"]
                print(">>>>> âœ… ì—”ì œë¡œ DB ì„œë²„ ì—°ê²°. í…ŒìŠ¤íŠ¸ ë²„ì „ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
                return pyodbc.connect(
                    f"DRIVER={db['driver']};SERVER={db['server']};PORT={db['port']};"
                    f"DB={db['db']};UID={db['uid']};PWD={db['pwd']};"
                )
            else:
                db = self.config.db_config["prod"]
                print(">>>>> âœ… ê°•ë¦‰ì‹œ í‹°ë² ë¡œ DB ì„œë²„ ì—°ê²°. ë°°í¬ ë²„ì „ì…ë‹ˆë‹¤.")
                return pyodbc.connect(f"DSN={db['dsn']};UID={db['uid']};PWD={db['pwd']}")
        except Exception as e:
            print("â›” DB ì—°ê²° ì‹¤íŒ¨:", e)
            return None

    def _exec(self, sql: str, params: Tuple = ()) -> Tuple[List[tuple], List[str]]:
        """
        ê³µìš© ì¿¼ë¦¬ ì‹¤í–‰. rows(tuple list)ì™€ cols(list[str]) ë°˜í™˜.
        """
        if not self.cursor:
            raise RuntimeError("DB ì»¤ë„¥ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
        self.cursor.execute(sql, params)
        rows = [tuple(r) for r in self.cursor.fetchall()]
        cols = [col[0] for col in self.cursor.description]
        return rows, cols

    def fetch_and_process_data(self, target_stat_hours: List[str]) -> Tuple[Dict[str, List[dict]], Dict[str, List[dict]], str]:
        """
        ì…ë ¥: ë™ì¼ ë‚ ì§œì˜ STAT_HOUR(YYYYMMDDHH) ë¦¬ìŠ¤íŠ¸
        ì²˜ë¦¬: ì‹œê°„ëŒ€ë³„ ì¡°íšŒ â†’ ì¼ë³„(í•˜ë£¨) ì¡°íšŒ
        ë°˜í™˜: (traffic_data_by_hour, traffic_data_by_day, query_day)
        """

        # ---------- 0) ì‹œê°„ íŒŒë¼ë¯¸í„° ì •ê·œí™” ----------
        def _clean_hour(x: str) -> str:
            return (x or "").strip().strip("'\"")

        hours: List[str] = sorted({h for h in (_clean_hour(h) for h in target_stat_hours) if is_valid_stat_hour(h)})
        print(f">>>>> âœ… target_stat_hours(clean): {hours}")
        if not hours:
            print("â›” ìœ íš¨í•œ STAT_HOURê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {f"{h:02d}": [] for h in range(24)}, {}, None  # type: ignore[return-value]

        # ---------- 1) ì¡°íšŒ ì¼ì í™•ì • ----------
        day: str = hours[0][:8]
        if not re.fullmatch(r"\d{8}", day):
            raise ValueError(f"STAT_DAY í˜•ì‹ ì˜¤ë¥˜: {repr(day)}")
        print(f">>>>> âœ… ì¡°íšŒ ì¼ì: {day}")

        # ---------- 2) ì‹œê°„ëŒ€ë³„ ì¡°íšŒ ----------
        ph_hours = ", ".join(["?"] * len(hours))
        sql_hour = f"""
            SELECT *
            FROM TOMMS.STAT_HOUR_CROSS
            WHERE STAT_HOUR IN ({ph_hours})
            AND TRIM(UPPER(INFRA_TYPE)) = 'SMT'
        """
        print(f"ğŸ” sql_hour params: {[repr(h) for h in hours]}")
        rows_hour, cols_hour = self._exec(sql_hour, tuple(hours))
        print(f">>>>> âœ… ì‹œê°„ëŒ€ë³„ ì¡°íšŒ í–‰ìˆ˜: {len(rows_hour)}")

        # ì»¬ëŸ¼ ì¸ë±ìŠ¤ ë§¤í•‘(ì‹œê°„)
        col_idx_h = {c: i for i, c in enumerate(cols_hour)}
        idx_stat_hour = col_idx_h["STAT_HOUR"]
        idx_cross_id_h = col_idx_h["CROSS_ID"]

        vol_names_h = extract_vol_columns(cols_hour)
        vol_idx_pairs_h = [(name, col_idx_h[name]) for name in vol_names_h]

        traffic_data_by_hour: Dict[str, List[dict]] = {f"{h:02d}": [] for h in range(24)}
        for r in rows_hour:
            stat_hour = str(r[idx_stat_hour]).strip()
            hh = stat_hour[-2:]
            if hh in traffic_data_by_hour:
                cross_id = str(r[idx_cross_id_h]).strip()
                traffic_data_by_hour[hh].append({
                    "cross_id": cross_id,
                    "data": [
                        {"direction": name, "value": to_py(r[idx])}
                        for name, idx in vol_idx_pairs_h
                        if to_py(r[idx]) is not None
                    ]
                })
        print(">>>>> âœ… ì‹œê°„ëŒ€ë³„ ê°€ê³µ ì™„ë£Œ.")

        # ---------- 3) ì¼ë³„ ì¡°íšŒ (ê²€ì¦ëœ ë¦¬í„°ëŸ´, fallback ì—†ìŒ) ----------
        traffic_data_by_day: Dict[str, List[dict]] = {day: []}

        sql_day = f"""
            SELECT *
            FROM TOMMS.STAT_DAY_CROSS
            WHERE STAT_DAY = '{day}'
            AND TRIM(UPPER(INFRA_TYPE)) = 'SMT'
        """
        print(f"ğŸ” sql_day(literal): {sql_day.strip()}")
        rows_day, cols_day = self._exec(sql_day)
        print(f">>>>> âœ… ì¼ë³„ ì¡°íšŒ í–‰ìˆ˜: {len(rows_day)}")

        if rows_day:
            col_idx_d = {c: i for i, c in enumerate(cols_day)}
            idx_cross_id_d = col_idx_d["CROSS_ID"]
            vol_names_d = extract_vol_columns(cols_day)
            vol_idx_pairs_d = [(name, col_idx_d[name]) for name in vol_names_d]

            for r in rows_day:
                cross_id = str(r[idx_cross_id_d]).strip()
                traffic_data_by_day[day].append({
                    "cross_id": cross_id,
                    "data": [
                        {"direction": name, "value": to_py(r[idx])}
                        for name, idx in vol_idx_pairs_d
                        if to_py(r[idx]) is not None
                    ]
                })
        else:
            # ì§„ë‹¨ë§Œ ë‚¨ê¹€
            cnt_sql = f"""
                SELECT COUNT(*) AS CNT
                FROM TOMMS.STAT_DAY_CROSS
                WHERE STAT_DAY = '{day}'
                AND TRIM(UPPER(INFRA_TYPE)) = 'SMT'
            """
            cnt_rows, _ = self._exec(cnt_sql)
            cnt = cnt_rows[0][0] if cnt_rows else 0
            print(f"â›” ì¼ë³„ 0ê±´(ì¹´ìš´íŠ¸={cnt}). fallback ë¯¸ì‚¬ìš©.")

        # ---------- 4) ìš”ì•½ ë° ë°˜í™˜ ----------
        total_day = len(traffic_data_by_day.get(day, []))
        print(">>>>> âœ… ì „ì¼ êµí†µëŸ‰ ê°€ê³µ ì™„ë£Œ.")
        print(f"      âœ… ì¼ë³„ ì´ {total_day}ê±´ / ì¼ìˆ˜=1")
        for hh, lst in traffic_data_by_hour.items():
            if lst:
                print(f"      âœ… ì‹œê°„ëŒ€ {hh}ì‹œ : {len(lst)}ê±´")

        return traffic_data_by_hour, traffic_data_by_day, day

    def close(self):
        try:
            if self.cursor:
                self.cursor.close()
                self.cursor = None
            if self.conn:
                self.conn.close()
                self.conn = None
            print(">>>>> âœ… DB ì—°ê²°ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        except Exception as e:
            print("â›” DB ì¢…ë£Œ ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤. ", e)








# ================================================== [ VISUM ]

class VisumSimulationManager:
    
    # --------------------------------------------------------------- [ ì‹œê°„ëŒ€ë³„ Active ë²ˆí˜¸ (ê³ ì • ì‹œë‚˜ë¦¬ì˜¤) ]
    
    HOUR_TO_PROC = {
        8: 312,
        11: 408,
        14: 505,
        17: 601,
    }

    # --------------------------------------------------------------- [ init ]

    def __init__(
        self,
        base_path: str,
        default_version_name: str,
        prev_day_proc_no: int = 22,
        csv_out_dir: str = r"C:/Digital Twin Simulation network/VISUM/result_export",
        table_map: dict = None,     # {"prev_day": "TABLE_A", "hourly": "TABLE_B"}
    ):
        self.base_path = base_path
        self.default_version_name = default_version_name
        self.prev_day_proc_no = prev_day_proc_no
        self.csv_out_dir = csv_out_dir
        os.makedirs(self.csv_out_dir, exist_ok=True)

        self.table_map = table_map or {
            "prev_day": "LINK_RESULT_DAY",
            "hourly": "LINK_RESULT_HOUR",
        }

        self.visum = None
        # last_run: {"type": "prev_day"|"hourly"|None, "hour": "00"~"23"|None, "stat_day": "YYYYMMDD"|None}
        self.last_run = {"type": None, "hour": None, "stat_day": None}











    # --------------------------------------------------------------- [ ê¸°ì¤€ì¼ ê´€ë¦¬ ]
    
    def set_stat_day(self, yyyymmdd: str):
        if not (isinstance(yyyymmdd, str) and len(yyyymmdd) == 8 and yyyymmdd.isdigit()):
            raise ValueError("STAT_DAY must be 'YYYYMMDD' string.")
        self.last_run["stat_day"] = yyyymmdd
        print(f">>>>> âœ… VISUM set_stat_day í•¨ìˆ˜ì—ì„œ ê±´ë„¤ë°›ì€ ì¼ìëŠ” [ {self.last_run} ] ì…ë‹ˆë‹¤.")

    def ensure_stat_day(self, preferred_day: str | None, payload_days: list[str] | None = None) -> str:
        """
        ê¸°ì¤€ì¼ì„ ë‹¨ í•œ ë²ˆë§Œ í™•ì •:
        1) preferred_dayê°€ ìœ íš¨í•˜ë©´ ê·¸ê±¸ ì‚¬ìš©
        2) ì—†ìœ¼ë©´ payload_daysì—ì„œ ìµœì‹ ì¼ìë¥¼ ì„ íƒ
        ì´í›„ self.last_run['stat_day']ë¥¼ ì„¤ì •í•˜ê³ , ì´í›„ì—ëŠ” ì ˆëŒ€ ë°”ê¾¸ì§€ ì•ŠìŒ
        """
        if self.last_run.get("stat_day"):
            return self.last_run["stat_day"]

        cand = None
        if isinstance(preferred_day, str) and len(preferred_day) == 8 and preferred_day.isdigit():
            cand = preferred_day
        elif payload_days:
            # payload_daysê°€ ë¬¸ìì—´ 'YYYYMMDD' ë¦¬ìŠ¤íŠ¸ë¼ê³  ê°€ì •
            cand = max([d for d in payload_days if isinstance(d, str) and len(d) == 8 and d.isdigit()], default=None)

        if not cand:
            raise ValueError("â›” ê¸°ì¤€ì¼(STAT_DAY)ì„ í™•ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. preferred_day ë˜ëŠ” payload_daysë¥¼ ì œê³µí•˜ì„¸ìš”.")

        self.last_run["stat_day"] = cand
        return cand

    def _require_stat_day(self) -> str:
        sd = self.last_run.get("stat_day")
        if not (isinstance(sd, str) and len(sd) == 8 and sd.isdigit()):
            raise ValueError("â›” STAT_DAY(yyyymmdd)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. vis.set_stat_day('YYYYMMDD') ë˜ëŠ” ensure_stat_day()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        return sd

    def _update_last_run(self, run_type: str, hour_label: str | None):
        sd = self.last_run.get("stat_day")  # ë³´ì¡´
        self.last_run = {"type": run_type, "hour": hour_label, "stat_day": sd}

    # --------------------------------------------------------------- [ VISUM LOAD & CLOSE ]
    
    def open(self, filename: str = None):
        self.visum = com.Dispatch("Visum.Visum.22")

        ver_filename = filename or self.default_version_name
        ver_path = os.path.join(self.base_path, ver_filename)
        if not os.path.isfile(ver_path):
            print(f"â›” VISUM ë„¤íŠ¸ì›Œí¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ver_path}")
            self.visum = None
            return

        self.visum.LoadVersion(ver_path)
        print(f">>>>> âœ… VISUM ë„¤íŠ¸ì›Œí¬ ë¡œë“œ ì™„ë£Œ: {ver_path}")

    def close(self):
        if self.visum:
            self.visum = None
            print(">>>>> âœ… VISUM ì„¸ì…˜ ì¢…ë£Œ")











    # --------------------------------------------------------------- [ êµí†µëŸ‰ ì—°ê³„(ê³µí†µ) ]
    
    def insert_turn_volumes(self, data_list, verbose: bool = False) -> int:
        if not self.visum:
            print("â›” Visum ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        print(">>>>> âœ… ì¡°íšŒëœ êµí†µëŸ‰ ë°ì´í„°ì˜ VISUM ì—°ê³„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

        turns = self.visum.Net.Turns
        gn_node_list = [int(x[1]) if x[1] is not None else 0 for x in turns.GetMultiAttValues("gn_node_id")]
        gn_dir_list  = [int(x[1]) if x[1] is not None else 0 for x in turns.GetMultiAttValues("gn_direction_id")]
        from_list    = [int(x[1]) if x[1] is not None else 0 for x in turns.GetMultiAttValues("FromNodeNo")]
        via_list     = [int(x[1]) if x[1] is not None else 0 for x in turns.GetMultiAttValues("ViaNodeNo")]
        to_list      = [int(x[1]) if x[1] is not None else 0 for x in turns.GetMultiAttValues("ToNodeNo")]

        key2nodes = {
            (gn_node_list[i], gn_dir_list[i]): (from_list[i], via_list[i], to_list[i])
            for i in range(len(gn_node_list))
            if gn_node_list[i] and gn_dir_list[i]
        }

        updates = 0
        for item in (data_list or []):
            try:
                cross_id = int(item.get("cross_id"))
            except Exception:
                if verbose:
                    print(f"â›” cross_id íŒŒì‹± ì‹¤íŒ¨: {item}")
                continue

            for d in item.get("data", []):
                dir_str = d.get("direction")
                val = d.get("value")
                if val is None or not dir_str:
                    continue

                try:
                    direction_num = int(dir_str.split("_")[-1])
                except Exception:
                    if verbose:
                        print(f"â›” direction íŒŒì‹± ì‹¤íŒ¨ â€” cross_id={cross_id}, direction={dir_str}")
                    continue

                nodes = key2nodes.get((cross_id, direction_num))
                if not nodes:
                    if verbose:
                        print(f"â›” ì—°ê³„ ì‹¤íŒ¨ >>> CROSS_ID={cross_id}, DIR={direction_num:02d} (Turn ë¯¸ì¡´ì¬)")
                    continue

                f, v, t = nodes
                try:
                    turn = turns.ItemByKey(f, v, t)
                    for att in ("ABT_TOTAL1", "ABT_TOTAL2", "TOTAL_TRAFFIC_VOL"):
                        turn.SetAttValue(att, val)
                    updates += 1
                except Exception as e:
                    print(f"â›” SetAttValue ì‹¤íŒ¨ â€” nodes=({f},{v},{t}), err={e}")

        return updates

    # --------------------------------------------------------------- [ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰(ê³µí†µ) ]
    
    def _execute_procedure(self, proc_no: int):
        ops = self.visum.Procedures.Operations
        try:
            print(f">>>>> âœ… Procedure Sequence Set Active Number : {proc_no}")
            ops.ItemByKey(proc_no).SetAttValue("Active", 1)
            self.visum.Procedures.Execute()
        finally:
            try:
                ops.ItemByKey(proc_no).SetAttValue("Active", 0)
            except Exception:
                pass

    def simulate_prev_day(self):
        self._require_stat_day()
        self._execute_procedure(self.prev_day_proc_no)
        self._update_last_run("prev_day", None)
        print(f"      âœ… ì „ì¼ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ (Active={self.prev_day_proc_no})")

    def simulate_hour(self, hour: int):
        self._require_stat_day()
        proc = self.HOUR_TO_PROC.get(int(hour))
        if proc is None:
            print(f"â›” {hour}ì‹œ í”„ë¡œì‹œì € ë²ˆí˜¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        self._execute_procedure(proc)
        self._update_last_run("hourly", f"{int(hour):02d}")
        print(f">>>>> âœ… {int(hour):02d}ì‹œ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ (Active={proc})")











    # --------------------------------------------------------------- [ ë§í¬ ê²°ê³¼ê°’ ì¶”ì¶œ ]
    
    def get_links_result_df(self) -> pd.DataFrame:
        """
        Visumì—ì„œ ë§í¬ ê²°ê³¼ê°’ì„ ì¶”ì¶œí•´ DataFrameìœ¼ë¡œ ë°˜í™˜.
        - ì¶”ì¶œ ì»¬ëŸ¼: LINK_ID, vc(VolCapRatioPrT(AP)), vehs(VolVehPrT(AP)), speed(TCur_PrTSys(a))
        - LINK_IDê°€ 'A, B, C' ê°™ì´ ì‰¼í‘œë¡œ í•©ì³ì§„ ê²½ìš° ê°œë³„ í–‰ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ë™ì¼í•œ vc/vehs/speed ë³µì œ
        - LINK_IDëŠ” VARCHAR(10)ë¡œ ê°€ê³µ(íŠ¸ë¦¼ í›„ ìµœëŒ€ 10ì, ë¹ˆë¬¸ì ì œì™¸)
        - STAT_DAY/STAT_HOURëŠ” ì—¬ê¸°ì„œ ì„¸íŒ…í•˜ì§€ ì•ŠìŒ(ê° INSERT í•¨ìˆ˜ì—ì„œ ì²˜ë¦¬)
        """
        if not self.visum:
            print("â›” Visum ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame(columns=["LINK_ID", "vc", "vehs", "speed"])

        run_type = (self.last_run or {}).get("type")
        if run_type not in ("prev_day", "hourly"):
            print("â›” ì‹¤í–‰ ì´ë ¥ ì—†ìŒ â€” simulate í˜¸ì¶œ í›„ ì‚¬ìš©í•˜ì„¸ìš”.")
            return pd.DataFrame(columns=["LINK_ID", "vc", "vehs", "speed"])

        # â–¶ í•„ìš”í•œ ì†ì„±ë§Œ ì¶”ì¶œ
        attrs = [
            "LINK_ID",
            "VolCapRatioPrT(AP)",   # â†’ vc
            "VolVehPrT(AP)",        # â†’ vehs
            "TCur_PrTSys(a)",       # â†’ speed
        ]

        rows = self.visum.Net.Links.GetMultipleAttributes(attrs, True)

        records, truncated_ids, empty_ids = [], set(), 0

        for row in rows:
            base = dict(zip(attrs, row))
            raw_id = base.get("LINK_ID")

            if not raw_id:               # None / ë¹ˆë¬¸ì â†’ ìŠ¤í‚µ ì¹´ìš´íŠ¸
                empty_ids += 1
                continue

            # 1) ì‰¼í‘œ ê¸°ì¤€ ë¶„ë¦¬(ê³µë°± íŠ¸ë¦¼) â†’ ë¹ˆ í† í° ì œê±°
            link_ids = [tok.strip() for tok in str(raw_id).split(",") if tok.strip()]

            # 2) ê° LINK_ID ë³„ë¡œ í•œ í–‰ì”© ë³µì œ (vc/vehs/speed ë™ì¼ ë³µì œ)
            for lid in link_ids:
                lid_trim = lid[:10]      # VARCHAR(10) ë³´ì¥
                if len(lid) > 10:
                    truncated_ids.add(lid)

                rec = {
                    "LINK_ID": lid_trim,
                    "vc":    base.get("VolCapRatioPrT(AP)"),
                    "vehs":  base.get("VolVehPrT(AP)"),
                    "speed": base.get("TCur_PrTSys(a)"),
                }
                records.append(rec)

        # ë°ì´í„°í”„ë ˆì„ êµ¬ì„±
        df = pd.DataFrame.from_records(records, columns=["LINK_ID", "vc", "vehs", "speed"])

        if df.empty:
            print(f"      ğŸ“Š DataFrame í¬ê¸°: 0 í–‰ Ã— 4 ì—´ (ë¹ˆ LINK_ID {empty_ids}ê±´)")
            if truncated_ids:
                ex = list(sorted(truncated_ids))[:5]
                print(f"âš ï¸ 10ì ì´ˆê³¼ LINK_ID {len(truncated_ids)}ê±´(ì˜ˆì‹œ ìµœëŒ€ 5ê°œ): {ex}")
            return df

        # ìˆ«ìí˜• ë³´ì •
        df["vc"]    = pd.to_numeric(df["vc"], errors="coerce")        # float
        df["vehs"]  = pd.to_numeric(df["vehs"], errors="coerce").fillna(0).astype(int)  # int
        df["speed"] = pd.to_numeric(df["speed"], errors="coerce")      # float

        # LINK_ID ë¬¸ìì—´ ë³´ì •(ê³µë°±/ë¹ˆë¬¸ì ì œê±° í›„ Noneì€ ë“œë)
        df["LINK_ID"] = df["LINK_ID"].astype(str).str.strip()
        df = df[df["LINK_ID"] != ""]

        # LINK_ID ê¸°ì¤€ ì¤‘ë³µ ì œê±° (ì²« ë“±ì¥ ìš°ì„ )
        before = len(df)
        df.drop_duplicates(subset=["LINK_ID"], keep="first", inplace=True)
        after = len(df)

        # ì •ë ¬
        df.sort_values(by=["LINK_ID"], inplace=True, ignore_index=True)

        print(
            f"      ğŸ“Š DataFrame í¬ê¸°: {len(df)} í–‰ Ã— {len(df.columns)} ì—´"
            f" (ì¤‘ë³µ ì œê±°: {before - after}ê±´, ë¹ˆ LINK_ID: {empty_ids}ê±´)"
        )
        if truncated_ids:
            ex = list(sorted(truncated_ids))[:5]
            print(f"âš ï¸ 10ì ì´ˆê³¼ LINK_ID {len(truncated_ids)}ê±´(ì˜ˆì‹œ ìµœëŒ€ 5ê°œ): {ex}")

        return df

    # --------------------------------------------------------------- [ CSV/DB I/O ]
    
    def export_csv(self, df: pd.DataFrame, run_type: str, hour: str | None) -> str:
        if df.empty:
            print("â›” CSV ë‚´ë³´ë‚¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return ""
        os.makedirs(self.csv_out_dir, exist_ok=True)  # ë””ë ‰í„°ë¦¬ ë³´ì¥

        sd = self._require_stat_day()
        ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        kind = "prevday" if run_type == "prev_day" else f"h{hour}"
        fname = f"links_{sd}_{kind}_{ts}.csv"
        fpath = os.path.join(self.csv_out_dir, fname)

        # ì—‘ì…€ ê²€í†  í¸ì˜ ìœ„í•´ na_rep ì§€ì •(ì„ íƒ)
        df.to_csv(fpath, index=False, encoding="utf-8-sig", na_rep="")
        print(f">>>>> âœ… ê²°ê³¼ê°’ì„ CSVíŒŒì¼ë¡œ ì €ì¥ì„ ì™„ë£Œí•˜ì˜€ìŠµë‹ˆë‹¤. ê²½ë¡œ: {fpath}")
        return fpath
    
    def _coerce_numeric(self, df: pd.DataFrame) -> pd.DataFrame:
        out = df.copy()
        if "DISTRICT" in out.columns:
            out["DISTRICT"] = pd.to_numeric(out["DISTRICT"], errors="coerce")
        if "UPDOWN" in out.columns:
            out["UPDOWN"] = pd.to_numeric(out["UPDOWN"], errors="coerce")
        if "vc" in out.columns:
            out["vc"] = pd.to_numeric(out["vc"], errors="coerce").round(2)
        if "vehs" in out.columns:
            out["vehs"] = pd.to_numeric(out["vehs"], errors="coerce").fillna(0).astype(int)
        if "speed" in out.columns:
            out["speed"] = pd.to_numeric(out["speed"], errors="coerce").round(2)
        return out

    def _rename_for_db(self, df: pd.DataFrame) -> pd.DataFrame:
        out = df.copy()
        out.rename(columns={"sa": "SA_NO", "vc": "VC", "vehs": "VEHS", "speed": "SPEED"}, inplace=True)
        return out

    def _coerce_str_fields(self, df: pd.DataFrame, fields=("LINK_ID","SA_NO","ROAD_NAME")) -> pd.DataFrame:
        out = df.copy()
        for c in fields:
            if c in out.columns:
                out[c] = out[c].astype(object).where(pd.notna(out[c]), None)
                out[c] = out[c].map(lambda x: str(x) if x is not None else None)
        return out









    def read_gpa_file_get_road_link(self, db_conn, gpa_file_path: str, stat_hour: str):
        """
            - TDA_ROAD_VOL_INFO.ROAD_ID ì „ì²´ ì¡°íšŒ
            - gpa_file_path/{ROAD_ID}.gpa ì ìš©
            - Linksì—ì„œ ['LINK_ID','ì „ì¼_ìš©ëŸ‰']ë§Œ ì¶”ì¶œ â†’ ì‰¼í‘œ ë¶„ë¦¬/10ì ì œí•œ/ë¹ˆê°’ ì œê±°
            - ê° ROAD_IDë§ˆë‹¤ df ìƒì„±, df['STAT_HOUR']=stat_hour ì„¸íŒ…
            - ê³§ë°”ë¡œ insert_hour_road_results(df, db_conn, stat_hour, road_id) í˜¸ì¶œ (ë³¸ë¬¸ì€ ì´í›„ êµ¬í˜„)
        """
        if not hasattr(self, "visum") or self.visum is None:
            print("â›” Visum ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤. GPA ì ìš© ìŠ¤í‚µ")
            return {}

        road_results: dict[str, pd.DataFrame] = {}

        try:
            cur = db_conn.cursor()
            cur.execute("SELECT ROAD_ID FROM TOMMS.TDA_ROAD_VOL_INFO")
            road_ids = [str(r[0]).strip() for r in cur.fetchall() if r and r[0] is not None]
            road_ids = sorted(set(road_ids))
            print(f"ğŸ” GPA ëŒ€ìƒ ROAD_ID {len(road_ids)}ê±´")

            missing, failed, applied = [], [], 0

            for rid in road_ids:
                gpa_file = os.path.join(gpa_file_path, f"{rid}.gpa")  # ROAD_IDê°€ ê³§ íŒŒì¼ëª…
                if not os.path.isabs(gpa_file):
                    gpa_file = os.path.abspath(gpa_file)

                if not os.path.isfile(gpa_file):
                    missing.append(gpa_file)
                    continue

                try:
                    # 1) GPA ì ìš©
                    self.visum.Net.GraphicParameters.Open(gpa_file)
                    applied += 1

                    # 2) ë§í¬ ì†ì„± ì¶”ì¶œ
                    attrs = ["LINK_ID", "ì „ì¼_ìš©ëŸ‰"]
                    rows = self.visum.Net.Links.GetMultipleAttributes(attrs, True)

                    # 3) ê°€ê³µ: LINK_ID ë¶„ë¦¬/ì •ë¦¬
                    records, truncated_ids, empty_ids = [], set(), 0
                    for row in rows:
                        base = dict(zip(attrs, row))
                        raw_id = base.get("LINK_ID")
                        if not raw_id:
                            empty_ids += 1
                            continue

                        link_ids = [tok.strip() for tok in str(raw_id).split(",") if tok.strip()]
                        for lid in link_ids:
                            lid_trim = lid[:10]
                            if len(lid) > 10:
                                truncated_ids.add(lid)
                            records.append({
                                "ROAD_ID": rid,
                                "LINK_ID": lid_trim,
                                "ì „ì¼_ìš©ëŸ‰": base.get("ì „ì¼_ìš©ëŸ‰"),
                            })

                    df = pd.DataFrame.from_records(records, columns=["ROAD_ID", "LINK_ID", "ì „ì¼_ìš©ëŸ‰"])
                    if df.empty:
                        print(f"      ğŸ“Š ROAD_ID={rid} â†’ 0í–‰ (ë¹ˆ LINK_ID {empty_ids}ê±´)")
                        if truncated_ids:
                            ex = list(sorted(truncated_ids))[:5]
                            print(f"      âš ï¸ 10ì ì´ˆê³¼ LINK_ID {len(truncated_ids)}ê±´(ì˜ˆì‹œâ‰¤5): {ex}")
                        road_results[rid] = df
                        continue

                    # ìˆ«ì/ë¬¸ì ë³´ì •
                    df["ì „ì¼_ìš©ëŸ‰"] = pd.to_numeric(df["ì „ì¼_ìš©ëŸ‰"], errors="coerce")
                    df["LINK_ID"] = df["LINK_ID"].astype(str).str.strip()
                    df = df[df["LINK_ID"] != ""]

                    # ì¤‘ë³µ ì œê±°/ì •ë ¬
                    before = len(df)
                    df.drop_duplicates(subset=["LINK_ID"], keep="first", inplace=True)
                    after = len(df)
                    df.sort_values(by=["LINK_ID"], inplace=True, ignore_index=True)

                    # ğŸ”µ STAT_HOUR ì„¸íŒ…
                    df["STAT_HOUR"] = stat_hour

                    print(f"      âœ… ROAD_ID={rid} ê²°ê³¼: {len(df)}í–‰ (ì¤‘ë³µ {before - after}ê±´, ë¹ˆ LINK_ID {empty_ids}ê±´)")
                    if truncated_ids:
                        ex = list(sorted(truncated_ids))[:5]
                        print(f"      âš ï¸ 10ì ì´ˆê³¼ LINK_ID {len(truncated_ids)}ê±´(ì˜ˆì‹œâ‰¤5): {ex}")

                    road_results[rid] = df

                    # ğŸ‘‰ ì—¬ê¸°ì„œ ë°”ë¡œ ì‹œê°„ëŒ€ road ê²°ê³¼ INSERT í˜¸ì¶œ(ë³¸ë¬¸ì€ ë‚˜ì¤‘ êµ¬í˜„)
                    self.insert_hour_road_results(df, db_conn=db_conn, stat_hour=stat_hour, road_id=rid)

                except Exception as e:
                    failed.append((gpa_file, str(e)))

            print(f"ğŸ–¼ï¸ GPA ì ìš© ì™„ë£Œ â€” ì„±ê³µ {applied}ê±´ / ë¯¸ì¡´ì¬ {len(missing)}ê±´ / ì‹¤íŒ¨ {len(failed)}ê±´")
            if missing:
                os.makedirs("./output", exist_ok=True)
                with open("./output/missing_gpa_files.txt", "w", encoding="utf-8") as f:
                    for p in missing:
                        f.write(p + "\n")
                print("ğŸ“‚ ë¯¸ì¡´ì¬ GPA íŒŒì¼ ëª©ë¡ ì €ì¥: ./output/missing_gpa_files.txt")
            if failed:
                with open("./output/failed_gpa_files.txt", "w", encoding="utf-8") as f:
                    for p, msg in failed:
                        f.write(f"{p}\t{msg}\n")
                print("ğŸ“‚ ì‹¤íŒ¨ GPA íŒŒì¼ ëª©ë¡ ì €ì¥: ./output/failed_gpa_files.txt")

            return road_results

        except Exception as ex:
            print(f"â›” GPA ì ìš© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {ex}")
            return {}

    # --------------------------------------------------------------- [ ì „ì¼ ê²°ê³¼ê°’ insert ]

    def insert_day_link_results(self, df: pd.DataFrame, db_conn, chunk_size: int = 20000) -> int:
        if df is None or df.empty or db_conn is None:
            print("â›” ì „ì¼ INSERT: DF/DB ëˆ„ë½")
            return 0

        # ìµœì¢… ìŠ¤í‚¤ë§ˆ(ìˆœì„œ ê³ ì •)
        required = ["STAT_DAY", "LINK_ID", "VC", "VEHS", "SPEED"]

        # 1) ì¤€ë¹„: ë¶ˆí•„ìš” ì»¬ëŸ¼ ì œê±°
        work = df.copy()
        for c in ("run_type", "hour"):
            if c in work.columns:
                work.drop(columns=[c], inplace=True)

        # 2) ì»¬ëŸ¼ëª… í†µì¼ + STAT_DAY ì„¸íŒ…
        rename_map = {"vc": "VC", "vehs": "VEHS", "speed": "SPEED", "link_id": "LINK_ID"}
        work.rename(columns={k: v for k, v in rename_map.items() if k in work.columns}, inplace=True)
        stat_day = self._require_stat_day()  # 'YYYYMMDD'
        work["STAT_DAY"] = stat_day

        # 3) íƒ€ì… ë³´ì •
        if "LINK_ID" in work.columns:
            work["LINK_ID"] = (
                work["LINK_ID"].astype(str).str.strip()
                .map(lambda x: x if x != "" else None)
                .map(lambda x: x[:10] if x is not None else None)
            )
        if "VC" in work.columns:
            work["VC"] = pd.to_numeric(work["VC"], errors="coerce")
        if "VEHS" in work.columns:
            work["VEHS"] = pd.to_numeric(work["VEHS"], errors="coerce").astype("Int64")
        if "SPEED" in work.columns:
            work["SPEED"] = pd.to_numeric(work["SPEED"], errors="coerce")
            work.loc[work["SPEED"] >= 360000, "SPEED"] = 0  # ì´ìƒì¹˜ ì»·(ì˜µì…˜)

        # 4) í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬ + ì •ë ¬
        missing = [c for c in required if c not in work.columns]
        if missing:
            print(f"â›” í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}")
            return 0
        work = work[required]

        # 5) NULL LINK_ID ì œê±°
        before = len(work)
        work = work[work["LINK_ID"].notna()]
        if before != len(work):
            print(f"âš ï¸ LINK_ID NULL {before - len(work)}í–‰ ì œê±°")

        # 6) FK ì‚¬ì „ê²€ì‚¬: TDA_LINK_INFOì— ì¡´ì¬í•˜ëŠ” LINK_IDë§Œ ë‚¨ê¹€
        cur = db_conn.cursor()
        unique_ids = sorted(set(work["LINK_ID"].tolist()))
        valid_ids = set()
        if unique_ids:
            BATCH = 900  # placeholder ì œí•œ ëŒ€ë¹„
            for i in range(0, len(unique_ids), BATCH):
                batch = unique_ids[i:i + BATCH]
                placeholders = ", ".join(["?"] * len(batch))
                sql_chk = f"SELECT LINK_ID FROM TOMMS.TDA_LINK_INFO WHERE LINK_ID IN ({placeholders})"
                cur.execute(sql_chk, batch)
                valid_ids.update(r[0] for r in cur.fetchall())

        missing_ids = sorted(set(unique_ids) - valid_ids)
        if missing_ids:
            print(f"âš ï¸ FK ë¯¸ì¡´ì¬ LINK_ID {len(missing_ids)}ê±´ â€” INSERT ì œì™¸ (ì˜ˆì‹œ 10ê°œ): {missing_ids[:10]}")
            os.makedirs("./output", exist_ok=True)
            miss_path = f"./output/missing_day_link_ids_{stat_day}.txt"
            with open(miss_path, "w", encoding="utf-8") as f:
                for lid in missing_ids:
                    f.write(str(lid) + "\n")
            print(f"ğŸ“‚ ë¯¸ì¡´ì¬ LINK_ID ëª©ë¡ ì €ì¥: {miss_path}")

        work = work[work["LINK_ID"].isin(valid_ids)]
        if work.empty:
            print("â›” ìœ íš¨ LINK_IDê°€ ì—†ì–´ INSERT ìŠ¤í‚µ")
            return 0

        # 7) DB ë°”ì¸ë”© ê°’ ë³€í™˜
        def _to_db_value(v):
            if v is pd.NA:
                return None
            if isinstance(v, np.generic):
                v = v.item()
            if isinstance(v, float) and (pd.isna(v) or np.isnan(v)):
                return None
            return v

        # 8) INSERT ì¤€ë¹„
        sql = f"INSERT INTO TOMMS.TDA_LINK_DAY_RESULT ({', '.join(required)}) VALUES ({', '.join(['?']*len(required))})"
        cur.fast_executemany = True  # ë°°ì¹˜ ì„±ëŠ¥

        # 9) SQL ë¡œê·¸ ì €ì¥(ê²€ì¦ìš©)
        os.makedirs("./output", exist_ok=True)
        sql_log_path = f"./output/day_link_result_insert_{stat_day}.sql.txt"

        def _sql_literal(v):
            if v is None:
                return "NULL"
            if isinstance(v, (int, np.integer)):
                return str(int(v))
            if isinstance(v, (float, np.floating)):
                return str(float(v))
            s = str(v).replace("'", "''")
            return f"'{s}'"

        total = 0
        try:
            with open(sql_log_path, "w", encoding="utf-8") as f_log:
                for s in range(0, len(work), chunk_size):
                    chunk = work.iloc[s:s+chunk_size]
                    data = [tuple(_to_db_value(v) for v in row) for row in chunk.itertuples(index=False, name=None)]

                    # ë¡œê·¸ìš© ì‹¤ì œ SQL
                    for row in data:
                        values_str = [_sql_literal(v) for v in row]
                        f_log.write(
                            f"INSERT INTO TOMMS.TDA_LINK_DAY_RESULT ({', '.join(required)}) VALUES ({', '.join(values_str)});\n"
                        )

                    cur.executemany(sql, data)
                    total += len(data)

            db_conn.commit()
            print(f"      âœ… TDA_LINK_DAY_RESULT INSERT â€” {total}í–‰ (STAT_DAY={stat_day})")
            print(f"      ğŸ“‚ SQL ë¡œê·¸ ì €ì¥: {sql_log_path}")

            return total

        except Exception as ex:
            db_conn.rollback()
            print(f"â›” TDA_LINK_DAY_RESULT INSERT ì˜¤ë¥˜ â€” ë¡¤ë°±: {ex}")
            print(f"ğŸ“‚ SQL ë¡œê·¸(ì‹¤íŒ¨ ì‹œì ê¹Œì§€): {sql_log_path}")
            return 0
    
    # --------------------------------------------------------------- [ ì „ì¼ ì‹œë®¬ë ˆì´ì…˜ íŒŒì´í”„ë¼ì¸ ]
    
    def run_prev_day_pipeline(self, day_payload, db_conn=None, preferred_day: str | None = None):
        # 0) ê¸°ì¤€ì¼ í™•ì • (ë‹¨ í•œ ë²ˆ)
        payload_days = list(day_payload.keys()) if isinstance(day_payload, dict) else None
        chosen_day = self.ensure_stat_day(preferred_day, payload_days)

        # 1) payload íŒŒì‹±
        if isinstance(day_payload, dict):
            payload_list = day_payload.get(chosen_day, [])
            print(f">>>>> âœ… ì „ì¼ êµí†µëŸ‰ ì—°ê³„ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘\n      ëŒ€ìƒì¼:{chosen_day}\n      entries:{len(payload_list)}")
        else:
            payload_list = day_payload or []
            print(f">>>>> âœ… ì „ì¼ êµí†µëŸ‰ ì—°ê³„ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘\n      ëŒ€ìƒì¼:{chosen_day}\n      (list ì…ë ¥) entries:{len(payload_list)}")

        # 2) êµí†µëŸ‰ ì£¼ì…
        upd = self.insert_turn_volumes(payload_list, verbose=True)
        print(f">>>>> âœ… ì „ì¼ ì£¼ì… ê±´ìˆ˜: {upd}")
        if upd == 0:
            print("â›” ì „ì¼ ì£¼ì… 0ê±´ â€” ë§¤í•‘/ì…ë ¥ê°’ í™•ì¸ ê¶Œì¥")

        # 3) ì‹œë®¬ â†’ ê²°ê³¼ â†’ CSV â†’ DB
        self.simulate_prev_day()

        # ê°€ë“œì²´í¬: ì „ì¼ë¡œ ì„¸íŒ…ëëŠ”ì§€ í™•ì¸
        assert self.last_run.get("type") == "prev_day" and self.last_run.get("hour") is None, \
            f"last_run ë¶ˆì¼ì¹˜: {self.last_run}"

        df = self.get_links_result_df()
        
        self.insert_day_link_results(df, db_conn=db.conn)

        print(">>>>> âœ… ì „ì¼ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ")

    # --------------------------------------------------------------- [ ì‹œê°„ëŒ€ë³„ ê²°ê³¼ê°’ insert ]
    
    def insert_hour_link_results(self, df: pd.DataFrame, db_conn, chunk_size: int = 20000) -> int:
        if df is None or df.empty or db_conn is None:
            print("â›” ì‹œê°„ëŒ€ INSERT: DF/DB ëˆ„ë½")
            return 0

        stat_day = self._require_stat_day()              # 'YYYYMMDD'
        hour_lbl = str((self.last_run or {}).get("hour") or "").zfill(2)
        if not hour_lbl:
            print("â›” last_run.hour ì—†ìŒ â€” simulate_hour ì´í›„ í˜¸ì¶œ í•„ìš”")
            return 0
        stat_hour = stat_day + hour_lbl                  # 'YYYYMMDDHH'

        required = ["STAT_HOUR", "LINK_ID", "VC", "VEHS", "SPEED"]

        # 1) ì‘ì—…ìš© ë³µì‚¬ & ë¶ˆí•„ìš” ì»¬ëŸ¼ ì œê±°
        work = df.copy()
        for c in ("run_type", "hour"):
            if c in work.columns:
                work.drop(columns=[c], inplace=True)

        # 2) ì»¬ëŸ¼ëª… í†µì¼ + STAT_HOUR ì„¸íŒ…
        work.rename(columns={
            "link_id": "LINK_ID",
            "vc": "VC",
            "vehs": "VEHS",
            "speed": "SPEED",
        }, inplace=True)
        work["STAT_HOUR"] = stat_hour

        # 3) íƒ€ì… ë³´ì •
        if "LINK_ID" in work.columns:
            work["LINK_ID"] = (
                work["LINK_ID"].astype(str).str.strip()
                .map(lambda x: x if x != "" else None)
                .map(lambda x: x[:10] if x is not None else None)
            )
        if "VC" in work.columns:
            work["VC"] = pd.to_numeric(work["VC"], errors="coerce")
        if "VEHS" in work.columns:
            work["VEHS"] = pd.to_numeric(work["VEHS"], errors="coerce").astype("Int64")
        if "SPEED" in work.columns:
            work["SPEED"] = pd.to_numeric(work["SPEED"], errors="coerce")
            work.loc[work["SPEED"] >= 360000, "SPEED"] = 0

        # 4) í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬ + ì •ë ¬
        missing = [c for c in required if c not in work.columns]
        if missing:
            print(f"â›” í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}")
            return 0
        work = work[required]

        # 5) NULL LINK_ID ì œê±°
        before = len(work)
        work = work[work["LINK_ID"].notna()]
        if before != len(work):
            print(f"âš ï¸ LINK_ID NULL {before - len(work)}í–‰ ì œê±°")

        # 6) FK ì‚¬ì „ê²€ì‚¬: TDA_LINK_INFOì— ì¡´ì¬í•˜ëŠ” LINK_IDë§Œ ë‚¨ê¹€
        cur = db_conn.cursor()
        unique_ids = sorted(set(work["LINK_ID"].tolist()))
        valid_ids = set()
        if unique_ids:
            BATCH = 900  # placeholder ì œí•œ ëŒ€ë¹„
            for i in range(0, len(unique_ids), BATCH):
                batch = unique_ids[i:i + BATCH]
                placeholders = ", ".join(["?"] * len(batch))
                sql_chk = f"SELECT LINK_ID FROM TOMMS.TDA_LINK_INFO WHERE LINK_ID IN ({placeholders})"
                cur.execute(sql_chk, batch)
                valid_ids.update(r[0] for r in cur.fetchall())

        missing_ids = sorted(set(unique_ids) - valid_ids)
        if missing_ids:
            print(f"âš ï¸ FK ë¯¸ì¡´ì¬ LINK_ID {len(missing_ids)}ê±´ â€” INSERT ëŒ€ìƒì—ì„œ ì œì™¸ (ì˜ˆì‹œ 10ê°œ): {missing_ids[:10]}")
            os.makedirs("./output", exist_ok=True)
            miss_path = f"./output/missing_link_ids_{stat_hour}.txt"
            with open(miss_path, "w", encoding="utf-8") as f:
                for lid in missing_ids:
                    f.write(str(lid) + "\n")
            print(f"ğŸ“‚ ë¯¸ì¡´ì¬ LINK_ID ëª©ë¡ ì €ì¥: {miss_path}")

        work = work[work["LINK_ID"].isin(valid_ids)]
        if work.empty:
            print("â›” ìœ íš¨ LINK_IDê°€ ì—†ì–´ INSERT ìŠ¤í‚µ")
            return 0

        # 7) íŒŒë¼ë¯¸í„° ë°”ì¸ë”© ê°’ ë³€í™˜
        def _to_db_value(v):
            if v is pd.NA:
                return None
            if isinstance(v, np.generic):
                v = v.item()
            if isinstance(v, float) and (pd.isna(v) or np.isnan(v)):
                return None
            return v

        # 8) INSERT
        sql = f"INSERT INTO TOMMS.TDA_LINK_HOUR_RESULT ({', '.join(required)}) VALUES ({', '.join(['?']*len(required))})"
        cur.fast_executemany = True

        total = 0
        try:
            for s in range(0, len(work), chunk_size):
                chunk = work.iloc[s:s + chunk_size]
                data = [tuple(_to_db_value(v) for v in row) for row in chunk.itertuples(index=False, name=None)]
                cur.executemany(sql, data)
                total += len(data)

            db_conn.commit()
            print(f"      âœ… TDA_LINK_HOUR_RESULT INSERT â€” {stat_hour} {total}í–‰")
            return total

        except Exception as ex:
            db_conn.rollback()
            print(f"â›” TDA_LINK_HOUR_RESULT INSERT ì˜¤ë¥˜ â€” ë¡¤ë°±: {ex}")
            return 0
    
    # --------------------------------------------------------------- [ ì‹œê°„ëŒ€ë³„ road_id ê²°ê³¼ê°’ insert ]
    
    def insert_hour_road_results(
        self,
        df: pd.DataFrame,
        db_conn,
        stat_hour: str,
        road_id: str,
        chunk_size: int = 20000,
    ) -> int:
        """
        TDA_ROAD_HOUR_RESULT ìŠ¤í‚¤ë§ˆ
        - STAT_HOUR (VARCHAR10, NN)
        - ROAD_ID   (VARCHAR10, NN)
        - LINK_ID   (VARCHAR10,  Y)  # í•˜ì§€ë§Œ LINK_ID ì—†ëŠ” í–‰ì€ ì—¬ê¸°ì„œ ë“œë¡­
        - FB_VEHS   (NUMBER(9),  Y)  # 'ì „ì¼_ìš©ëŸ‰' ë§¤í•‘
        """
        if df is None or df.empty or db_conn is None:
            print("â›” ROAD HOUR INSERT: DF/DB ëˆ„ë½")
            return 0

        # 0) ê²°ê³¼ í…Œì´ë¸”ëª…
        table = "TOMMS.TDA_ROAD_VOL_HOUR_RESULT"

        # 1) ìŠ¤í‚¤ë§ˆ ì •ê·œí™”
        work = df.copy()

        # ì»¬ëŸ¼ëª… í†µì¼: ì „ì¼_ìš©ëŸ‰ â†’ FB_VEHS, link_idâ†’LINK_ID ë“±
        work.rename(columns={
            "ì „ì¼_ìš©ëŸ‰": "FB_VEHS",
            "link_id": "LINK_ID",
            "stat_hour": "STAT_HOUR",
            "road_id": "ROAD_ID",
        }, inplace=True)

        # íŒŒë¼ë¯¸í„°ë¡œ ë°›ì€ STAT_HOUR/ROAD_IDë¥¼ ê°•ì œ ì„¸íŒ… (ì‹ ë¢°ì› í†µì¼)
        work["STAT_HOUR"] = str(stat_hour)
        work["ROAD_ID"]   = str(road_id)

        # í•„ìš” ì»¬ëŸ¼ë§Œ ìœ ì§€ (ìˆœì„œ ê³ ì •)
        required = ["STAT_HOUR", "ROAD_ID", "LINK_ID", "FB_VEHS"]
        for c in required:
            if c not in work.columns:
                work[c] = pd.Series(dtype="object")  # ëˆ„ë½ ì»¬ëŸ¼ ìƒì„±
        work = work[required]

        # 2) íƒ€ì… ë³´ì •
        # LINK_ID: ë¬¸ìì—´ 10ì, ê³µë°±/ë¹ˆë¬¸ì None
        work["LINK_ID"] = (
            work["LINK_ID"].astype(str).str.strip()
            .map(lambda x: None if x == "" or x.lower() == "none" else x[:10])
        )
        # FB_VEHS: ì •ìˆ˜(Int64)ë¡œ
        work["FB_VEHS"] = pd.to_numeric(work["FB_VEHS"], errors="coerce").astype("Int64")

        # 3) LINK_ID ì—†ëŠ” í–‰ ì œê±°(ìš”êµ¬ì‚¬í•­: ê°’ì´ ì—†ìœ¼ë©´ ëª¨ë‘ ë‚ ë¦¼)
        before = len(work)
        work = work[work["LINK_ID"].notna()]
        if before != len(work):
            print(f"âš ï¸ LINK_ID NULL {before - len(work)}í–‰ ì œê±° (ROAD_ID={road_id}, STAT_HOUR={stat_hour})")

        if work.empty:
            print(f"â›” INSERT ìŠ¤í‚µ â€” ìœ íš¨í–‰ 0 (ROAD_ID={road_id}, STAT_HOUR={stat_hour})")
            return 0

        # 4) FK ì‚¬ì „ê²€ì‚¬: TDA_LINK_INFO(LINK_ID)
        cur = db_conn.cursor()
        unique_ids = sorted(set(work["LINK_ID"].tolist()))
        valid_ids = set()
        if unique_ids:
            BATCH = 900
            for i in range(0, len(unique_ids), BATCH):
                batch = unique_ids[i:i+BATCH]
                placeholders = ", ".join(["?"] * len(batch))
                sql_chk = f"SELECT LINK_ID FROM TOMMS.TDA_LINK_INFO WHERE LINK_ID IN ({placeholders})"
                cur.execute(sql_chk, batch)
                valid_ids.update(r[0] for r in cur.fetchall())
        missing_ids = sorted(set(unique_ids) - valid_ids)
        if missing_ids:
            print(f"âš ï¸ LINK_ID FK ë¯¸ì¡´ì¬ {len(missing_ids)}ê±´ â€” ì œì™¸ (ì˜ˆì‹œâ‰¤10): {missing_ids[:10]}")
            os.makedirs("./output", exist_ok=True)
            miss_path = f"./output/missing_hour_road_link_ids_{stat_hour}_{road_id}.txt"
            with open(miss_path, "w", encoding="utf-8") as f:
                for lid in missing_ids:
                    f.write(str(lid) + "\n")
            print(f"ğŸ“‚ FK ë¯¸ì¡´ì¬ LINK_ID ì €ì¥: {miss_path}")
        work = work[work["LINK_ID"].isin(valid_ids)]

        if work.empty:
            print(f"â›” INSERT ìŠ¤í‚µ â€” FK ìœ íš¨í–‰ 0 (ROAD_ID={road_id}, STAT_HOUR={stat_hour})")
            return 0

        # 5) ë°”ì¸ë”© ê°’ ë³€í™˜
        def _to_db_value(v):
            if v is pd.NA:
                return None
            if isinstance(v, np.generic):
                v = v.item()
            if isinstance(v, float) and (pd.isna(v) or np.isnan(v)):
                return None
            return v

        # 6) INSERT
        sql = f"INSERT INTO {table} ({', '.join(required)}) VALUES ({', '.join(['?']*len(required))})"
        cur.fast_executemany = True

        # (ì„ íƒ) SQL ë¡œê·¸
        os.makedirs("./output", exist_ok=True)
        log_path = f"./output/road_hour_result_insert_{stat_hour}_{road_id}.sql.txt"
        def _sql_literal(v):
            if v is None: return "NULL"
            if isinstance(v, (int, np.integer)): return str(int(v))
            if isinstance(v, (float, np.floating)): return str(float(v))
            s = str(v).replace("'", "''"); return f"'{s}'"

        total = 0
        try:
            with open(log_path, "w", encoding="utf-8") as f_log:
                for s in range(0, len(work), chunk_size):
                    chunk = work.iloc[s:s+chunk_size]
                    data = [tuple(_to_db_value(v) for v in row) for row in chunk.itertuples(index=False, name=None)]
                    # ë¡œê·¸ìš© SQL
                    for row in data:
                        values_str = [_sql_literal(v) for v in row]
                        f_log.write(
                            f"INSERT INTO {table} ({', '.join(required)}) "
                            f"VALUES ({', '.join(values_str)});\n"
                        )
                    cur.executemany(sql, data)
                    total += len(data)

            db_conn.commit()
            print(f"      âœ… ROAD_HOUR_RESULT INSERT â€” STAT_HOUR={stat_hour}, ROAD_ID={road_id}, í–‰ìˆ˜={total}")
            print(f"      ğŸ“‚ SQL ë¡œê·¸: {log_path}")
            return total

        except Exception as ex:
            db_conn.rollback()
            print(f"â›” ROAD_HOUR_RESULT INSERT ì˜¤ë¥˜ â€” ë¡¤ë°±: {ex}")
            print(f"ğŸ“‚ SQL ë¡œê·¸(ì‹¤íŒ¨ ì‹œì ê¹Œì§€): {log_path}")
            return 0
    
    # --------------------------------------------------------------- [ ì‹œê°„ëŒ€ë³„ ì‹œë®¬ë ˆì´ì…˜ íŒŒì´í”„ë¼ì¸ ]

    def run_hourly_pipeline(self, hourly_payload_map: dict, db_conn=None):
        """
        ê³ ì • ìˆœì„œ: 08 â†’ 11 â†’ 14 â†’ 17
        STAT_DAYëŠ” ì´ë¯¸ ensure_stat_day/set_stat_dayë¡œ í™•ì •ë˜ì–´ ìˆì–´ì•¼ í•¨.
        """
        
        # ğŸ”µ GPA íŒŒì¼ ê²½ë¡œ ì§€ì •
        gpa_file_path = r"C:\Digital Twin Simulation Network\VISUM\gpa_file"
        
        self._require_stat_day()
        stat_day = self.last_run['stat_day']
        print(f">>>>> âœ… ì‹œê°„ëŒ€ êµí†µëŸ‰ ì—°ê³„ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘\n      STAT_DAY={self.last_run['stat_day']}")

        for hh in [8, 11, 14, 17]:
            key = f"{hh:02d}"
            payload = hourly_payload_map.get(key, [])
            print(f">>>>> âœ… {key}ì‹œ ì²˜ë¦¬ ì‹œì‘ â€” êµì°¨ë¡œ:{len(payload)}")

            upd = self.insert_turn_volumes(payload, verbose=False)
            print(f">>>>> âœ… {key}ì‹œ ì£¼ì… ê±´ìˆ˜: {upd}")

            self.simulate_hour(hh)

            # ê°€ë“œì²´í¬: ì‹œê°„ëŒ€ ì„¸íŒ… í™•ì¸
            assert self.last_run.get("type") == "hourly" and self.last_run.get("hour") == key, \
                f"last_run ë¶ˆì¼ì¹˜: {self.last_run}"

            df = self.get_links_result_df()
            self.insert_hour_link_results(df, db_conn=db.conn)
            
            # ğŸ”µ INSERT ì´í›„: GPA íŒŒì¼ ì ìš©
            stat_hour = f"{stat_day}{key}"
            if gpa_file_path:
                pass
                # self.read_gpa_file_get_road_link(db_conn, gpa_file_path, stat_hour)

        print(">>>>> âœ… ì‹œê°„ëŒ€ë³„ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ")











# ====================================================================================== [ main ì‹¤í–‰í•¨ìˆ˜ ]

class DualLogger:
    def __init__(self, file_path):
        self.terminal = sys.stdout
        self.log = open(file_path, "a", encoding="utf-8")

    def write(self, message):
        self.terminal.write(message)   # ì½˜ì†” ì¶œë ¥
        self.log.write(message)        # íŒŒì¼ ê¸°ë¡

    def flush(self):
        self.terminal.flush()
        self.log.flush()

if __name__ == "__main__":
    # ë¡œê·¸ í´ë” ìƒì„±
    log_dir = r"C:\Digital Twin Simulation Program\auto simulation\logs"
    os.makedirs(log_dir, exist_ok=True)

    # íŒŒì¼ëª…: ë‚ ì§œ+ì‹œê°„ ì ‘ë‘ì–´
    ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"{ts}_visum_simulation.log")

    # dual logger ì„¸íŒ…
    sys.stdout = DualLogger(log_file)

    print(">>>>> âœ… VISUM ìë™í™” ì‹œë®¬ë ˆì´ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    USE_FIXED_TIME = True  # ì‹¤ì‹œê°„ ë‹¨ìœ„ë¡œ ë³€ê²½í•˜ë ¤ë©´ False

    if USE_FIXED_TIME:
        fixed_now = datetime.datetime.strptime("2025070201", "%Y%m%d%H")
        query_day, target_stat_hours = compute_target_hours(fixed_now, ["08", "11", "14", "17"])
    else:
        query_day, target_stat_hours = compute_target_hours(None, ["08", "11", "14", "17"])

    # 1) DB
    config = Config()
    db = DatabaseManager(config)
    print(">>>>> âœ… Config, DB í´ë˜ìŠ¤ê°€ ì„ ì–¸ë˜ì–´ main í•¨ìˆ˜ ë‚´ ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    try:
        print(">>>>> âœ… êµí†µëŸ‰ ë°ì´í„° ì¡°íšŒë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        traffic_by_hour, traffic_by_day, query_day_from_db = db.fetch_and_process_data(target_stat_hours)

        stat_day_final = query_day_from_db or query_day

        vis = VisumSimulationManager(
            base_path=r"C:/Digital Twin Simulation network/VISUM",
            default_version_name="ê°•ë¦‰ì‹œ ì „êµ­ ì „ì¼ ìµœì¢…ë³¸.ver",
            prev_day_proc_no=22,
            csv_out_dir=r"C:/Digital Twin Simulation network/VISUM/result_export",
        )

        print(">>>>> âœ… Visum í´ë˜ìŠ¤ê°€ ì„ ì–¸ë˜ì–´ main í•¨ìˆ˜ ë‚´ ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

        # 4) Visum open & load
        vis.open()
        vis.set_stat_day(stat_day_final)

        # 5) ì „ì¼ íŒŒì´í”„ë¼ì¸
        vis.run_prev_day_pipeline(traffic_by_day, db_conn=db.conn, preferred_day=stat_day_final)

        # 6) ì‹œê°„ëŒ€ íŒŒì´í”„ë¼ì¸
        vis.run_hourly_pipeline(traffic_by_hour, db_conn=db.conn)

    finally:
        if 'vis' in locals():
            vis.close()
        db.close()
        print(f"ğŸ“‚ ë¡œê·¸ ì €ì¥ ì™„ë£Œ â†’ {log_file}")