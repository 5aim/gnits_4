import sys
import os
import re
import time
import shutil
import pyodbc
import fnmatch
import datetime
import pywintypes
import pandas as pd
import win32com.client as com
import numpy as np

from pprint import pprint
from decimal import Decimal
from dotenv import load_dotenv
from contextlib import redirect_stdout
from collections import defaultdict
from typing import Dict, List, Tuple, Optional








# ================================================== [ DPI ÏÑ§Ï†ï ]

def set_dpi_awareness() -> None:
    try:
        from ctypes import windll
        windll.shcore.SetProcessDpiAwareness(1)
    except ImportError:
        print(" ‚õî ctypes Î™®ÎìàÏùÑ Í∞ÄÏ†∏Ïò§Îäî Îç∞ Ïã§Ìå®ÌñàÏäµÎãàÎã§. Python ÌôòÍ≤ΩÏùÑ ÌôïÏù∏ÌïòÏÑ∏Ïöî.")
    except AttributeError:
        print(" ‚õî SetProcessDpiAwareness Ìï®ÏàòÍ∞Ä ÏßÄÏõêÎêòÏßÄ ÏïäÎäî ÌôòÍ≤ΩÏûÖÎãàÎã§. Windows 8.1 Ïù¥ÏÉÅÏóêÏÑúÎßå ÏßÄÏõêÎê©ÎãàÎã§.")
    except OSError as os_error:
        print(f" ‚õî OS Í¥ÄÎ†® Ïò§Î•ò Î∞úÏÉù: {os_error}")
    except Exception as e:
        print(f" ‚õî Ïïå Ïàò ÏóÜÎäî Ïò§Î•ò Î∞úÏÉù: {e}")

set_dpi_awareness()
sys.path.append(os.path.dirname(os.path.abspath(__file__)))








# ================================================== [ Í≥µÏö© Ïú†Ìã∏ ]

# Decimal ‚Üí int/float Î≥ÄÌôò, ÎÇòÎ®∏ÏßÄÎäî Í∑∏ÎåÄÎ°ú.
def to_py(val):
    if isinstance(val, Decimal):
        return int(val) if val == int(val) else float(val)
    return val

# ÌòïÏãù Í≤ÄÏÇ¨: YYYYMMDDHH (10ÏûêÎ¶¨ Ïà´Ïûê)
def is_valid_stat_hour(s: str) -> bool:
    return bool(re.fullmatch(r"\d{10}", s))

# VOL_00 ~ VOL_23 Í∞ôÏùÄ Ïª¨ÎüºÏùÑ Ïù¥Î¶Ñ Í∏∞Ï§Ä Ï†ïÎ†¨Ìïú Î¶¨Ïä§Ìä∏Î°ú Î∞òÌôò. Î∞òÌôò ÌòïÌÉú: [(Ïª¨ÎüºÎ™Ö, Ïª¨ÎüºÏù∏Îç±Ïä§), ...]
def extract_vol_columns(cols: List[str]) -> List[Tuple[str, int]]:
    # Ïù∏Îç±Ïä§Îäî Ïô∏Î∂ÄÏóêÏÑú Îß§ÌïëÎêú dict Î°ú Í∞ÄÏ†∏Ïò§Îäî Í≤ÉÏù¥ ÏùºÎ∞òÏ†ÅÏù¥ÎØÄÎ°ú, Ïó¨Í∏∞ÏÑ† Ïù¥Î¶ÑÎßå Ï†ïÎ†¨.
    vol_names = sorted([c for c in cols if re.fullmatch(r"VOL_\d{2}", c)])
    return vol_names

def to_db_py(v):
    # None/NaN Ï≤òÎ¶¨
    if v is None:
        return None
    # pandasÏùò NAÎ•òÎèÑ NoneÏúºÎ°ú
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass

    # numpy Ïä§ÏπºÎùº ‚Üí ÌååÏù¥Ïç¨ Ïä§ÏπºÎùº
    if isinstance(v, (np.integer,)):
        return int(v)
    if isinstance(v, (np.floating,)):
        return float(v)
    if isinstance(v, (np.bool_,)):
        return bool(v)

    # Decimal ‚Üí int/float
    if isinstance(v, Decimal):
        return int(v) if v == int(v) else float(v)

    # Í∑∏ Ïô∏Îäî ÏõêÎ≥∏ Ïú†ÏßÄ(Î¨∏ÏûêÏó¥ Îì±)
    return v





# ================================================== [ ÌòÑÏû¨ ÏùºÏãú/ÌÉÄÍπÉ ÏãúÍ∞Ñ Í≥ÑÏÇ∞ ]

def compute_target_hours(now: Optional[datetime.datetime] = None,
                        pick_hours: List[str] = None) -> Tuple[str, List[str]]:
    
    """
    Í∏∞Ï§ÄÏãúÍ∞Å(now) Í∏∞Ï§Ä Ï†ÑÎÇ† ÎÇ†Ïßú(YYYYMMDD)ÏôÄ, Ï†ÑÎÇ†Ïùò ÌäπÏ†ï HH Î¶¨Ïä§Ìä∏(YYYYMMDDHH)Î•º ÎèåÎ†§Ï§å.
    """
    
    if now is None:
        now = datetime.datetime.now()
    if pick_hours is None:
        pick_hours = ["08", "11", "14", "17"]

    target_date = (now - datetime.timedelta(days=1)).strftime("%Y%m%d")
    target_stat_hours = [f"{target_date}{h}" for h in pick_hours]
    print(f">>>>> ‚úÖ Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå Í∏∞Ï§Ä ÏãúÍ∞Ñ : {target_date}")
    return target_date, target_stat_hours







# ================================================== [ ÌôòÍ≤ΩÏÑ§Ï†ï Î∞è DB Ï†ëÏÜçÏ†ïÎ≥¥ Î°úÎî© ]

class Config:

    def __init__(self):
        load_dotenv(dotenv_path="C:/Digital Twin Simulation Program/.env")
        raw_env = (os.getenv("FLASK_ENV", "prod") or "").lower()
        self.env = "prod" if raw_env in ("prod", "production") else "test"

        self.db_config = {
            "test": {
                "driver": "Tibero 5 ODBC Driver",
                "server": os.getenv("ENZERO_SERVER"),
                "port": os.getenv("ENZERO_PORT"),
                "db": os.getenv("ENZERO_DB"),
                "uid": os.getenv("ENZERO_UID"),
                "pwd": os.getenv("ENZERO_PWD"),
            },
            "prod": {
                "dsn": os.getenv("DSNNAME"),
                "uid": os.getenv("DBUSER"),
                "pwd": os.getenv("DBPWD"),
            },
        }








# ================================================== [ DB ]

class DatabaseManager:
    """
    Tibero Í∏∞Î∞ò ÏãúÍ∞ÑÎåÄ/ÏùºÎ≥Ñ ÍµêÌÜµÎüâ Ï°∞Ìöå Îß§ÎãàÏ†Ä
    """
    def __init__(self, config: Config):
        self.config = config
        self.conn = self._connect()
        self.cursor = self.conn.cursor() if self.conn else None

        # ÏÇ¨Ïö©Ìï† Ïª¨ÎüºÎì§(Ï∞∏Í≥†Ïö©)
        self.columns = [
            "STAT_HOUR", "CROSS_ID",
            "VOL_00","VOL_01","VOL_02","VOL_03","VOL_04","VOL_05",
            "VOL_06","VOL_07","VOL_08","VOL_09","VOL_10","VOL_11",
            "VOL_12","VOL_13","VOL_14","VOL_15","VOL_16","VOL_17",
            "VOL_18","VOL_19","VOL_20","VOL_21","VOL_22","VOL_23",
        ]

    def _connect(self):
        try:
            if self.config.env == "test":
                db = self.config.db_config["test"]
                print(">>>>> ‚úÖ ÏóîÏ†úÎ°ú DB ÏÑúÎ≤Ñ Ïó∞Í≤∞. ÌÖåÏä§Ìä∏ Î≤ÑÏ†ÑÏúºÎ°ú ÏÑ§Ï†ïÌï©ÎãàÎã§.")
                return pyodbc.connect(
                    f"DRIVER={db['driver']};SERVER={db['server']};PORT={db['port']};"
                    f"DB={db['db']};UID={db['uid']};PWD={db['pwd']};"
                )
            else:
                db = self.config.db_config["prod"]
                print(">>>>> ‚úÖ Í∞ïÎ¶âÏãú Ìã∞Î≤†Î°ú DB ÏÑúÎ≤Ñ Ïó∞Í≤∞. Î∞∞Ìè¨ Î≤ÑÏ†ÑÏûÖÎãàÎã§.")
                return pyodbc.connect(f"DSN={db['dsn']};UID={db['uid']};PWD={db['pwd']}")
        except Exception as e:
            print("‚õî DB Ïó∞Í≤∞ Ïã§Ìå®:", e)
            return None

    def _exec(self, sql: str, params: Tuple = ()) -> Tuple[List[tuple], List[str]]:
        """
        Í≥µÏö© ÏøºÎ¶¨ Ïã§Ìñâ. rows(tuple list)ÏôÄ cols(list[str]) Î∞òÌôò.
        """
        if not self.cursor:
            raise RuntimeError("DB Ïª§ÎÑ•ÏÖòÏù¥ ÏóÜÏäµÎãàÎã§.")
        self.cursor.execute(sql, params)
        rows = [tuple(r) for r in self.cursor.fetchall()]
        cols = [col[0] for col in self.cursor.description]
        return rows, cols

    def fetch_and_process_data(self, target_stat_hours: List[str]) -> Tuple[Dict[str, List[dict]], Dict[str, List[dict]], str]:
        """
        ÏûÖÎ†•: ÎèôÏùº ÎÇ†ÏßúÏùò STAT_HOUR(YYYYMMDDHH) Î¶¨Ïä§Ìä∏
        Ï≤òÎ¶¨: ÏãúÍ∞ÑÎåÄÎ≥Ñ Ï°∞Ìöå ‚Üí ÏùºÎ≥Ñ(ÌïòÎ£®) Ï°∞Ìöå
        Î∞òÌôò: (traffic_data_by_hour, traffic_data_by_day, query_day)
        """

        # ---------- 0) ÏãúÍ∞Ñ ÌååÎùºÎØ∏ÌÑ∞ Ï†ïÍ∑úÌôî ----------
        def _clean_hour(x: str) -> str:
            return (x or "").strip().strip("'\"")

        hours: List[str] = sorted({h for h in (_clean_hour(h) for h in target_stat_hours) if is_valid_stat_hour(h)})
        print(f">>>>> ‚úÖ target_stat_hours(clean): {hours}")
        if not hours:
            print("‚õî Ïú†Ìö®Ìïú STAT_HOURÍ∞Ä ÏóÜÏäµÎãàÎã§.")
            return {f"{h:02d}": [] for h in range(24)}, {}, None  # type: ignore[return-value]

        # ---------- 1) Ï°∞Ìöå ÏùºÏûê ÌôïÏ†ï ----------
        day: str = hours[0][:8]
        if not re.fullmatch(r"\d{8}", day):
            raise ValueError(f"STAT_DAY ÌòïÏãù Ïò§Î•ò: {repr(day)}")
        print(f">>>>> ‚úÖ Ï°∞Ìöå ÏùºÏûê: {day}")

        # ---------- 2) ÏãúÍ∞ÑÎåÄÎ≥Ñ Ï°∞Ìöå ----------
        ph_hours = ", ".join(["?"] * len(hours))
        sql_hour = f"""
            SELECT *
            FROM TOMMS.STAT_HOUR_CROSS
            WHERE STAT_HOUR IN ({ph_hours})
            AND TRIM(UPPER(INFRA_TYPE)) = 'SMT'
        """
        print(f"üîé sql_hour params: {[repr(h) for h in hours]}")
        rows_hour, cols_hour = self._exec(sql_hour, tuple(hours))
        print(f">>>>> ‚úÖ ÏãúÍ∞ÑÎåÄÎ≥Ñ Ï°∞Ìöå ÌñâÏàò: {len(rows_hour)}")

        # Ïª¨Îüº Ïù∏Îç±Ïä§ Îß§Ìïë(ÏãúÍ∞Ñ)
        col_idx_h = {c: i for i, c in enumerate(cols_hour)}
        idx_stat_hour = col_idx_h["STAT_HOUR"]
        idx_cross_id_h = col_idx_h["CROSS_ID"]

        vol_names_h = extract_vol_columns(cols_hour)
        vol_idx_pairs_h = [(name, col_idx_h[name]) for name in vol_names_h]

        traffic_data_by_hour: Dict[str, List[dict]] = {f"{h:02d}": [] for h in range(24)}
        for r in rows_hour:
            stat_hour = str(r[idx_stat_hour]).strip()
            hh = stat_hour[-2:]
            if hh in traffic_data_by_hour:
                cross_id = str(r[idx_cross_id_h]).strip()
                traffic_data_by_hour[hh].append({
                    "cross_id": cross_id,
                    "data": [
                        {"direction": name, "value": to_py(r[idx])}
                        for name, idx in vol_idx_pairs_h
                        if to_py(r[idx]) is not None
                    ]
                })
        print(">>>>> ‚úÖ ÏãúÍ∞ÑÎåÄÎ≥Ñ Í∞ÄÍ≥µ ÏôÑÎ£å.")

        # ---------- 3) ÏùºÎ≥Ñ Ï°∞Ìöå (Í≤ÄÏ¶ùÎêú Î¶¨ÌÑ∞Îü¥, fallback ÏóÜÏùå) ----------
        traffic_data_by_day: Dict[str, List[dict]] = {day: []}

        sql_day = f"""
            SELECT *
            FROM TOMMS.STAT_DAY_CROSS
            WHERE STAT_DAY = '{day}'
            AND TRIM(UPPER(INFRA_TYPE)) = 'SMT'
        """
        print(f"üîé sql_day(literal): {sql_day.strip()}")
        rows_day, cols_day = self._exec(sql_day)
        print(f">>>>> ‚úÖ ÏùºÎ≥Ñ Ï°∞Ìöå ÌñâÏàò: {len(rows_day)}")

        if rows_day:
            col_idx_d = {c: i for i, c in enumerate(cols_day)}
            idx_cross_id_d = col_idx_d["CROSS_ID"]
            vol_names_d = extract_vol_columns(cols_day)
            vol_idx_pairs_d = [(name, col_idx_d[name]) for name in vol_names_d]

            for r in rows_day:
                cross_id = str(r[idx_cross_id_d]).strip()
                traffic_data_by_day[day].append({
                    "cross_id": cross_id,
                    "data": [
                        {"direction": name, "value": to_py(r[idx])}
                        for name, idx in vol_idx_pairs_d
                        if to_py(r[idx]) is not None
                    ]
                })
        else:
            # ÏßÑÎã®Îßå ÎÇ®ÍπÄ
            cnt_sql = f"""
                SELECT COUNT(*) AS CNT
                FROM TOMMS.STAT_DAY_CROSS
                WHERE STAT_DAY = '{day}'
                AND TRIM(UPPER(INFRA_TYPE)) = 'SMT'
            """
            cnt_rows, _ = self._exec(cnt_sql)
            cnt = cnt_rows[0][0] if cnt_rows else 0
            print(f"‚õî ÏùºÎ≥Ñ 0Í±¥(Ïπ¥Ïö¥Ìä∏={cnt}). fallback ÎØ∏ÏÇ¨Ïö©.")

        # ---------- 4) ÏöîÏïΩ Î∞è Î∞òÌôò ----------
        total_day = len(traffic_data_by_day.get(day, []))
        print(">>>>> ‚úÖ Ï†ÑÏùº ÍµêÌÜµÎüâ Í∞ÄÍ≥µ ÏôÑÎ£å.")
        print(f"      ‚úÖ ÏùºÎ≥Ñ Ï¥ù {total_day}Í±¥ / ÏùºÏàò=1")
        for hh, lst in traffic_data_by_hour.items():
            if lst:
                print(f"      ‚úÖ ÏãúÍ∞ÑÎåÄ {hh}Ïãú : {len(lst)}Í±¥")

        return traffic_data_by_hour, traffic_data_by_day, day

    def close(self):
        try:
            if self.cursor:
                self.cursor.close()
                self.cursor = None
            if self.conn:
                self.conn.close()
                self.conn = None
            print(">>>>> ‚úÖ DB Ïó∞Í≤∞ÏùÑ Ï¢ÖÎ£åÌï©ÎãàÎã§.")
        except Exception as e:
            print("‚õî DB Ï¢ÖÎ£å Ï§ë ÏóêÎü¨Í∞Ä Î∞úÏÉùÌï©ÎãàÎã§. ", e)








# ================================================== [ VISUM ]

class VisumSimulationManager:
    
    # --------------------------------------------------------------- [ ÏãúÍ∞ÑÎåÄÎ≥Ñ Active Î≤àÌò∏ (Í≥†Ï†ï ÏãúÎÇòÎ¶¨Ïò§) ]
    
    HOUR_TO_PROC = {
        8: 312,
        11: 408,
        14: 505,
        17: 601,
    }

    # --------------------------------------------------------------- [ init ]

    def __init__(
        self,
        base_path: str,
        default_version_name: str,
        prev_day_proc_no: int = 22,
        csv_out_dir: str = r"C:/Digital Twin Simulation network/VISUM/result_export",
        table_map: dict = None,     # {"prev_day": "TABLE_A", "hourly": "TABLE_B"}
    ):
        self.base_path = base_path
        self.default_version_name = default_version_name
        self.prev_day_proc_no = prev_day_proc_no
        self.csv_out_dir = csv_out_dir
        os.makedirs(self.csv_out_dir, exist_ok=True)

        self.table_map = table_map or {
            "prev_day": "LINK_RESULT_DAY",
            "hourly": "LINK_RESULT_HOUR",
        }

        self.visum = None
        # last_run: {"type": "prev_day"|"hourly"|None, "hour": "00"~"23"|None, "stat_day": "YYYYMMDD"|None}
        self.last_run = {"type": None, "hour": None, "stat_day": None}











    # --------------------------------------------------------------- [ Í∏∞Ï§ÄÏùº Í¥ÄÎ¶¨ ]
    
    def set_stat_day(self, yyyymmdd: str):
        if not (isinstance(yyyymmdd, str) and len(yyyymmdd) == 8 and yyyymmdd.isdigit()):
            raise ValueError("STAT_DAY must be 'YYYYMMDD' string.")
        self.last_run["stat_day"] = yyyymmdd
        print(f">>>>> ‚úÖ VISUM set_stat_day Ìï®ÏàòÏóêÏÑú Í±¥ÎÑ§Î∞õÏùÄ ÏùºÏûêÎäî [ {self.last_run} ] ÏûÖÎãàÎã§.")

    def ensure_stat_day(self, preferred_day: str | None, payload_days: list[str] | None = None) -> str:
        """
        Í∏∞Ï§ÄÏùºÏùÑ Îã® Ìïú Î≤àÎßå ÌôïÏ†ï:
        1) preferred_dayÍ∞Ä Ïú†Ìö®ÌïòÎ©¥ Í∑∏Í±∏ ÏÇ¨Ïö©
        2) ÏóÜÏúºÎ©¥ payload_daysÏóêÏÑú ÏµúÏã†ÏùºÏûêÎ•º ÏÑ†ÌÉù
        Ïù¥ÌõÑ self.last_run['stat_day']Î•º ÏÑ§Ï†ïÌïòÍ≥†, Ïù¥ÌõÑÏóêÎäî Ï†àÎåÄ Î∞îÍæ∏ÏßÄ ÏïäÏùå
        """
        if self.last_run.get("stat_day"):
            return self.last_run["stat_day"]

        cand = None
        if isinstance(preferred_day, str) and len(preferred_day) == 8 and preferred_day.isdigit():
            cand = preferred_day
        elif payload_days:
            # payload_daysÍ∞Ä Î¨∏ÏûêÏó¥ 'YYYYMMDD' Î¶¨Ïä§Ìä∏ÎùºÍ≥† Í∞ÄÏ†ï
            cand = max([d for d in payload_days if isinstance(d, str) and len(d) == 8 and d.isdigit()], default=None)

        if not cand:
            raise ValueError("‚õî Í∏∞Ï§ÄÏùº(STAT_DAY)ÏùÑ ÌôïÏ†ïÌï† Ïàò ÏóÜÏäµÎãàÎã§. preferred_day ÎòêÎäî payload_daysÎ•º Ï†úÍ≥µÌïòÏÑ∏Ïöî.")

        self.last_run["stat_day"] = cand
        return cand

    def _require_stat_day(self) -> str:
        sd = self.last_run.get("stat_day")
        if not (isinstance(sd, str) and len(sd) == 8 and sd.isdigit()):
            raise ValueError("‚õî STAT_DAY(yyyymmdd)Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. vis.set_stat_day('YYYYMMDD') ÎòêÎäî ensure_stat_day()Î•º Î®ºÏ†Ä Ìò∏Ï∂úÌïòÏÑ∏Ïöî.")
        return sd

    def _update_last_run(self, run_type: str, hour_label: str | None):
        sd = self.last_run.get("stat_day")  # Î≥¥Ï°¥
        self.last_run = {"type": run_type, "hour": hour_label, "stat_day": sd}

    # --------------------------------------------------------------- [ VISUM LOAD & CLOSE ]
    
    def open(self, filename: str = None):
        self.visum = com.Dispatch("Visum.Visum.22")

        ver_filename = filename or self.default_version_name
        ver_path = os.path.join(self.base_path, ver_filename)
        if not os.path.isfile(ver_path):
            print(f"‚õî VISUM ÎÑ§Ìä∏ÏõåÌÅ¨ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {ver_path}")
            self.visum = None
            return

        self.visum.LoadVersion(ver_path)
        print(f">>>>> ‚úÖ VISUM ÎÑ§Ìä∏ÏõåÌÅ¨ Î°úÎìú ÏôÑÎ£å: {ver_path}")

    def close(self):
        if self.visum:
            self.visum = None
            print(">>>>> ‚úÖ VISUM ÏÑ∏ÏÖò Ï¢ÖÎ£å")











    # --------------------------------------------------------------- [ ÍµêÌÜµÎüâ Ïó∞Í≥Ñ(Í≥µÌÜµ) ]
    
    def insert_turn_volumes(self, data_list, verbose: bool = False) -> int:
        if not self.visum:
            print("‚õî Visum Í∞ùÏ≤¥Í∞Ä ÏóÜÏäµÎãàÎã§.")
            return 0
        
        print(">>>>> ‚úÖ Ï°∞ÌöåÎêú ÍµêÌÜµÎüâ Îç∞Ïù¥ÌÑ∞Ïùò VISUM Ïó∞Í≥ÑÎ•º ÏãúÏûëÌï©ÎãàÎã§.")

        turns = self.visum.Net.Turns
        gn_node_list = [int(x[1]) if x[1] is not None else 0 for x in turns.GetMultiAttValues("gn_node_id")]
        gn_dir_list  = [int(x[1]) if x[1] is not None else 0 for x in turns.GetMultiAttValues("gn_direction_id")]
        from_list    = [int(x[1]) if x[1] is not None else 0 for x in turns.GetMultiAttValues("FromNodeNo")]
        via_list     = [int(x[1]) if x[1] is not None else 0 for x in turns.GetMultiAttValues("ViaNodeNo")]
        to_list      = [int(x[1]) if x[1] is not None else 0 for x in turns.GetMultiAttValues("ToNodeNo")]

        key2nodes = {
            (gn_node_list[i], gn_dir_list[i]): (from_list[i], via_list[i], to_list[i])
            for i in range(len(gn_node_list))
            if gn_node_list[i] and gn_dir_list[i]
        }

        updates = 0
        for item in (data_list or []):
            try:
                cross_id = int(item.get("cross_id"))
            except Exception:
                if verbose:
                    print(f"‚õî cross_id ÌååÏã± Ïã§Ìå®: {item}")
                continue

            for d in item.get("data", []):
                dir_str = d.get("direction")
                val = d.get("value")
                if val is None or not dir_str:
                    continue

                try:
                    direction_num = int(dir_str.split("_")[-1])
                except Exception:
                    if verbose:
                        print(f"‚õî direction ÌååÏã± Ïã§Ìå® ‚Äî cross_id={cross_id}, direction={dir_str}")
                    continue

                nodes = key2nodes.get((cross_id, direction_num))
                if not nodes:
                    if verbose:
                        print(f"‚õî Ïó∞Í≥Ñ Ïã§Ìå® >>> CROSS_ID={cross_id}, DIR={direction_num:02d} (Turn ÎØ∏Ï°¥Ïû¨)")
                    continue

                f, v, t = nodes
                try:
                    turn = turns.ItemByKey(f, v, t)
                    for att in ("ABT_TOTAL1", "ABT_TOTAL2", "TOTAL_TRAFFIC_VOL"):
                        turn.SetAttValue(att, val)
                    updates += 1
                except Exception as e:
                    print(f"‚õî SetAttValue Ïã§Ìå® ‚Äî nodes=({f},{v},{t}), err={e}")

        return updates

    # --------------------------------------------------------------- [ ÏãúÎÆ¨Î†àÏù¥ÏÖò Ïã§Ìñâ(Í≥µÌÜµ) ]
    
    def _execute_procedure(self, proc_no: int):
        ops = self.visum.Procedures.Operations
        try:
            print(f">>>>> ‚úÖ Procedure Sequence Set Active Number : {proc_no}")
            ops.ItemByKey(proc_no).SetAttValue("Active", 1)
            self.visum.Procedures.Execute()
        finally:
            try:
                ops.ItemByKey(proc_no).SetAttValue("Active", 0)
            except Exception:
                pass

    def simulate_prev_day(self):
        self._require_stat_day()
        self._execute_procedure(self.prev_day_proc_no)
        self._update_last_run("prev_day", None)
        print(f"      ‚úÖ Ï†ÑÏùº ÏãúÎÆ¨Î†àÏù¥ÏÖò ÏôÑÎ£å (Active={self.prev_day_proc_no})")

    def simulate_hour(self, hour: int):
        self._require_stat_day()
        proc = self.HOUR_TO_PROC.get(int(hour))
        if proc is None:
            print(f"‚õî {hour}Ïãú ÌîÑÎ°úÏãúÏ†Ä Î≤àÌò∏Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
            return
        self._execute_procedure(proc)
        self._update_last_run("hourly", f"{int(hour):02d}")
        print(f">>>>> ‚úÖ {int(hour):02d}Ïãú ÏãúÎÆ¨Î†àÏù¥ÏÖò ÏôÑÎ£å (Active={proc})")











    # --------------------------------------------------------------- [ ÎßÅÌÅ¨ Í≤∞Í≥ºÍ∞í Ï∂îÏ∂ú ]
    
    def get_links_result_df(self) -> pd.DataFrame:
        """
        VisumÏóêÏÑú ÎßÅÌÅ¨ Í≤∞Í≥ºÍ∞íÏùÑ Ï∂îÏ∂úÌï¥ DataFrameÏúºÎ°ú Î∞òÌôò.
        - Ï∂îÏ∂ú Ïª¨Îüº: LINK_ID, vc(VolCapRatioPrT(AP)), vehs(VolVehPrT(AP)), speed(TCur_PrTSys(a))
        - LINK_IDÍ∞Ä 'A, B, C' Í∞ôÏù¥ ÏâºÌëúÎ°ú Ìï©Ï≥êÏßÑ Í≤ΩÏö∞ Í∞úÎ≥Ñ ÌñâÏúºÎ°ú Î∂ÑÎ¶¨ÌïòÏó¨ ÎèôÏùºÌïú vc/vehs/speed Î≥µÏ†ú
        - LINK_IDÎäî VARCHAR(10)Î°ú Í∞ÄÍ≥µ(Ìä∏Î¶º ÌõÑ ÏµúÎåÄ 10Ïûê, ÎπàÎ¨∏Ïûê Ï†úÏô∏)
        - STAT_DAY/STAT_HOURÎäî Ïó¨Í∏∞ÏÑú ÏÑ∏ÌåÖÌïòÏßÄ ÏïäÏùå(Í∞Å INSERT Ìï®ÏàòÏóêÏÑú Ï≤òÎ¶¨)
        """
        if not self.visum:
            print("‚õî Visum Í∞ùÏ≤¥Í∞Ä ÏóÜÏäµÎãàÎã§.")
            return pd.DataFrame(columns=["LINK_ID", "vc", "vehs", "speed"])

        run_type = (self.last_run or {}).get("type")
        if run_type not in ("prev_day", "hourly"):
            print("‚õî Ïã§Ìñâ Ïù¥Î†• ÏóÜÏùå ‚Äî simulate Ìò∏Ï∂ú ÌõÑ ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.")
            return pd.DataFrame(columns=["LINK_ID", "vc", "vehs", "speed"])

        # ‚ñ∂ ÌïÑÏöîÌïú ÏÜçÏÑ±Îßå Ï∂îÏ∂ú
        attrs = [
            "LINK_ID",
            "VolCapRatioPrT(AP)",   # ‚Üí vc
            "VolVehPrT(AP)",        # ‚Üí vehs
            "TCur_PrTSys(a)",       # ‚Üí speed
        ]

        rows = self.visum.Net.Links.GetMultipleAttributes(attrs, True)

        records, truncated_ids, empty_ids = [], set(), 0

        for row in rows:
            base = dict(zip(attrs, row))
            raw_id = base.get("LINK_ID")

            if not raw_id:               # None / ÎπàÎ¨∏Ïûê ‚Üí Ïä§ÌÇµ Ïπ¥Ïö¥Ìä∏
                empty_ids += 1
                continue

            # 1) ÏâºÌëú Í∏∞Ï§Ä Î∂ÑÎ¶¨(Í≥µÎ∞± Ìä∏Î¶º) ‚Üí Îπà ÌÜ†ÌÅ∞ Ï†úÍ±∞
            link_ids = [tok.strip() for tok in str(raw_id).split(",") if tok.strip()]

            # 2) Í∞Å LINK_ID Î≥ÑÎ°ú Ìïú ÌñâÏî© Î≥µÏ†ú (vc/vehs/speed ÎèôÏùº Î≥µÏ†ú)
            for lid in link_ids:
                lid_trim = lid[:10]      # VARCHAR(10) Î≥¥Ïû•
                if len(lid) > 10:
                    truncated_ids.add(lid)

                rec = {
                    "LINK_ID": lid_trim,
                    "vc":    base.get("VolCapRatioPrT(AP)"),
                    "vehs":  base.get("VolVehPrT(AP)"),
                    "speed": base.get("TCur_PrTSys(a)"),
                }
                records.append(rec)

        # Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ Íµ¨ÏÑ±
        df = pd.DataFrame.from_records(records, columns=["LINK_ID", "vc", "vehs", "speed"])

        if df.empty:
            print(f"      üìä DataFrame ÌÅ¨Í∏∞: 0 Ìñâ √ó 4 Ïó¥ (Îπà LINK_ID {empty_ids}Í±¥)")
            if truncated_ids:
                ex = list(sorted(truncated_ids))[:5]
                print(f"‚ö†Ô∏è 10Ïûê Ï¥àÍ≥º LINK_ID {len(truncated_ids)}Í±¥(ÏòàÏãú ÏµúÎåÄ 5Í∞ú): {ex}")
            return df

        # Ïà´ÏûêÌòï Î≥¥Ï†ï
        df["vc"]    = pd.to_numeric(df["vc"], errors="coerce")        # float
        df["vehs"]  = pd.to_numeric(df["vehs"], errors="coerce").fillna(0).astype(int)  # int
        df["speed"] = pd.to_numeric(df["speed"], errors="coerce")      # float

        # LINK_ID Î¨∏ÏûêÏó¥ Î≥¥Ï†ï(Í≥µÎ∞±/ÎπàÎ¨∏Ïûê Ï†úÍ±∞ ÌõÑ NoneÏùÄ ÎìúÎûç)
        df["LINK_ID"] = df["LINK_ID"].astype(str).str.strip()
        df = df[df["LINK_ID"] != ""]

        # LINK_ID Í∏∞Ï§Ä Ï§ëÎ≥µ Ï†úÍ±∞ (Ï≤´ Îì±Ïû• Ïö∞ÏÑ†)
        before = len(df)
        df.drop_duplicates(subset=["LINK_ID"], keep="first", inplace=True)
        after = len(df)

        # Ï†ïÎ†¨
        df.sort_values(by=["LINK_ID"], inplace=True, ignore_index=True)

        print(
            f"      üìä DataFrame ÌÅ¨Í∏∞: {len(df)} Ìñâ √ó {len(df.columns)} Ïó¥"
            f" (Ï§ëÎ≥µ Ï†úÍ±∞: {before - after}Í±¥, Îπà LINK_ID: {empty_ids}Í±¥)"
        )
        if truncated_ids:
            ex = list(sorted(truncated_ids))[:5]
            print(f"‚ö†Ô∏è 10Ïûê Ï¥àÍ≥º LINK_ID {len(truncated_ids)}Í±¥(ÏòàÏãú ÏµúÎåÄ 5Í∞ú): {ex}")

        return df

    # --------------------------------------------------------------- [ CSV/DB I/O ]
    
    def export_csv(self, df: pd.DataFrame, run_type: str, hour: str | None) -> str:
        if df.empty:
            print("‚õî CSV ÎÇ¥Î≥¥ÎÇº Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.")
            return ""
        os.makedirs(self.csv_out_dir, exist_ok=True)  # ÎîîÎ†âÌÑ∞Î¶¨ Î≥¥Ïû•

        sd = self._require_stat_day()
        ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        kind = "prevday" if run_type == "prev_day" else f"h{hour}"
        fname = f"links_{sd}_{kind}_{ts}.csv"
        fpath = os.path.join(self.csv_out_dir, fname)

        # ÏóëÏÖÄ Í≤ÄÌÜ† Ìé∏Ïùò ÏúÑÌï¥ na_rep ÏßÄÏ†ï(ÏÑ†ÌÉù)
        df.to_csv(fpath, index=False, encoding="utf-8-sig", na_rep="")
        print(f">>>>> ‚úÖ Í≤∞Í≥ºÍ∞íÏùÑ CSVÌååÏùºÎ°ú Ï†ÄÏû•ÏùÑ ÏôÑÎ£åÌïòÏòÄÏäµÎãàÎã§. Í≤ΩÎ°ú: {fpath}")
        return fpath
    
    def _coerce_numeric(self, df: pd.DataFrame) -> pd.DataFrame:
        out = df.copy()
        if "DISTRICT" in out.columns:
            out["DISTRICT"] = pd.to_numeric(out["DISTRICT"], errors="coerce")
        if "UPDOWN" in out.columns:
            out["UPDOWN"] = pd.to_numeric(out["UPDOWN"], errors="coerce")
        if "vc" in out.columns:
            out["vc"] = pd.to_numeric(out["vc"], errors="coerce").round(2)
        if "vehs" in out.columns:
            out["vehs"] = pd.to_numeric(out["vehs"], errors="coerce").fillna(0).astype(int)
        if "speed" in out.columns:
            out["speed"] = pd.to_numeric(out["speed"], errors="coerce").round(2)
        return out

    def _rename_for_db(self, df: pd.DataFrame) -> pd.DataFrame:
        out = df.copy()
        out.rename(columns={"sa": "SA_NO", "vc": "VC", "vehs": "VEHS", "speed": "SPEED"}, inplace=True)
        return out

    def _coerce_str_fields(self, df: pd.DataFrame, fields=("LINK_ID","SA_NO","ROAD_NAME")) -> pd.DataFrame:
        out = df.copy()
        for c in fields:
            if c in out.columns:
                out[c] = out[c].astype(object).where(pd.notna(out[c]), None)
                out[c] = out[c].map(lambda x: str(x) if x is not None else None)
        return out









    def read_gpa_file_get_road_link(self, db_conn, gpa_file_path: str, stat_hour: str):
        """
            - TDA_ROAD_VOL_INFO.ROAD_ID Ï†ÑÏ≤¥ Ï°∞Ìöå
            - gpa_file_path/{ROAD_ID}.gpa Ï†ÅÏö©
            - LinksÏóêÏÑú ['LINK_ID','Ï†ÑÏùº_Ïö©Îüâ']Îßå Ï∂îÏ∂ú ‚Üí ÏâºÌëú Î∂ÑÎ¶¨/10Ïûê Ï†úÌïú/ÎπàÍ∞í Ï†úÍ±∞
            - Í∞Å ROAD_IDÎßàÎã§ df ÏÉùÏÑ±, df['STAT_HOUR']=stat_hour ÏÑ∏ÌåÖ
            - Í≥ßÎ∞îÎ°ú insert_hour_road_results(df, db_conn, stat_hour, road_id) Ìò∏Ï∂ú (Î≥∏Î¨∏ÏùÄ Ïù¥ÌõÑ Íµ¨ÌòÑ)
        """
        if not hasattr(self, "visum") or self.visum is None:
            print("‚õî Visum Í∞ùÏ≤¥Í∞Ä ÏóÜÏäµÎãàÎã§. GPA Ï†ÅÏö© Ïä§ÌÇµ")
            return {}

        road_results: dict[str, pd.DataFrame] = {}

        try:
            cur = db_conn.cursor()
            cur.execute("SELECT ROAD_ID FROM TOMMS.TDA_ROAD_VOL_INFO")
            road_ids = [str(r[0]).strip() for r in cur.fetchall() if r and r[0] is not None]
            road_ids = sorted(set(road_ids))
            print(f"üîé GPA ÎåÄÏÉÅ ROAD_ID {len(road_ids)}Í±¥")

            missing, failed, applied = [], [], 0

            for rid in road_ids:
                gpa_file = os.path.join(gpa_file_path, f"{rid}.gpa")  # ROAD_IDÍ∞Ä Í≥ß ÌååÏùºÎ™Ö
                if not os.path.isabs(gpa_file):
                    gpa_file = os.path.abspath(gpa_file)

                if not os.path.isfile(gpa_file):
                    missing.append(gpa_file)
                    continue

                try:
                    # 1) GPA Ï†ÅÏö©
                    self.visum.Net.GraphicParameters.Open(gpa_file)
                    applied += 1

                    # 2) ÎßÅÌÅ¨ ÏÜçÏÑ± Ï∂îÏ∂ú
                    attrs = ["LINK_ID", "Ï†ÑÏùº_Ïö©Îüâ"]
                    rows = self.visum.Net.Links.GetMultipleAttributes(attrs, True)

                    # 3) Í∞ÄÍ≥µ: LINK_ID Î∂ÑÎ¶¨/Ï†ïÎ¶¨
                    records, truncated_ids, empty_ids = [], set(), 0
                    for row in rows:
                        base = dict(zip(attrs, row))
                        raw_id = base.get("LINK_ID")
                        if not raw_id:
                            empty_ids += 1
                            continue

                        link_ids = [tok.strip() for tok in str(raw_id).split(",") if tok.strip()]
                        for lid in link_ids:
                            lid_trim = lid[:10]
                            if len(lid) > 10:
                                truncated_ids.add(lid)
                            records.append({
                                "ROAD_ID": rid,
                                "LINK_ID": lid_trim,
                                "Ï†ÑÏùº_Ïö©Îüâ": base.get("Ï†ÑÏùº_Ïö©Îüâ"),
                            })

                    df = pd.DataFrame.from_records(records, columns=["ROAD_ID", "LINK_ID", "Ï†ÑÏùº_Ïö©Îüâ"])
                    if df.empty:
                        print(f"      üìä ROAD_ID={rid} ‚Üí 0Ìñâ (Îπà LINK_ID {empty_ids}Í±¥)")
                        if truncated_ids:
                            ex = list(sorted(truncated_ids))[:5]
                            print(f"      ‚ö†Ô∏è 10Ïûê Ï¥àÍ≥º LINK_ID {len(truncated_ids)}Í±¥(ÏòàÏãú‚â§5): {ex}")
                        road_results[rid] = df
                        continue

                    # Ïà´Ïûê/Î¨∏Ïûê Î≥¥Ï†ï
                    df["Ï†ÑÏùº_Ïö©Îüâ"] = pd.to_numeric(df["Ï†ÑÏùº_Ïö©Îüâ"], errors="coerce")
                    df["LINK_ID"] = df["LINK_ID"].astype(str).str.strip()
                    df = df[df["LINK_ID"] != ""]

                    # Ï§ëÎ≥µ Ï†úÍ±∞/Ï†ïÎ†¨
                    before = len(df)
                    df.drop_duplicates(subset=["LINK_ID"], keep="first", inplace=True)
                    after = len(df)
                    df.sort_values(by=["LINK_ID"], inplace=True, ignore_index=True)

                    # üîµ STAT_HOUR ÏÑ∏ÌåÖ
                    df["STAT_HOUR"] = stat_hour

                    print(f"      ‚úÖ ROAD_ID={rid} Í≤∞Í≥º: {len(df)}Ìñâ (Ï§ëÎ≥µ {before - after}Í±¥, Îπà LINK_ID {empty_ids}Í±¥)")
                    if truncated_ids:
                        ex = list(sorted(truncated_ids))[:5]
                        print(f"      ‚ö†Ô∏è 10Ïûê Ï¥àÍ≥º LINK_ID {len(truncated_ids)}Í±¥(ÏòàÏãú‚â§5): {ex}")

                    road_results[rid] = df

                    # üëâ Ïó¨Í∏∞ÏÑú Î∞îÎ°ú ÏãúÍ∞ÑÎåÄ road Í≤∞Í≥º INSERT Ìò∏Ï∂ú(Î≥∏Î¨∏ÏùÄ ÎÇòÏ§ë Íµ¨ÌòÑ)
                    self.insert_hour_road_results(df, db_conn=db_conn, stat_hour=stat_hour, road_id=rid)

                except Exception as e:
                    failed.append((gpa_file, str(e)))

            print(f"üñºÔ∏è GPA Ï†ÅÏö© ÏôÑÎ£å ‚Äî ÏÑ±Í≥µ {applied}Í±¥ / ÎØ∏Ï°¥Ïû¨ {len(missing)}Í±¥ / Ïã§Ìå® {len(failed)}Í±¥")
            if missing:
                os.makedirs("./output", exist_ok=True)
                with open("./output/missing_gpa_files.txt", "w", encoding="utf-8") as f:
                    for p in missing:
                        f.write(p + "\n")
                print("üìÇ ÎØ∏Ï°¥Ïû¨ GPA ÌååÏùº Î™©Î°ù Ï†ÄÏû•: ./output/missing_gpa_files.txt")
            if failed:
                with open("./output/failed_gpa_files.txt", "w", encoding="utf-8") as f:
                    for p, msg in failed:
                        f.write(f"{p}\t{msg}\n")
                print("üìÇ Ïã§Ìå® GPA ÌååÏùº Î™©Î°ù Ï†ÄÏû•: ./output/failed_gpa_files.txt")

            return road_results

        except Exception as ex:
            print(f"‚õî GPA Ï†ÅÏö© Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò: {ex}")
            return {}

    # --------------------------------------------------------------- [ Ï†ÑÏùº Í≤∞Í≥ºÍ∞í insert ]

    def insert_day_link_results(self, df: pd.DataFrame, db_conn, chunk_size: int = 20000) -> int:
        if df is None or df.empty or db_conn is None:
            print("‚õî Ï†ÑÏùº INSERT: DF/DB ÎàÑÎùΩ")
            return 0

        # ÏµúÏ¢Ö Ïä§ÌÇ§Îßà(ÏàúÏÑú Í≥†Ï†ï)
        required = ["STAT_DAY", "LINK_ID", "VC", "VEHS", "SPEED"]

        # 1) Ï§ÄÎπÑ: Î∂àÌïÑÏöî Ïª¨Îüº Ï†úÍ±∞
        work = df.copy()
        for c in ("run_type", "hour"):
            if c in work.columns:
                work.drop(columns=[c], inplace=True)

        # 2) Ïª¨ÎüºÎ™Ö ÌÜµÏùº + STAT_DAY ÏÑ∏ÌåÖ
        rename_map = {"vc": "VC", "vehs": "VEHS", "speed": "SPEED", "link_id": "LINK_ID"}
        work.rename(columns={k: v for k, v in rename_map.items() if k in work.columns}, inplace=True)
        stat_day = self._require_stat_day()  # 'YYYYMMDD'
        work["STAT_DAY"] = stat_day

        # 3) ÌÉÄÏûÖ Î≥¥Ï†ï
        if "LINK_ID" in work.columns:
            work["LINK_ID"] = (
                work["LINK_ID"].astype(str).str.strip()
                .map(lambda x: x if x != "" else None)
                .map(lambda x: x[:10] if x is not None else None)
            )
        if "VC" in work.columns:
            work["VC"] = pd.to_numeric(work["VC"], errors="coerce")
        if "VEHS" in work.columns:
            work["VEHS"] = pd.to_numeric(work["VEHS"], errors="coerce").astype("Int64")
        if "SPEED" in work.columns:
            work["SPEED"] = pd.to_numeric(work["SPEED"], errors="coerce")
            work.loc[work["SPEED"] >= 360000, "SPEED"] = 0  # Ïù¥ÏÉÅÏπò Ïª∑(ÏòµÏÖò)

        # 4) ÌïÑÏàò Ïª¨Îüº Ï≤¥ÌÅ¨ + Ï†ïÎ†¨
        missing = [c for c in required if c not in work.columns]
        if missing:
            print(f"‚õî ÌïÑÏàò Ïª¨Îüº ÎàÑÎùΩ: {missing}")
            return 0
        work = work[required]

        # 5) NULL LINK_ID Ï†úÍ±∞
        before = len(work)
        work = work[work["LINK_ID"].notna()]
        if before != len(work):
            print(f"‚ö†Ô∏è LINK_ID NULL {before - len(work)}Ìñâ Ï†úÍ±∞")

        # 6) FK ÏÇ¨Ï†ÑÍ≤ÄÏÇ¨: TDA_LINK_INFOÏóê Ï°¥Ïû¨ÌïòÎäî LINK_IDÎßå ÎÇ®ÍπÄ
        cur = db_conn.cursor()
        unique_ids = sorted(set(work["LINK_ID"].tolist()))
        valid_ids = set()
        if unique_ids:
            BATCH = 900  # placeholder Ï†úÌïú ÎåÄÎπÑ
            for i in range(0, len(unique_ids), BATCH):
                batch = unique_ids[i:i + BATCH]
                placeholders = ", ".join(["?"] * len(batch))
                sql_chk = f"SELECT LINK_ID FROM TOMMS.TDA_LINK_INFO WHERE LINK_ID IN ({placeholders})"
                cur.execute(sql_chk, batch)
                valid_ids.update(r[0] for r in cur.fetchall())

        missing_ids = sorted(set(unique_ids) - valid_ids)
        if missing_ids:
            print(f"‚ö†Ô∏è FK ÎØ∏Ï°¥Ïû¨ LINK_ID {len(missing_ids)}Í±¥ ‚Äî INSERT Ï†úÏô∏ (ÏòàÏãú 10Í∞ú): {missing_ids[:10]}")
            os.makedirs("./output", exist_ok=True)
            miss_path = f"./output/missing_day_link_ids_{stat_day}.txt"
            with open(miss_path, "w", encoding="utf-8") as f:
                for lid in missing_ids:
                    f.write(str(lid) + "\n")
            print(f"üìÇ ÎØ∏Ï°¥Ïû¨ LINK_ID Î™©Î°ù Ï†ÄÏû•: {miss_path}")

        work = work[work["LINK_ID"].isin(valid_ids)]
        if work.empty:
            print("‚õî Ïú†Ìö® LINK_IDÍ∞Ä ÏóÜÏñ¥ INSERT Ïä§ÌÇµ")
            return 0

        # 7) DB Î∞îÏù∏Îî© Í∞í Î≥ÄÌôò
        def _to_db_value(v):
            if v is pd.NA:
                return None
            if isinstance(v, np.generic):
                v = v.item()
            if isinstance(v, float) and (pd.isna(v) or np.isnan(v)):
                return None
            return v

        # 8) INSERT Ï§ÄÎπÑ
        sql = f"INSERT INTO TOMMS.TDA_LINK_DAY_RESULT ({', '.join(required)}) VALUES ({', '.join(['?']*len(required))})"
        cur.fast_executemany = True  # Î∞∞Ïπò ÏÑ±Îä•

        # 9) SQL Î°úÍ∑∏ Ï†ÄÏû•(Í≤ÄÏ¶ùÏö©)
        os.makedirs("./output", exist_ok=True)
        sql_log_path = f"./output/day_link_result_insert_{stat_day}.sql.txt"

        def _sql_literal(v):
            if v is None:
                return "NULL"
            if isinstance(v, (int, np.integer)):
                return str(int(v))
            if isinstance(v, (float, np.floating)):
                return str(float(v))
            s = str(v).replace("'", "''")
            return f"'{s}'"

        total = 0
        try:
            with open(sql_log_path, "w", encoding="utf-8") as f_log:
                for s in range(0, len(work), chunk_size):
                    chunk = work.iloc[s:s+chunk_size]
                    data = [tuple(_to_db_value(v) for v in row) for row in chunk.itertuples(index=False, name=None)]

                    # Î°úÍ∑∏Ïö© Ïã§Ï†ú SQL
                    for row in data:
                        values_str = [_sql_literal(v) for v in row]
                        f_log.write(
                            f"INSERT INTO TOMMS.TDA_LINK_DAY_RESULT ({', '.join(required)}) VALUES ({', '.join(values_str)});\n"
                        )

                    cur.executemany(sql, data)
                    total += len(data)

            db_conn.commit()
            print(f"      ‚úÖ TDA_LINK_DAY_RESULT INSERT ‚Äî {total}Ìñâ (STAT_DAY={stat_day})")
            print(f"      üìÇ SQL Î°úÍ∑∏ Ï†ÄÏû•: {sql_log_path}")

            return total

        except Exception as ex:
            db_conn.rollback()
            print(f"‚õî TDA_LINK_DAY_RESULT INSERT Ïò§Î•ò ‚Äî Î°§Î∞±: {ex}")
            print(f"üìÇ SQL Î°úÍ∑∏(Ïã§Ìå® ÏãúÏ†êÍπåÏßÄ): {sql_log_path}")
            return 0
    
    # --------------------------------------------------------------- [ Ï†ÑÏùº ÏãúÎÆ¨Î†àÏù¥ÏÖò ÌååÏù¥ÌîÑÎùºÏù∏ ]
    
    def run_prev_day_pipeline(self, day_payload, db_conn=None, preferred_day: str | None = None):
        # 0) Í∏∞Ï§ÄÏùº ÌôïÏ†ï (Îã® Ìïú Î≤à)
        payload_days = list(day_payload.keys()) if isinstance(day_payload, dict) else None
        chosen_day = self.ensure_stat_day(preferred_day, payload_days)

        # 1) payload ÌååÏã±
        if isinstance(day_payload, dict):
            payload_list = day_payload.get(chosen_day, [])
            print(f">>>>> ‚úÖ Ï†ÑÏùº ÍµêÌÜµÎüâ Ïó∞Í≥Ñ ÏãúÎÆ¨Î†àÏù¥ÏÖò ÏãúÏûë\n      ÎåÄÏÉÅÏùº:{chosen_day}\n      entries:{len(payload_list)}")
        else:
            payload_list = day_payload or []
            print(f">>>>> ‚úÖ Ï†ÑÏùº ÍµêÌÜµÎüâ Ïó∞Í≥Ñ ÏãúÎÆ¨Î†àÏù¥ÏÖò ÏãúÏûë\n      ÎåÄÏÉÅÏùº:{chosen_day}\n      (list ÏûÖÎ†•) entries:{len(payload_list)}")

        # 2) ÍµêÌÜµÎüâ Ï£ºÏûÖ
        upd = self.insert_turn_volumes(payload_list, verbose=True)
        print(f">>>>> ‚úÖ Ï†ÑÏùº Ï£ºÏûÖ Í±¥Ïàò: {upd}")
        if upd == 0:
            print("‚õî Ï†ÑÏùº Ï£ºÏûÖ 0Í±¥ ‚Äî Îß§Ìïë/ÏûÖÎ†•Í∞í ÌôïÏù∏ Í∂åÏû•")

        # 3) ÏãúÎÆ¨ ‚Üí Í≤∞Í≥º ‚Üí CSV ‚Üí DB
        self.simulate_prev_day()

        # Í∞ÄÎìúÏ≤¥ÌÅ¨: Ï†ÑÏùºÎ°ú ÏÑ∏ÌåÖÎêêÎäîÏßÄ ÌôïÏù∏
        assert self.last_run.get("type") == "prev_day" and self.last_run.get("hour") is None, \
            f"last_run Î∂àÏùºÏπò: {self.last_run}"

        df = self.get_links_result_df()
        
        self.insert_day_link_results(df, db_conn=db.conn)

        print(">>>>> ‚úÖ Ï†ÑÏùº ÏãúÎÆ¨Î†àÏù¥ÏÖò ÏôÑÎ£å")

    # --------------------------------------------------------------- [ ÏãúÍ∞ÑÎåÄÎ≥Ñ Í≤∞Í≥ºÍ∞í insert ]
    
    def insert_hour_link_results(self, df: pd.DataFrame, db_conn, chunk_size: int = 20000) -> int:
        if df is None or df.empty or db_conn is None:
            print("‚õî ÏãúÍ∞ÑÎåÄ INSERT: DF/DB ÎàÑÎùΩ")
            return 0

        stat_day = self._require_stat_day()              # 'YYYYMMDD'
        hour_lbl = str((self.last_run or {}).get("hour") or "").zfill(2)
        if not hour_lbl:
            print("‚õî last_run.hour ÏóÜÏùå ‚Äî simulate_hour Ïù¥ÌõÑ Ìò∏Ï∂ú ÌïÑÏöî")
            return 0
        stat_hour = stat_day + hour_lbl                  # 'YYYYMMDDHH'

        required = ["STAT_HOUR", "LINK_ID", "VC", "VEHS", "SPEED"]

        # 1) ÏûëÏóÖÏö© Î≥µÏÇ¨ & Î∂àÌïÑÏöî Ïª¨Îüº Ï†úÍ±∞
        work = df.copy()
        for c in ("run_type", "hour"):
            if c in work.columns:
                work.drop(columns=[c], inplace=True)

        # 2) Ïª¨ÎüºÎ™Ö ÌÜµÏùº + STAT_HOUR ÏÑ∏ÌåÖ
        work.rename(columns={
            "link_id": "LINK_ID",
            "vc": "VC",
            "vehs": "VEHS",
            "speed": "SPEED",
        }, inplace=True)
        work["STAT_HOUR"] = stat_hour

        # 3) ÌÉÄÏûÖ Î≥¥Ï†ï
        if "LINK_ID" in work.columns:
            work["LINK_ID"] = (
                work["LINK_ID"].astype(str).str.strip()
                .map(lambda x: x if x != "" else None)
                .map(lambda x: x[:10] if x is not None else None)
            )
        if "VC" in work.columns:
            work["VC"] = pd.to_numeric(work["VC"], errors="coerce")
        if "VEHS" in work.columns:
            work["VEHS"] = pd.to_numeric(work["VEHS"], errors="coerce").astype("Int64")
        if "SPEED" in work.columns:
            work["SPEED"] = pd.to_numeric(work["SPEED"], errors="coerce")
            work.loc[work["SPEED"] >= 360000, "SPEED"] = 0

        # 4) ÌïÑÏàò Ïª¨Îüº Ï≤¥ÌÅ¨ + Ï†ïÎ†¨
        missing = [c for c in required if c not in work.columns]
        if missing:
            print(f"‚õî ÌïÑÏàò Ïª¨Îüº ÎàÑÎùΩ: {missing}")
            return 0
        work = work[required]

        # 5) NULL LINK_ID Ï†úÍ±∞
        before = len(work)
        work = work[work["LINK_ID"].notna()]
        if before != len(work):
            print(f"‚ö†Ô∏è LINK_ID NULL {before - len(work)}Ìñâ Ï†úÍ±∞")

        # 6) FK ÏÇ¨Ï†ÑÍ≤ÄÏÇ¨: TDA_LINK_INFOÏóê Ï°¥Ïû¨ÌïòÎäî LINK_IDÎßå ÎÇ®ÍπÄ
        cur = db_conn.cursor()
        unique_ids = sorted(set(work["LINK_ID"].tolist()))
        valid_ids = set()
        if unique_ids:
            BATCH = 900  # placeholder Ï†úÌïú ÎåÄÎπÑ
            for i in range(0, len(unique_ids), BATCH):
                batch = unique_ids[i:i + BATCH]
                placeholders = ", ".join(["?"] * len(batch))
                sql_chk = f"SELECT LINK_ID FROM TOMMS.TDA_LINK_INFO WHERE LINK_ID IN ({placeholders})"
                cur.execute(sql_chk, batch)
                valid_ids.update(r[0] for r in cur.fetchall())

        missing_ids = sorted(set(unique_ids) - valid_ids)
        if missing_ids:
            print(f"‚ö†Ô∏è FK ÎØ∏Ï°¥Ïû¨ LINK_ID {len(missing_ids)}Í±¥ ‚Äî INSERT ÎåÄÏÉÅÏóêÏÑú Ï†úÏô∏ (ÏòàÏãú 10Í∞ú): {missing_ids[:10]}")
            os.makedirs("./output", exist_ok=True)
            miss_path = f"./output/missing_link_ids_{stat_hour}.txt"
            with open(miss_path, "w", encoding="utf-8") as f:
                for lid in missing_ids:
                    f.write(str(lid) + "\n")
            print(f"üìÇ ÎØ∏Ï°¥Ïû¨ LINK_ID Î™©Î°ù Ï†ÄÏû•: {miss_path}")

        work = work[work["LINK_ID"].isin(valid_ids)]
        if work.empty:
            print("‚õî Ïú†Ìö® LINK_IDÍ∞Ä ÏóÜÏñ¥ INSERT Ïä§ÌÇµ")
            return 0

        # 7) ÌååÎùºÎØ∏ÌÑ∞ Î∞îÏù∏Îî© Í∞í Î≥ÄÌôò
        def _to_db_value(v):
            if v is pd.NA:
                return None
            if isinstance(v, np.generic):
                v = v.item()
            if isinstance(v, float) and (pd.isna(v) or np.isnan(v)):
                return None
            return v

        # 8) INSERT
        sql = f"INSERT INTO TOMMS.TDA_LINK_HOUR_RESULT ({', '.join(required)}) VALUES ({', '.join(['?']*len(required))})"
        cur.fast_executemany = True

        total = 0
        try:
            for s in range(0, len(work), chunk_size):
                chunk = work.iloc[s:s + chunk_size]
                data = [tuple(_to_db_value(v) for v in row) for row in chunk.itertuples(index=False, name=None)]
                cur.executemany(sql, data)
                total += len(data)

            db_conn.commit()
            print(f"      ‚úÖ TDA_LINK_HOUR_RESULT INSERT ‚Äî {stat_hour} {total}Ìñâ")
            return total

        except Exception as ex:
            db_conn.rollback()
            print(f"‚õî TDA_LINK_HOUR_RESULT INSERT Ïò§Î•ò ‚Äî Î°§Î∞±: {ex}")
            return 0
    
    # --------------------------------------------------------------- [ ÏãúÍ∞ÑÎåÄÎ≥Ñ road_id Í≤∞Í≥ºÍ∞í insert ]
    
    def insert_hour_road_results(
        self,
        df: pd.DataFrame,
        db_conn,
        stat_hour: str,
        road_id: str,
        chunk_size: int = 20000,
    ) -> int:
        """
        TDA_ROAD_HOUR_RESULT Ïä§ÌÇ§Îßà
        - STAT_HOUR (VARCHAR10, NN)
        - ROAD_ID   (VARCHAR10, NN)
        - LINK_ID   (VARCHAR10,  Y)  # ÌïòÏßÄÎßå LINK_ID ÏóÜÎäî ÌñâÏùÄ Ïó¨Í∏∞ÏÑú ÎìúÎ°≠
        - FB_VEHS   (NUMBER(9),  Y)  # 'Ï†ÑÏùº_Ïö©Îüâ' Îß§Ìïë
        """
        if df is None or df.empty or db_conn is None:
            print("‚õî ROAD HOUR INSERT: DF/DB ÎàÑÎùΩ")
            return 0

        # 0) Í≤∞Í≥º ÌÖåÏù¥Î∏îÎ™Ö
        table = "TOMMS.TDA_ROAD_VOL_HOUR_RESULT"

        # 1) Ïä§ÌÇ§Îßà Ï†ïÍ∑úÌôî
        work = df.copy()

        # Ïª¨ÎüºÎ™Ö ÌÜµÏùº: Ï†ÑÏùº_Ïö©Îüâ ‚Üí FB_VEHS, link_id‚ÜíLINK_ID Îì±
        work.rename(columns={
            "Ï†ÑÏùº_Ïö©Îüâ": "FB_VEHS",
            "link_id": "LINK_ID",
            "stat_hour": "STAT_HOUR",
            "road_id": "ROAD_ID",
        }, inplace=True)

        # ÌååÎùºÎØ∏ÌÑ∞Î°ú Î∞õÏùÄ STAT_HOUR/ROAD_IDÎ•º Í∞ïÏ†ú ÏÑ∏ÌåÖ (Ïã†Î¢∞Ïõê ÌÜµÏùº)
        work["STAT_HOUR"] = str(stat_hour)
        work["ROAD_ID"]   = str(road_id)

        # ÌïÑÏöî Ïª¨ÎüºÎßå Ïú†ÏßÄ (ÏàúÏÑú Í≥†Ï†ï)
        required = ["STAT_HOUR", "ROAD_ID", "LINK_ID", "FB_VEHS"]
        for c in required:
            if c not in work.columns:
                work[c] = pd.Series(dtype="object")  # ÎàÑÎùΩ Ïª¨Îüº ÏÉùÏÑ±
        work = work[required]

        # 2) ÌÉÄÏûÖ Î≥¥Ï†ï
        # LINK_ID: Î¨∏ÏûêÏó¥ 10Ïûê, Í≥µÎ∞±/ÎπàÎ¨∏Ïûê None
        work["LINK_ID"] = (
            work["LINK_ID"].astype(str).str.strip()
            .map(lambda x: None if x == "" or x.lower() == "none" else x[:10])
        )
        # FB_VEHS: Ï†ïÏàò(Int64)Î°ú
        work["FB_VEHS"] = pd.to_numeric(work["FB_VEHS"], errors="coerce").astype("Int64")

        # 3) LINK_ID ÏóÜÎäî Ìñâ Ï†úÍ±∞(ÏöîÍµ¨ÏÇ¨Ìï≠: Í∞íÏù¥ ÏóÜÏúºÎ©¥ Î™®Îëê ÎÇ†Î¶º)
        before = len(work)
        work = work[work["LINK_ID"].notna()]
        if before != len(work):
            print(f"‚ö†Ô∏è LINK_ID NULL {before - len(work)}Ìñâ Ï†úÍ±∞ (ROAD_ID={road_id}, STAT_HOUR={stat_hour})")

        if work.empty:
            print(f"‚õî INSERT Ïä§ÌÇµ ‚Äî Ïú†Ìö®Ìñâ 0 (ROAD_ID={road_id}, STAT_HOUR={stat_hour})")
            return 0

        # 4) FK ÏÇ¨Ï†ÑÍ≤ÄÏÇ¨: TDA_LINK_INFO(LINK_ID)
        cur = db_conn.cursor()
        unique_ids = sorted(set(work["LINK_ID"].tolist()))
        valid_ids = set()
        if unique_ids:
            BATCH = 900
            for i in range(0, len(unique_ids), BATCH):
                batch = unique_ids[i:i+BATCH]
                placeholders = ", ".join(["?"] * len(batch))
                sql_chk = f"SELECT LINK_ID FROM TOMMS.TDA_LINK_INFO WHERE LINK_ID IN ({placeholders})"
                cur.execute(sql_chk, batch)
                valid_ids.update(r[0] for r in cur.fetchall())
        missing_ids = sorted(set(unique_ids) - valid_ids)
        if missing_ids:
            print(f"‚ö†Ô∏è LINK_ID FK ÎØ∏Ï°¥Ïû¨ {len(missing_ids)}Í±¥ ‚Äî Ï†úÏô∏ (ÏòàÏãú‚â§10): {missing_ids[:10]}")
            os.makedirs("./output", exist_ok=True)
            miss_path = f"./output/missing_hour_road_link_ids_{stat_hour}_{road_id}.txt"
            with open(miss_path, "w", encoding="utf-8") as f:
                for lid in missing_ids:
                    f.write(str(lid) + "\n")
            print(f"üìÇ FK ÎØ∏Ï°¥Ïû¨ LINK_ID Ï†ÄÏû•: {miss_path}")
        work = work[work["LINK_ID"].isin(valid_ids)]

        if work.empty:
            print(f"‚õî INSERT Ïä§ÌÇµ ‚Äî FK Ïú†Ìö®Ìñâ 0 (ROAD_ID={road_id}, STAT_HOUR={stat_hour})")
            return 0

        # 5) Î∞îÏù∏Îî© Í∞í Î≥ÄÌôò
        def _to_db_value(v):
            if v is pd.NA:
                return None
            if isinstance(v, np.generic):
                v = v.item()
            if isinstance(v, float) and (pd.isna(v) or np.isnan(v)):
                return None
            return v

        # 6) INSERT
        sql = f"INSERT INTO {table} ({', '.join(required)}) VALUES ({', '.join(['?']*len(required))})"
        cur.fast_executemany = True

        # (ÏÑ†ÌÉù) SQL Î°úÍ∑∏
        os.makedirs("./output", exist_ok=True)
        log_path = f"./output/road_hour_result_insert_{stat_hour}_{road_id}.sql.txt"
        def _sql_literal(v):
            if v is None: return "NULL"
            if isinstance(v, (int, np.integer)): return str(int(v))
            if isinstance(v, (float, np.floating)): return str(float(v))
            s = str(v).replace("'", "''"); return f"'{s}'"

        total = 0
        try:
            with open(log_path, "w", encoding="utf-8") as f_log:
                for s in range(0, len(work), chunk_size):
                    chunk = work.iloc[s:s+chunk_size]
                    data = [tuple(_to_db_value(v) for v in row) for row in chunk.itertuples(index=False, name=None)]
                    # Î°úÍ∑∏Ïö© SQL
                    for row in data:
                        values_str = [_sql_literal(v) for v in row]
                        f_log.write(
                            f"INSERT INTO {table} ({', '.join(required)}) "
                            f"VALUES ({', '.join(values_str)});\n"
                        )
                    cur.executemany(sql, data)
                    total += len(data)

            db_conn.commit()
            print(f"      ‚úÖ ROAD_HOUR_RESULT INSERT ‚Äî STAT_HOUR={stat_hour}, ROAD_ID={road_id}, ÌñâÏàò={total}")
            print(f"      üìÇ SQL Î°úÍ∑∏: {log_path}")
            return total

        except Exception as ex:
            db_conn.rollback()
            print(f"‚õî ROAD_HOUR_RESULT INSERT Ïò§Î•ò ‚Äî Î°§Î∞±: {ex}")
            print(f"üìÇ SQL Î°úÍ∑∏(Ïã§Ìå® ÏãúÏ†êÍπåÏßÄ): {log_path}")
            return 0
    
    # --------------------------------------------------------------- [ ÏãúÍ∞ÑÎåÄÎ≥Ñ ÏãúÎÆ¨Î†àÏù¥ÏÖò ÌååÏù¥ÌîÑÎùºÏù∏ ]

    def run_hourly_pipeline(self, hourly_payload_map: dict, db_conn=None):
        """
        Í≥†Ï†ï ÏàúÏÑú: 08 ‚Üí 11 ‚Üí 14 ‚Üí 17
        STAT_DAYÎäî Ïù¥ÎØ∏ ensure_stat_day/set_stat_dayÎ°ú ÌôïÏ†ïÎêòÏñ¥ ÏûàÏñ¥Ïïº Ìï®.
        """
        
        # üîµ GPA ÌååÏùº Í≤ΩÎ°ú ÏßÄÏ†ï
        gpa_file_path = r"C:\Digital Twin Simulation Network\VISUM\gpa_file"
        
        self._require_stat_day()
        stat_day = self.last_run['stat_day']
        print(f">>>>> ‚úÖ ÏãúÍ∞ÑÎåÄ ÍµêÌÜµÎüâ Ïó∞Í≥Ñ ÏãúÎÆ¨Î†àÏù¥ÏÖò ÏãúÏûë\n      STAT_DAY={self.last_run['stat_day']}")

        for hh in [8, 11, 14, 17]:
            key = f"{hh:02d}"
            payload = hourly_payload_map.get(key, [])
            print(f">>>>> ‚úÖ {key}Ïãú Ï≤òÎ¶¨ ÏãúÏûë ‚Äî ÍµêÏ∞®Î°ú:{len(payload)}")

            upd = self.insert_turn_volumes(payload, verbose=False)
            print(f">>>>> ‚úÖ {key}Ïãú Ï£ºÏûÖ Í±¥Ïàò: {upd}")

            self.simulate_hour(hh)

            # Í∞ÄÎìúÏ≤¥ÌÅ¨: ÏãúÍ∞ÑÎåÄ ÏÑ∏ÌåÖ ÌôïÏù∏
            assert self.last_run.get("type") == "hourly" and self.last_run.get("hour") == key, \
                f"last_run Î∂àÏùºÏπò: {self.last_run}"

            df = self.get_links_result_df()
            self.insert_hour_link_results(df, db_conn=db.conn)
            
            # üîµ INSERT Ïù¥ÌõÑ: GPA ÌååÏùº Ï†ÅÏö©
            stat_hour = f"{stat_day}{key}"
            if gpa_file_path:
                pass
                # self.read_gpa_file_get_road_link(db_conn, gpa_file_path, stat_hour)

        print(">>>>> ‚úÖ ÏãúÍ∞ÑÎåÄÎ≥Ñ ÏãúÎÆ¨Î†àÏù¥ÏÖò ÏôÑÎ£å")











# ====================================================================================== [ main Ïã§ÌñâÌï®Ïàò ]

class DualLogger:
    def __init__(self, file_path):
        self.terminal = sys.stdout
        self.log = open(file_path, "a", encoding="utf-8")

    def write(self, message):
        self.terminal.write(message)   # ÏΩòÏÜî Ï∂úÎ†•
        self.log.write(message)        # ÌååÏùº Í∏∞Î°ù

    def flush(self):
        self.terminal.flush()
        self.log.flush()

if __name__ == "__main__":
    # Î°úÍ∑∏ Ìè¥Îçî ÏÉùÏÑ±
    log_dir = r"C:\Digital Twin Simulation Program\auto simulation\logs"
    os.makedirs(log_dir, exist_ok=True)

    # ÌååÏùºÎ™Ö: ÎÇ†Ïßú+ÏãúÍ∞Ñ Ï†ëÎëêÏñ¥
    ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"{ts}_visum_simulation.log")

    # dual logger ÏÑ∏ÌåÖ
    sys.stdout = DualLogger(log_file)

    print(">>>>> ‚úÖ VISUM ÏûêÎèôÌôî ÏãúÎÆ¨Î†àÏù¥ÏÖòÏùÑ ÏãúÏûëÌï©ÎãàÎã§.")
    USE_FIXED_TIME = True  # Ïã§ÏãúÍ∞Ñ Îã®ÏúÑÎ°ú Î≥ÄÍ≤ΩÌïòÎ†§Î©¥ False

    if USE_FIXED_TIME:
        fixed_now = datetime.datetime.strptime("2025070201", "%Y%m%d%H")
        query_day, target_stat_hours = compute_target_hours(fixed_now, ["08", "11", "14", "17"])
    else:
        query_day, target_stat_hours = compute_target_hours(None, ["08", "11", "14", "17"])

    # 1) DB
    config = Config()
    db = DatabaseManager(config)
    print(">>>>> ‚úÖ Config, DB ÌÅ¥ÎûòÏä§Í∞Ä ÏÑ†Ïñ∏ÎêòÏñ¥ main Ìï®Ïàò ÎÇ¥ ÏÑ§Ï†ïÏù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§.")

    try:
        print(">>>>> ‚úÖ ÍµêÌÜµÎüâ Îç∞Ïù¥ÌÑ∞ Ï°∞ÌöåÎ•º ÏãúÏûëÌï©ÎãàÎã§.")
        traffic_by_hour, traffic_by_day, query_day_from_db = db.fetch_and_process_data(target_stat_hours)

        stat_day_final = query_day_from_db or query_day

        vis = VisumSimulationManager(
            base_path=r"C:/Digital Twin Simulation network/VISUM",
            default_version_name="Í∞ïÎ¶âÏãú Ï†ÑÍµ≠ Ï†ÑÏùº ÏµúÏ¢ÖÎ≥∏.ver",
            prev_day_proc_no=22,
            csv_out_dir=r"C:/Digital Twin Simulation network/VISUM/result_export",
        )

        print(">>>>> ‚úÖ Visum ÌÅ¥ÎûòÏä§Í∞Ä ÏÑ†Ïñ∏ÎêòÏñ¥ main Ìï®Ïàò ÎÇ¥ ÏÑ§Ï†ïÏù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§.")

        # 4) Visum open & load
        vis.open()
        vis.set_stat_day(stat_day_final)

        # 5) Ï†ÑÏùº ÌååÏù¥ÌîÑÎùºÏù∏
        vis.run_prev_day_pipeline(traffic_by_day, db_conn=db.conn, preferred_day=stat_day_final)

        # 6) ÏãúÍ∞ÑÎåÄ ÌååÏù¥ÌîÑÎùºÏù∏
        vis.run_hourly_pipeline(traffic_by_hour, db_conn=db.conn)

    finally:
        if 'vis' in locals():
            vis.close()
        db.close()
        print(f"üìÇ Î°úÍ∑∏ Ï†ÄÏû• ÏôÑÎ£å ‚Üí {log_file}")