import sys
import os
import re
import time
import shutil
import pyodbc
import fnmatch
import datetime
import pywintypes
import pandas as pd
import win32com.client as com
import numpy as np

from pprint import pprint
from decimal import Decimal
from dotenv import load_dotenv
from contextlib import redirect_stdout
from collections import defaultdict
from typing import Dict, List, Tuple, Optional








# ================================================== [ DPI ì„¤ì • ]

def set_dpi_awareness() -> None:
    try:
        from ctypes import windll
        windll.shcore.SetProcessDpiAwareness(1)
    except ImportError:
        print(" â›” ctypes ëª¨ë“ˆì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. Python í™˜ê²½ì„ í™•ì¸í•˜ì„¸ìš”.")
    except AttributeError:
        print(" â›” SetProcessDpiAwareness í•¨ìˆ˜ê°€ ì§€ì›ë˜ì§€ ì•ŠëŠ” í™˜ê²½ì…ë‹ˆë‹¤. Windows 8.1 ì´ìƒì—ì„œë§Œ ì§€ì›ë©ë‹ˆë‹¤.")
    except OSError as os_error:
        print(f" â›” OS ê´€ë ¨ ì˜¤ë¥˜ ë°œìƒ: {os_error}")
    except Exception as e:
        print(f" â›” ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")

set_dpi_awareness()
sys.path.append(os.path.dirname(os.path.abspath(__file__)))








# ================================================== [ ê³µìš© ìœ í‹¸ ]

# Decimal â†’ int/float ë³€í™˜, ë‚˜ë¨¸ì§€ëŠ” ê·¸ëŒ€ë¡œ.
def to_py(val):
    if isinstance(val, Decimal):
        return int(val) if val == int(val) else float(val)
    return val

# í˜•ì‹ ê²€ì‚¬: YYYYMMDDHH (10ìë¦¬ ìˆ«ì)
def is_valid_stat_hour(s: str) -> bool:
    return bool(re.fullmatch(r"\d{10}", s))

# VOL_00 ~ VOL_23 ê°™ì€ ì»¬ëŸ¼ì„ ì´ë¦„ ê¸°ì¤€ ì •ë ¬í•œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜. ë°˜í™˜ í˜•íƒœ: [(ì»¬ëŸ¼ëª…, ì»¬ëŸ¼ì¸ë±ìŠ¤), ...]
def extract_vol_columns(cols: List[str]) -> List[Tuple[str, int]]:
    # ì¸ë±ìŠ¤ëŠ” ì™¸ë¶€ì—ì„œ ë§¤í•‘ëœ dict ë¡œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë¯€ë¡œ, ì—¬ê¸°ì„  ì´ë¦„ë§Œ ì •ë ¬.
    vol_names = sorted([c for c in cols if re.fullmatch(r"VOL_\d{2}", c)])
    return vol_names

def to_db_py(v):
    # None/NaN ì²˜ë¦¬
    if v is None:
        return None
    # pandasì˜ NAë¥˜ë„ Noneìœ¼ë¡œ
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass

    # numpy ìŠ¤ì¹¼ë¼ â†’ íŒŒì´ì¬ ìŠ¤ì¹¼ë¼
    if isinstance(v, (np.integer,)):
        return int(v)
    if isinstance(v, (np.floating,)):
        return float(v)
    if isinstance(v, (np.bool_,)):
        return bool(v)

    # Decimal â†’ int/float
    if isinstance(v, Decimal):
        return int(v) if v == int(v) else float(v)

    # ê·¸ ì™¸ëŠ” ì›ë³¸ ìœ ì§€(ë¬¸ìì—´ ë“±)
    return v





# ================================================== [ í˜„ì¬ ì¼ì‹œ/íƒ€ê¹ƒ ì‹œê°„ ê³„ì‚° ]

def compute_target_hours(now: Optional[datetime.datetime] = None,
                        pick_hours: List[str] = None) -> Tuple[str, List[str]]:
    
    """
    ê¸°ì¤€ì‹œê°(now) ê¸°ì¤€ ì „ë‚  ë‚ ì§œ(YYYYMMDD)ì™€, ì „ë‚ ì˜ íŠ¹ì • HH ë¦¬ìŠ¤íŠ¸(YYYYMMDDHH)ë¥¼ ëŒë ¤ì¤Œ.
    """
    
    if now is None:
        now = datetime.datetime.now()
    if pick_hours is None:
        pick_hours = ["08", "11", "14", "17"]

    target_date = (now - datetime.timedelta(days=1)).strftime("%Y%m%d")
    target_stat_hours = [f"{target_date}{h}" for h in pick_hours]
    print(f">>>>> âœ… ë°ì´í„° ì¡°íšŒ ê¸°ì¤€ ì‹œê°„ : {target_date}")
    return target_date, target_stat_hours







# ================================================== [ í™˜ê²½ì„¤ì • ë° DB ì ‘ì†ì •ë³´ ë¡œë”© ]

class Config:

    def __init__(self):
        load_dotenv(dotenv_path="C:/Digital Twin Simulation Program/.env")
        raw_env = (os.getenv("FLASK_ENV", "prod") or "").lower()
        self.env = "prod" if raw_env in ("prod", "production") else "test"

        self.db_config = {
            "test": {
                "driver": "Tibero 5 ODBC Driver",
                "server": os.getenv("ENZERO_SERVER"),
                "port": os.getenv("ENZERO_PORT"),
                "db": os.getenv("ENZERO_DB"),
                "uid": os.getenv("ENZERO_UID"),
                "pwd": os.getenv("ENZERO_PWD"),
            },
            "prod": {
                "dsn": os.getenv("DSNNAME"),
                "uid": os.getenv("DBUSER"),
                "pwd": os.getenv("DBPWD"),
            },
        }








# ================================================== [ DB ]

class DatabaseManager:
    """
    Tibero ê¸°ë°˜ ì‹œê°„ëŒ€/ì¼ë³„ êµí†µëŸ‰ ì¡°íšŒ ë§¤ë‹ˆì €
    """
    def __init__(self, config: Config):
        self.config = config
        self.conn = self._connect()
        self.cursor = self.conn.cursor() if self.conn else None

        # ì‚¬ìš©í•  ì»¬ëŸ¼ë“¤(ì°¸ê³ ìš©)
        self.columns = [
            "STAT_HOUR", "CROSS_ID",
            "VOL_00","VOL_01","VOL_02","VOL_03","VOL_04","VOL_05",
            "VOL_06","VOL_07","VOL_08","VOL_09","VOL_10","VOL_11",
            "VOL_12","VOL_13","VOL_14","VOL_15","VOL_16","VOL_17",
            "VOL_18","VOL_19","VOL_20","VOL_21","VOL_22","VOL_23",
        ]

    def _connect(self):
        try:
            if self.config.env == "test":
                db = self.config.db_config["test"]
                print(">>>>> âœ… ì—”ì œë¡œ DB ì„œë²„ ì—°ê²°. í…ŒìŠ¤íŠ¸ ë²„ì „ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
                return pyodbc.connect(
                    f"DRIVER={db['driver']};SERVER={db['server']};PORT={db['port']};"
                    f"DB={db['db']};UID={db['uid']};PWD={db['pwd']};"
                )
            else:
                db = self.config.db_config["prod"]
                print(">>>>> âœ… ê°•ë¦‰ì‹œ í‹°ë² ë¡œ DB ì„œë²„ ì—°ê²°. ë°°í¬ ë²„ì „ì…ë‹ˆë‹¤.")
                return pyodbc.connect(f"DSN={db['dsn']};UID={db['uid']};PWD={db['pwd']}")
        except Exception as e:
            print("â›” DB ì—°ê²° ì‹¤íŒ¨:", e)
            return None

    def _exec(self, sql: str, params: Tuple = ()) -> Tuple[List[tuple], List[str]]:
        """
        ê³µìš© ì¿¼ë¦¬ ì‹¤í–‰. rows(tuple list)ì™€ cols(list[str]) ë°˜í™˜.
        """
        if not self.cursor:
            raise RuntimeError("DB ì»¤ë„¥ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
        self.cursor.execute(sql, params)
        rows = [tuple(r) for r in self.cursor.fetchall()]
        cols = [col[0] for col in self.cursor.description]
        return rows, cols

    def fetch_and_process_data(self, target_stat_hours: List[str]):
        """
        ì…ë ¥ëœ STAT_HOUR(YYYYMMDDHH) ì§‘í•©ì— ëŒ€í•´:
          - STAT_HOUR_CROSSì—ì„œ ì‹œê°„ëŒ€ë³„ ìë£Œ ìˆ˜ì§‘
          - í•´ë‹¹ ì¼ì(YYYYMMDD)ë¥¼ ëª¨ì•„ STAT_DAY_CROSSì—ì„œ ì¼ë³„ ìë£Œ ìˆ˜ì§‘
        ë°˜í™˜:
          - traffic_data_by_hour: { "00":[{cross_id, data:[{direction, value}...] }], ... }
          - traffic_data_by_day : { "YYYYMMDD":[{cross_id, data:[...]}], ... }
          - query_day: ìµœì´ˆ ìš”ì²­ ì‹œê°ì˜ YYYYMMDD
        """
        # 1) ì…ë ¥ ê²€ì¦ & íŒŒë¼ë¯¸í„° êµ¬ì„±
        target_stat_hours = [h for h in target_stat_hours if is_valid_stat_hour(h)]
        print(f">>>>> âœ… target_stat_hours ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤. : {target_stat_hours}")
        if not target_stat_hours:
            print("â›” ìœ íš¨í•œ STAT_HOURê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {f"{h:02d}": [] for h in range(24)}, {}, None

        # WHERE (STAT_HOUR = ? OR STAT_HOUR = ? ...)
        where_clause = " OR ".join(["STAT_HOUR = ?"] * len(target_stat_hours))
        sql_hour = f"""
            SELECT *
            FROM STAT_HOUR_CROSS
            WHERE ({where_clause})
              AND INFRA_TYPE = 'SMT'
        """

        # 2) ì‹œê°„ëŒ€ë³„ ì¡°íšŒ
        rows, cols = self._exec(sql_hour, tuple(target_stat_hours))
        print(f">>>>> âœ… ì‹œê°„ëŒ€ë³„ ì¡°íšŒëœ í–‰ì˜ ê°¯ìˆ˜ì…ë‹ˆë‹¤. : {len(rows)}")

        col_idx = {c: i for i, c in enumerate(cols)}
        idx_stat_hour = col_idx["STAT_HOUR"]
        idx_cross_id  = col_idx["CROSS_ID"]

        vol_names = extract_vol_columns(cols)  # ì´ë¦„ë§Œ ì¶”ì¶œ í›„ ì •ë ¬
        vol_idx_pairs = [(name, col_idx[name]) for name in vol_names]

        traffic_data_by_hour: Dict[str, List[dict]] = {f"{h:02d}": [] for h in range(24)}
        stat_days = set()

        for r in rows:
            stat_hour = str(r[idx_stat_hour]).strip()
            yyyymmdd = stat_hour[:8]
            stat_days.add(yyyymmdd)

            hh = stat_hour[-2:]
            if hh not in traffic_data_by_hour:
                print(f"â›” ì˜ˆìƒì¹˜ ëª»í•œ hh ê°’ ë°œê²¬: {hh}")
                continue

            cross_id = str(r[idx_cross_id]).strip()
            traffic_data_by_hour[hh].append({
                "cross_id": cross_id,
                "data": [
                    {"direction": name, "value": to_py(r[idx])}
                    for name, idx in vol_idx_pairs
                    if to_py(r[idx]) is not None
                ]
            })
        print(f">>>>> âœ… ì‹œê°„ëŒ€ë³„ êµí†µëŸ‰ì„ VISUMì— ì—°ê³„í•˜ê¸° ìœ„í•œ ê°€ê³µì„ ì™„ë£Œí•˜ì˜€ìŠµë‹ˆë‹¤.")

        # 3) ì¼ë³„ ì¡°íšŒ
        traffic_data_by_day: Dict[str, List[dict]] = {}

        for day in stat_days:
            sql_day = """
                SELECT *
                FROM STAT_DAY_CROSS
                WHERE STAT_DAY = ?
                  AND INFRA_TYPE = 'SMT'
            """
            rows_day, cols_day = self._exec(sql_day, (day,))
            col_idx_day = {c: i for i, c in enumerate(cols_day)}

            idx_cross_id_day = col_idx_day["CROSS_ID"]
            vol_names_day = extract_vol_columns(cols_day)
            vol_idx_pairs_day = [(name, col_idx_day[name]) for name in vol_names_day]

            items = []
            for r in rows_day:
                cross_id = str(r[idx_cross_id_day]).strip()
                items.append({
                    "cross_id": cross_id,
                    "data": [
                        {"direction": name, "value": to_py(r[idx])}
                        for name, idx in vol_idx_pairs_day
                        if to_py(r[idx]) is not None
                    ]
                })
            traffic_data_by_day[day] = items

        # 4) ìš”ì•½ ì¶œë ¥ (ë£¨í”„ ë°–ì—ì„œ í•œ ë²ˆë§Œ)
        total_day = sum(len(v) for v in traffic_data_by_day.values())
        if stat_days:
            print(f">>>>> âœ… ì „ì¼ êµí†µëŸ‰ì„ VISUMì— ì—°ê³„í•˜ê¸° ìœ„í•œ ê°€ê³µì„ ì™„ë£Œí•˜ì˜€ìŠµë‹ˆë‹¤.")
            print(f"      âœ… ì „ì¼ êµí†µëŸ‰ ì´ {total_day}ê±´ ({len(traffic_data_by_day)}ì¼)")
        for hh, lst in traffic_data_by_hour.items():
            if lst:
                print(f"      âœ… ì‹œê°„ëŒ€ë³„ êµí†µëŸ‰ {hh}ì‹œ : {len(lst)}ê±´")

        # 5) ì²« ì¿¼ë¦¬ ê¸°ì¤€ day
        query_day = target_stat_hours[0][:8] if target_stat_hours else None

        return traffic_data_by_hour, traffic_data_by_day, query_day

    def close(self):
        try:
            if self.cursor:
                self.cursor.close()
                self.cursor = None
            if self.conn:
                self.conn.close()
                self.conn = None
            print(">>>>> âœ… DB ì—°ê²°ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        except Exception as e:
            print("â›” DB ì¢…ë£Œ ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤. ", e)








# ================================================== [ VISUM ]

class VisumSimulationManager:
    
    # --------------------------------------------------------------- [ ì‹œê°„ëŒ€ë³„ Active ë²ˆí˜¸ (ê³ ì • ì‹œë‚˜ë¦¬ì˜¤) ]
    
    HOUR_TO_PROC = {
        8: 312,
        11: 408,
        14: 505,
        17: 601,
    }

    # --------------------------------------------------------------- [ init ]

    def __init__(
        self,
        base_path: str,
        default_version_name: str,
        prev_day_proc_no: int = 22,
        csv_out_dir: str = r"C:/Digital Twin Simulation network/VISUM/result_export",
        table_map: dict = None,     # {"prev_day": "TABLE_A", "hourly": "TABLE_B"}
    ):
        self.base_path = base_path
        self.default_version_name = default_version_name
        self.prev_day_proc_no = prev_day_proc_no
        self.csv_out_dir = csv_out_dir
        os.makedirs(self.csv_out_dir, exist_ok=True)

        self.table_map = table_map or {
            "prev_day": "LINK_RESULT_DAY",
            "hourly": "LINK_RESULT_HOUR",
        }

        self.visum = None
        # last_run: {"type": "prev_day"|"hourly"|None, "hour": "00"~"23"|None, "stat_day": "YYYYMMDD"|None}
        self.last_run = {"type": None, "hour": None, "stat_day": None}











    # --------------------------------------------------------------- [ ê¸°ì¤€ì¼ ê´€ë¦¬ ]
    
    def set_stat_day(self, yyyymmdd: str):
        if not (isinstance(yyyymmdd, str) and len(yyyymmdd) == 8 and yyyymmdd.isdigit()):
            raise ValueError("STAT_DAY must be 'YYYYMMDD' string.")
        self.last_run["stat_day"] = yyyymmdd
        print(f">>>>> âœ… VISUM set_stat_day í•¨ìˆ˜ì—ì„œ ê±´ë„¤ë°›ì€ ì¼ìëŠ” [ {self.last_run} ] ì…ë‹ˆë‹¤.")

    def ensure_stat_day(self, preferred_day: str | None, payload_days: list[str] | None = None) -> str:
        """
        ê¸°ì¤€ì¼ì„ ë‹¨ í•œ ë²ˆë§Œ í™•ì •:
        1) preferred_dayê°€ ìœ íš¨í•˜ë©´ ê·¸ê±¸ ì‚¬ìš©
        2) ì—†ìœ¼ë©´ payload_daysì—ì„œ ìµœì‹ ì¼ìë¥¼ ì„ íƒ
        ì´í›„ self.last_run['stat_day']ë¥¼ ì„¤ì •í•˜ê³ , ì´í›„ì—ëŠ” ì ˆëŒ€ ë°”ê¾¸ì§€ ì•ŠìŒ
        """
        if self.last_run.get("stat_day"):
            return self.last_run["stat_day"]

        cand = None
        if isinstance(preferred_day, str) and len(preferred_day) == 8 and preferred_day.isdigit():
            cand = preferred_day
        elif payload_days:
            # payload_daysê°€ ë¬¸ìì—´ 'YYYYMMDD' ë¦¬ìŠ¤íŠ¸ë¼ê³  ê°€ì •
            cand = max([d for d in payload_days if isinstance(d, str) and len(d) == 8 and d.isdigit()], default=None)

        if not cand:
            raise ValueError("â›” ê¸°ì¤€ì¼(STAT_DAY)ì„ í™•ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. preferred_day ë˜ëŠ” payload_daysë¥¼ ì œê³µí•˜ì„¸ìš”.")

        self.last_run["stat_day"] = cand
        return cand

    def _require_stat_day(self) -> str:
        sd = self.last_run.get("stat_day")
        if not (isinstance(sd, str) and len(sd) == 8 and sd.isdigit()):
            raise ValueError("â›” STAT_DAY(yyyymmdd)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. vis.set_stat_day('YYYYMMDD') ë˜ëŠ” ensure_stat_day()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        return sd

    def _update_last_run(self, run_type: str, hour_label: str | None):
        sd = self.last_run.get("stat_day")  # ë³´ì¡´
        self.last_run = {"type": run_type, "hour": hour_label, "stat_day": sd}

    # --------------------------------------------------------------- [ VISUM LOAD & CLOSE ]
    
    def open(self, filename: str = None):
        self.visum = com.Dispatch("Visum.Visum.22")

        ver_filename = filename or self.default_version_name
        ver_path = os.path.join(self.base_path, ver_filename)
        if not os.path.isfile(ver_path):
            print(f"â›” VISUM ë„¤íŠ¸ì›Œí¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ver_path}")
            self.visum = None
            return

        self.visum.LoadVersion(ver_path)
        print(f">>>>> âœ… VISUM ë„¤íŠ¸ì›Œí¬ ë¡œë“œ ì™„ë£Œ: {ver_path}")

    def close(self):
        if self.visum:
            self.visum = None
            print(">>>>> âœ… VISUM ì„¸ì…˜ ì¢…ë£Œ")











    # --------------------------------------------------------------- [ êµí†µëŸ‰ ì—°ê³„(ê³µí†µ) ]
    
    def insert_turn_volumes(self, data_list, verbose: bool = False) -> int:
        if not self.visum:
            print("â›” Visum ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        print(">>>>> âœ… ì¡°íšŒëœ êµí†µëŸ‰ ë°ì´í„°ì˜ VISUM ì—°ê³„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

        turns = self.visum.Net.Turns
        gn_node_list = [int(x[1]) if x[1] is not None else 0 for x in turns.GetMultiAttValues("gn_node_id")]
        gn_dir_list  = [int(x[1]) if x[1] is not None else 0 for x in turns.GetMultiAttValues("gn_direction_id")]
        from_list    = [int(x[1]) if x[1] is not None else 0 for x in turns.GetMultiAttValues("FromNodeNo")]
        via_list     = [int(x[1]) if x[1] is not None else 0 for x in turns.GetMultiAttValues("ViaNodeNo")]
        to_list      = [int(x[1]) if x[1] is not None else 0 for x in turns.GetMultiAttValues("ToNodeNo")]

        key2nodes = {
            (gn_node_list[i], gn_dir_list[i]): (from_list[i], via_list[i], to_list[i])
            for i in range(len(gn_node_list))
            if gn_node_list[i] and gn_dir_list[i]
        }

        updates = 0
        for item in (data_list or []):
            try:
                cross_id = int(item.get("cross_id"))
            except Exception:
                if verbose:
                    print(f"â›” cross_id íŒŒì‹± ì‹¤íŒ¨: {item}")
                continue

            for d in item.get("data", []):
                dir_str = d.get("direction")
                val = d.get("value")
                if val is None or not dir_str:
                    continue

                try:
                    direction_num = int(dir_str.split("_")[-1])
                except Exception:
                    if verbose:
                        print(f"â›” direction íŒŒì‹± ì‹¤íŒ¨ â€” cross_id={cross_id}, direction={dir_str}")
                    continue

                nodes = key2nodes.get((cross_id, direction_num))
                if not nodes:
                    if verbose:
                        print(f"â›” ì—°ê³„ ì‹¤íŒ¨ >>> CROSS_ID={cross_id}, DIR={direction_num:02d} (Turn ë¯¸ì¡´ì¬)")
                    continue

                f, v, t = nodes
                try:
                    turn = turns.ItemByKey(f, v, t)
                    for att in ("ABT_TOTAL1", "ABT_TOTAL2", "TOTAL_TRAFFIC_VOL"):
                        turn.SetAttValue(att, val)
                    updates += 1
                except Exception as e:
                    print(f"â›” SetAttValue ì‹¤íŒ¨ â€” nodes=({f},{v},{t}), err={e}")

        return updates

    # --------------------------------------------------------------- [ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰(ê³µí†µ) ]
    
    def _execute_procedure(self, proc_no: int):
        ops = self.visum.Procedures.Operations
        try:
            print(f">>>>> âœ… Procedure Sequence Set Active Number : {proc_no}")
            ops.ItemByKey(proc_no).SetAttValue("Active", 1)
            self.visum.Procedures.Execute()
        finally:
            try:
                ops.ItemByKey(proc_no).SetAttValue("Active", 0)
            except Exception:
                pass

    def simulate_prev_day(self):
        self._require_stat_day()
        self._execute_procedure(self.prev_day_proc_no)
        self._update_last_run("prev_day", None)
        print(f"      âœ… ì „ì¼ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ (Active={self.prev_day_proc_no})")

    def simulate_hour(self, hour: int):
        self._require_stat_day()
        proc = self.HOUR_TO_PROC.get(int(hour))
        if proc is None:
            print(f"â›” {hour}ì‹œ í”„ë¡œì‹œì € ë²ˆí˜¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        self._execute_procedure(proc)
        self._update_last_run("hourly", f"{int(hour):02d}")
        print(f">>>>> âœ… {int(hour):02d}ì‹œ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ (Active={proc})")











    # --------------------------------------------------------------- [ ë§í¬ ê²°ê³¼ê°’ ì¶”ì¶œ ]
    
    def get_links_result_df(self) -> pd.DataFrame:
        if not self.visum:
            print("â›” Visum ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()

        run_type = (self.last_run or {}).get("type")
        hour_lbl = (self.last_run or {}).get("hour")
        if run_type not in ("prev_day", "hourly"):
            print("â›” ì‹¤í–‰ ì´ë ¥ ì—†ìŒ â€” simulate í˜¸ì¶œ í›„ ì‚¬ìš©í•˜ì„¸ìš”.")
            return pd.DataFrame()

        stat_day = self._require_stat_day()

        attrs = [
            "AREA", "SUBAREA", "LINK_ID", "ROAD_NAME", "UPDOWN",
            "VolCapRatioPrT(AP)", "VolVehPrT(AP)", "TCur_PrTSys(a)",
        ]

        rows = self.visum.Net.Links.GetMultipleAttributes(attrs, True)

        records_expanded, invalid_ids = [], set()

        for row in rows:
            base = dict(zip(attrs, row))
            raw_id = base.get("LINK_ID")

            if not raw_id:  # None, ë¹ˆ ë¬¸ìì—´ ë“± ì œì™¸
                continue

            # 1) ì‰¼í‘œ ë¶„í•´ + íŠ¸ë¦¼
            link_ids = [x.strip() for x in str(raw_id).split(",") if x.strip()]

            # 2) ê° link_idë³„ë¡œ í•œ í–‰ì”© ë³µì œ (10ìë¦¬ ìˆ«ìë§Œ ìœ íš¨)
            for lid in link_ids:
                if len(lid) == 10 and lid.isdigit():
                    rec = base.copy()
                    rec["LINK_ID"] = lid  # ê°œë³„ IDë¡œ ì¹˜í™˜
                    records_expanded.append(rec)
                else:
                    invalid_ids.add(lid)

        # ë°ì´í„°í”„ë ˆì„ êµ¬ì„±
        df = pd.DataFrame.from_records(records_expanded)

        if df.empty:
            # ê·¸ë˜ë„ ìŠ¤í‚¤ë§ˆëŠ” ìœ ì§€
            df = pd.DataFrame(columns=[
                "AREA", "SUBAREA", "LINK_ID", "ROAD_NAME", "UPDOWN",
                "vc", "vehs", "speed", "STAT_DAY"
            ])
            df["STAT_DAY"] = stat_day
            print("      ğŸ“Š DataFrame í¬ê¸°: 0 í–‰ Ã— {0} ì—´".format(len(df.columns)))
            if invalid_ids:
                print(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ LINK_ID ì˜ˆì‹œ(ìµœëŒ€ 5ê°œ): {list(sorted(invalid_ids))[:5]}")
            return df

        # ì»¬ëŸ¼ëª… í†µì¼ + ìˆ«ìí˜• ë³´ì •(ì†Œìˆ˜ ë‘˜ì§¸ ìë¦¬)
        df.rename(columns={
            "VolCapRatioPrT(AP)": "vc",
            "VolVehPrT(AP)": "vehs",
            "SUBAREA": "sa",
            "AREA": "DISTRICT",
            "TCur_PrTSys(a)": "speed"
        }, inplace=True)

        df["DISTRICT"]   = pd.to_numeric(df["DISTRICT"], errors="coerce")
        df["UPDOWN"] = pd.to_numeric(df["UPDOWN"], errors="coerce")
        df["vc"]     = pd.to_numeric(df["vc"], errors="coerce").round(2)
        df["vehs"] = pd.to_numeric(df["vehs"], errors="coerce").fillna(0).astype(int)
        df["speed"]  = pd.to_numeric(df["speed"], errors="coerce").round(2)

        # ë¬¸ìì—´ ì»¬ëŸ¼ì€ í™•ì‹¤íˆ ë¬¸ìì—´/Noneë¡œ
        for c in ("LINK_ID", "sa", "ROAD_NAME"):
            if c in df.columns:
                df[c] = df[c].astype(object).where(pd.notna(df[c]), None)
                df[c] = df[c].map(lambda x: str(x) if x is not None else None)

        # 3) LINK_ID ê¸°ì¤€ ì¤‘ë³µ ì œê±° (ì²« ë“±ì¥ í–‰ ìš°ì„ )
        before = len(df)
        df.drop_duplicates(subset=["LINK_ID"], keep="first", inplace=True)
        after = len(df)

        # (ì„ íƒ) ì •ë ¬
        df.sort_values(by=["LINK_ID"], inplace=True, ignore_index=True)

        df["STAT_DAY"] = stat_day

        print(f"      ğŸ“Š DataFrame í¬ê¸°: {len(df)} í–‰ Ã— {len(df.columns)} ì—´"
            f" (ì¤‘ë³µ ì œê±°: {before - after}ê±´)")
        if invalid_ids:
            print(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ LINK_ID ì˜ˆì‹œ(ìµœëŒ€ 5ê°œ): {list(sorted(invalid_ids))[:5]}")

        return df

    # --------------------------------------------------------------- [ CSV/DB I/O ]
    
    def export_csv(self, df: pd.DataFrame, run_type: str, hour: str | None) -> str:
        if df.empty:
            print("â›” CSV ë‚´ë³´ë‚¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return ""
        os.makedirs(self.csv_out_dir, exist_ok=True)  # ë””ë ‰í„°ë¦¬ ë³´ì¥

        sd = self._require_stat_day()
        ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        kind = "prevday" if run_type == "prev_day" else f"h{hour}"
        fname = f"links_{sd}_{kind}_{ts}.csv"
        fpath = os.path.join(self.csv_out_dir, fname)

        # ì—‘ì…€ ê²€í†  í¸ì˜ ìœ„í•´ na_rep ì§€ì •(ì„ íƒ)
        df.to_csv(fpath, index=False, encoding="utf-8-sig", na_rep="")
        print(f">>>>> âœ… ê²°ê³¼ê°’ì„ CSVíŒŒì¼ë¡œ ì €ì¥ì„ ì™„ë£Œí•˜ì˜€ìŠµë‹ˆë‹¤. ê²½ë¡œ: {fpath}")
        return fpath
    
    def _coerce_numeric(self, df: pd.DataFrame) -> pd.DataFrame:
        out = df.copy()
        if "DISTRICT" in out.columns:
            out["DISTRICT"] = pd.to_numeric(out["DISTRICT"], errors="coerce")
        if "UPDOWN" in out.columns:
            out["UPDOWN"] = pd.to_numeric(out["UPDOWN"], errors="coerce")
        if "vc" in out.columns:
            out["vc"] = pd.to_numeric(out["vc"], errors="coerce").round(2)
        if "vehs" in out.columns:
            out["vehs"] = pd.to_numeric(out["vehs"], errors="coerce").fillna(0).astype(int)
        if "speed" in out.columns:
            out["speed"] = pd.to_numeric(out["speed"], errors="coerce").round(2)
        return out

    def _rename_for_db(self, df: pd.DataFrame) -> pd.DataFrame:
        out = df.copy()
        out.rename(columns={"sa": "SA_NO", "vc": "VC", "vehs": "VEHS", "speed": "SPEED"}, inplace=True)
        return out

    def _coerce_str_fields(self, df: pd.DataFrame, fields=("LINK_ID","SA_NO","ROAD_NAME")) -> pd.DataFrame:
        out = df.copy()
        for c in fields:
            if c in out.columns:
                out[c] = out[c].astype(object).where(pd.notna(out[c]), None)
                out[c] = out[c].map(lambda x: str(x) if x is not None else None)
        return out











    # --------------------------------------------------------------- [ ì „ì¼ ê²°ê³¼ê°’ insert ]

    def insert_day_link_results(self, df: pd.DataFrame, db_conn, chunk_size: int = 20000) -> int:
        if df is None or df.empty or db_conn is None:
            print("â›” ì „ì¼ INSERT: DF/DB ëˆ„ë½")
            return 0

        required = ["STAT_DAY","LINK_ID","DISTRICT","SA_NO","ROAD_NAME","UPDOWN","VC","VEHS","SPEED"]

        # 1) ì¤€ë¹„ ë° ë¶ˆí•„ìš” ì»¬ëŸ¼ ì œê±°
        work = df.copy()
        for c in ("run_type", "hour"):
            if c in work.columns:
                work.drop(columns=[c], inplace=True)

        # 2) ìŠ¤í‚¤ë§ˆ ì»¬ëŸ¼ëª… ë§ì¶¤ + STAT_DAY ì„¸íŒ…
        work.rename(columns={"sa": "SA_NO", "vc": "VC", "vehs": "VEHS", "speed": "SPEED"}, inplace=True)
        work["STAT_DAY"] = self._require_stat_day()

        # 3) ìˆ«ìí˜• ë³´ì •
        #    - DISTRICT, UPDOWN, VEHS: ì •ìˆ˜í˜• (nullable Int64)
        #    - VC: float(ê·¸ëŒ€ë¡œ), SPEED: ìˆ«ìí™” + 360000 ì´ìƒì€ 0
        work["DISTRICT"]   = pd.to_numeric(work.get("DISTRICT"), errors="coerce").astype("Int64")
        work["UPDOWN"] = pd.to_numeric(work.get("UPDOWN"), errors="coerce").astype("Int64")
        work["VEHS"] = pd.to_numeric(work.get("VEHS"), errors="coerce").astype("Int64")
        if "VC" in work.columns:
            work["VC"] = pd.to_numeric(work["VC"], errors="coerce")
        if "SPEED" in work.columns:
            work["SPEED"] = pd.to_numeric(work["SPEED"], errors="coerce")
            work.loc[work["SPEED"] >= 360000, "SPEED"] = 0  # ë¹„ì •ìƒ ì†ë„ê°’ ë°©ì§€

        # 4) ë¬¸ìì—´ ì»¬ëŸ¼: ê³µë°±/ë¹ˆë¬¸ì -> None
        def _str_or_none(x):
            if pd.isna(x):
                return None
            s = str(x).strip()
            return s if s != "" else None

        for c in ("LINK_ID", "SA_NO", "ROAD_NAME"):
            if c in work.columns:
                work[c] = work[c].map(_str_or_none)

        # ê¸¸ì´ ì œí•œ(ìŠ¤í‚¤ë§ˆ ë³´í˜¸)
        if "ROAD_NAME" in work.columns:
            work["ROAD_NAME"] = work["ROAD_NAME"].map(lambda x: x[:200] if x is not None else None)
        if "LINK_ID" in work.columns:
            work["LINK_ID"] = work["LINK_ID"].map(lambda x: x[:400] if x is not None else None)
        if "SA_NO" in work.columns:
            work["SA_NO"] = work["SA_NO"].map(lambda x: x[:10]  if x is not None else None)

        # 5) í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬ + ì •ë ¬
        missing = [c for c in required if c not in work.columns]
        if missing:
            print(f"â›” í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}")
            return 0
        work = work[required]  # ì—´ ì •ë ¬ë§Œ ìˆ˜í–‰, í–‰ í•„í„°ë§ ì—†ìŒ

        # 6) íŒŒë¼ë¯¸í„° ë°”ì¸ë”©ìš© Python ê¸°ë³¸í˜•ìœ¼ë¡œ ë³€í™˜
        def _to_db_value(v):
            # pandas NA -> None
            if v is pd.NA:
                return None
            # numpy -> python ê¸°ë³¸í˜•
            if isinstance(v, np.generic):
                v = v.item()
            # float NaN -> None
            if isinstance(v, float) and (pd.isna(v) or np.isnan(v)):
                return None
            return v

        # Tibero(ì˜¤ë¼í´ í˜¸í™˜)ì—ì„œ íŒŒë¼ë¯¸í„°ì— Python Noneì„ ë„˜ê¸°ë©´ DBì˜ NULLë¡œ ë“¤ì–´ê°„ë‹¤.
        sql = f"INSERT INTO TOMMS.DAY_LINK_RESULT ({', '.join(required)}) VALUES ({', '.join(['?']*len(required))})"
        cur = db_conn.cursor()
        cur.fast_executemany = False

        # 7) SQL ë¡œê·¸ ì €ì¥(ê²€ì¦ìš©)
        os.makedirs("./output", exist_ok=True)
        sql_log_path = "./output/day_link_result_insert.sql.txt"

        def _sql_literal(v):
            if v is None:
                return "NULL"
            if isinstance(v, (int, np.integer)):
                return str(int(v))
            if isinstance(v, (float, np.floating)):
                # ì†Œìˆ˜ì  í‘œí˜„ ì•ˆì •í™”
                return str(float(v))
            # ë¬¸ìì—´ ì´ìŠ¤ì¼€ì´í”„
            s = str(v).replace("'", "''")
            return "'" + s + "'"

        total = 0
        try:
            with open(sql_log_path, "w", encoding="utf-8") as f_log:
                for s in range(0, len(work), chunk_size):
                    chunk = work.iloc[s:s+chunk_size]
                    data = [tuple(_to_db_value(v) for v in row) for row in chunk.itertuples(index=False, name=None)]

                    # ë¡œê·¸ìš© ì‹¤ì œ SQL
                    for row in data:
                        values_str = [_sql_literal(v) for v in row]
                        f_log.write(
                            f"INSERT INTO TOMMS.DAY_LINK_RESULT ({', '.join(required)}) VALUES ({', '.join(values_str)});\n"
                        )

                    cur.executemany(sql, data)
                    total += len(data)

            db_conn.commit()
            print(f"      âœ… DAY_LINK_RESULT INSERT â€” {total}í–‰")
            print(f"      ğŸ“‚ SQL ë¡œê·¸ ì €ì¥: {sql_log_path}")
            return total

        except Exception as ex:
            db_conn.rollback()
            print(f"â›” DAY_LINK_RESULT INSERT ì˜¤ë¥˜ â€” ë¡¤ë°±: {ex}")
            print(f"ğŸ“‚ SQL ë¡œê·¸(ì‹¤íŒ¨ ì‹œì ê¹Œì§€): {sql_log_path}")
            return 0

    # --------------------------------------------------------------- [ ì „ì¼ ì‹œë®¬ë ˆì´ì…˜ íŒŒì´í”„ë¼ì¸ ]
    
    def run_prev_day_pipeline(self, day_payload, db_conn=None, preferred_day: str | None = None):
        # 0) ê¸°ì¤€ì¼ í™•ì • (ë‹¨ í•œ ë²ˆ)
        payload_days = list(day_payload.keys()) if isinstance(day_payload, dict) else None
        chosen_day = self.ensure_stat_day(preferred_day, payload_days)

        # 1) payload íŒŒì‹±
        if isinstance(day_payload, dict):
            payload_list = day_payload.get(chosen_day, [])
            print(f">>>>> âœ… ì „ì¼ êµí†µëŸ‰ ì—°ê³„ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘\n      ëŒ€ìƒì¼:{chosen_day}\n      entries:{len(payload_list)}")
        else:
            payload_list = day_payload or []
            print(f">>>>> âœ… ì „ì¼ êµí†µëŸ‰ ì—°ê³„ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘\n      ëŒ€ìƒì¼:{chosen_day}\n      (list ì…ë ¥) entries:{len(payload_list)}")

        # 2) êµí†µëŸ‰ ì£¼ì…
        upd = self.insert_turn_volumes(payload_list, verbose=True)
        print(f">>>>> âœ… ì „ì¼ ì£¼ì… ê±´ìˆ˜: {upd}")
        if upd == 0:
            print("â›” ì „ì¼ ì£¼ì… 0ê±´ â€” ë§¤í•‘/ì…ë ¥ê°’ í™•ì¸ ê¶Œì¥")

        # 3) ì‹œë®¬ â†’ ê²°ê³¼ â†’ CSV â†’ DB
        self.simulate_prev_day()

        # ê°€ë“œì²´í¬: ì „ì¼ë¡œ ì„¸íŒ…ëëŠ”ì§€ í™•ì¸
        assert self.last_run.get("type") == "prev_day" and self.last_run.get("hour") is None, \
            f"last_run ë¶ˆì¼ì¹˜: {self.last_run}"

        df = self.get_links_result_df()
        self.insert_day_link_results(df, db_conn=db.conn)

        print(">>>>> âœ… ì „ì¼ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ")

    # --------------------------------------------------------------- [ ì‹œê°„ëŒ€ë³„ ê²°ê³¼ê°’ insert ]
    
    def insert_hour_link_results(self, df: pd.DataFrame, db_conn, chunk_size: int = 20000) -> int:
        if df is None or df.empty or db_conn is None:
            print("â›” ì‹œê°„ëŒ€ INSERT: DF/DB ëˆ„ë½")
            return 0

        stat_day = self._require_stat_day()
        hour_lbl = (self.last_run or {}).get("hour")
        if not hour_lbl:
            print("â›” last_run.hour ì—†ìŒ â€” simulate_hour ì´í›„ í˜¸ì¶œ í•„ìš”")
            return 0

        required = ["STAT_HOUR","LINK_ID","DISTRICT","SA_NO","ROAD_NAME","UPDOWN","VC","VEHS","SPEED"]

        # 1) ì‘ì—…ìš© ë³µì‚¬ & ë¶ˆí•„ìš” ì»¬ëŸ¼ ì œê±°
        work = df.copy()
        for c in ("run_type", "hour"):
            if c in work.columns:
                work.drop(columns=[c], inplace=True)

        # 2) ìŠ¤í‚¤ë§ˆ ì»¬ëŸ¼ëª… ì •ë¦¬ + STAT_HOUR ì„¸íŒ…
        work.rename(columns={"sa": "SA_NO", "vc": "VC", "vehs": "VEHS", "speed": "SPEED"}, inplace=True)
        work["STAT_HOUR"] = stat_day + hour_lbl  # ì˜ˆ: 2025070109

        # 3) ìˆ«ìí˜• ë³´ì •
        #    - DISTRICT/UPDOWN/VEHS: nullable ì •ìˆ˜(Int64)
        #    - VC: float ê·¸ëŒ€ë¡œ(ë¼ìš´ë”©/ìŠ¤ì¼€ì¼ë§ ì—†ìŒ)
        #    - SPEED: ìˆ«ìí™” + (>=360000 â†’ 0), ê·¸ ì™¸ ì›ë³¸
        work["DISTRICT"]   = pd.to_numeric(work.get("DISTRICT"), errors="coerce").astype("Int64")
        work["UPDOWN"] = pd.to_numeric(work.get("UPDOWN"), errors="coerce").astype("Int64")
        work["VEHS"] = pd.to_numeric(work.get("VEHS"), errors="coerce").astype("Int64")
        if "VC" in work.columns:
            work["VC"] = pd.to_numeric(work["VC"], errors="coerce")
        if "SPEED" in work.columns:
            work["SPEED"] = pd.to_numeric(work["SPEED"], errors="coerce")
            work.loc[work["SPEED"] >= 360000, "SPEED"] = 0

        # 4) ë¬¸ìì—´: ê³µë°± ì œê±° í›„ ë¹ˆ ê°’ì€ None
        def _str_or_none(x):
            if pd.isna(x):
                return None
            s = str(x).strip()
            return s if s != "" else None

        for c in ("LINK_ID", "SA_NO", "ROAD_NAME"):
            if c in work.columns:
                work[c] = work[c].map(_str_or_none)

        # ê¸¸ì´ ì œí•œ(ìŠ¤í‚¤ë§ˆ ë³´í˜¸)
        if "ROAD_NAME" in work.columns:
            work["ROAD_NAME"] = work["ROAD_NAME"].map(lambda x: x[:200] if x is not None else None)
        if "LINK_ID" in work.columns:
            work["LINK_ID"] = work["LINK_ID"].map(lambda x: x[:400] if x is not None else None)
        if "SA_NO" in work.columns:
            work["SA_NO"] = work["SA_NO"].map(lambda x: x[:10]  if x is not None else None)

        # 5) í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬ + ì»¬ëŸ¼ ì •ë ¬(í–‰ í•„í„°ë§ ì—†ìŒ)
        missing = [c for c in required if c not in work.columns]
        if missing:
            print(f"â›” í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}")
            return 0
        work = work[required]

        # 6) íŒŒë¼ë¯¸í„° ë°”ì¸ë”©ìš© Python ê¸°ë³¸í˜•ìœ¼ë¡œ ë³€í™˜ (pandas/NumPy â†’ ê¸°ë³¸í˜•, NA/NaN â†’ None)
        def _to_db_value(v):
            if v is pd.NA:
                return None
            if isinstance(v, np.generic):
                v = v.item()
            if isinstance(v, float) and (pd.isna(v) or np.isnan(v)):
                return None
            return v

        sql = f"INSERT INTO TOMMS.HOUR_LINK_RESULT ({', '.join(required)}) VALUES ({', '.join(['?']*len(required))})"
        cur = db_conn.cursor()
        cur.fast_executemany = True

        total = 0
        try:
            for s in range(0, len(work), chunk_size):
                chunk = work.iloc[s:s+chunk_size]
                data = [tuple(_to_db_value(v) for v in row) for row in chunk.itertuples(index=False, name=None)]
                cur.executemany(sql, data)
                total += len(data)

            db_conn.commit()
            print(f"      âœ… HOUR_LINK_RESULT INSERT â€” {stat_day+hour_lbl} {total}í–‰")
            return total

        except Exception as ex:
            db_conn.rollback()
            print(f"â›” HOUR_LINK_RESULT INSERT ì˜¤ë¥˜ â€” ë¡¤ë°±: {ex}")
            return 0

    # --------------------------------------------------------------- [ ì‹œê°„ëŒ€ë³„ ì‹œë®¬ë ˆì´ì…˜ íŒŒì´í”„ë¼ì¸ ]

    def run_hourly_pipeline(self, hourly_payload_map: dict, db_conn=None):
        """
        ê³ ì • ìˆœì„œ: 08 â†’ 11 â†’ 14 â†’ 17
        STAT_DAYëŠ” ì´ë¯¸ ensure_stat_day/set_stat_dayë¡œ í™•ì •ë˜ì–´ ìˆì–´ì•¼ í•¨.
        """
        self._require_stat_day()
        print(f">>>>> âœ… ì‹œê°„ëŒ€ êµí†µëŸ‰ ì—°ê³„ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘\n      STAT_DAY={self.last_run['stat_day']}")

        for hh in [8, 11, 14, 17]:
            key = f"{hh:02d}"
            payload = hourly_payload_map.get(key, [])
            print(f">>>>> âœ… {key}ì‹œ ì²˜ë¦¬ ì‹œì‘ â€” êµì°¨ë¡œ:{len(payload)}")

            upd = self.insert_turn_volumes(payload, verbose=False)
            print(f">>>>> âœ… {key}ì‹œ ì£¼ì… ê±´ìˆ˜: {upd}")

            self.simulate_hour(hh)

            # ê°€ë“œì²´í¬: ì‹œê°„ëŒ€ ì„¸íŒ… í™•ì¸
            assert self.last_run.get("type") == "hourly" and self.last_run.get("hour") == key, \
                f"last_run ë¶ˆì¼ì¹˜: {self.last_run}"

            df = self.get_links_result_df()
            self.insert_hour_link_results(df, db_conn=db.conn)

        print(">>>>> âœ… ì‹œê°„ëŒ€ë³„ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ")











# ====================================================================================== [ main ì‹¤í–‰í•¨ìˆ˜ ]

if __name__ == "__main__":
    
    print(">>>>> âœ… VISUM ìë™í™” ì‹œë®¬ë ˆì´ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    USE_FIXED_TIME = True # ì‹¤ì‹œê°„ ë‹¨ìœ„ë¡œ ë³€ê²½í•˜ë ¤ë©´ ì´ ê°’ì„ Falseë¡œ ì„¤ì •.
    
    if USE_FIXED_TIME:
        fixed_now = datetime.datetime.strptime("2025070204", "%Y%m%d%H")
        query_day, target_stat_hours = compute_target_hours(fixed_now, ["08", "11", "14", "17"])
    else:
        query_day, target_stat_hours = compute_target_hours(None, ["08", "11", "14", "17"])

    # 1) DB
    config = Config()
    db = DatabaseManager(config)
    print(">>>>> âœ… Config, DB í´ë˜ìŠ¤ê°€ ì„ ì–¸ë˜ì–´ main í•¨ìˆ˜ ë‚´ ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    try:
        print(">>>>> âœ… êµí†µëŸ‰ ë°ì´í„° ì¡°íšŒë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        traffic_by_hour, traffic_by_day, query_day_from_db = db.fetch_and_process_data(target_stat_hours)
        
        # query_day ìš°ì„ ìˆœìœ„: DBì—ì„œ ìœ ì¶”í•œ ê°’ì´ ìˆìœ¼ë©´ ê·¸ê±¸ ì‚¬ìš©
        stat_day_final = query_day_from_db or query_day
        
        vis = VisumSimulationManager(
            base_path=r"C:/Digital Twin Simulation network/VISUM",
            default_version_name="ê°•ë¦‰ì‹œ ì „êµ­ ì „ì¼ ìµœì¢…ë³¸.ver",
            prev_day_proc_no=22,
            csv_out_dir=r"C:/Digital Twin Simulation network/VISUM/result_export",
        )
        
        print(">>>>> âœ… Visum í´ë˜ìŠ¤ê°€ ì„ ì–¸ë˜ì–´ main í•¨ìˆ˜ ë‚´ ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

        # 4) Visum open & load
        vis.open()  # default_version_name ì‚¬ìš©
        vis.set_stat_day(stat_day_final)  # â˜… ê¸°ì¤€ì¼ í™•ì •(í•œ ë²ˆë§Œ)

        # 5) ì „ì¼ íŒŒì´í”„ë¼ì¸
        vis.run_prev_day_pipeline(traffic_by_day, db_conn=db.conn, preferred_day=stat_day_final) # traffic_by_day: {"YYYYMMDD": [ ... ]} êµ¬ì¡°

        # 6) ì‹œê°„ëŒ€ íŒŒì´í”„ë¼ì¸(08â†’11â†’14â†’17)
        vis.run_hourly_pipeline(traffic_by_hour, db_conn=db.conn) # traffic_by_hour: {"00":[...], ..., "23":[...]}

    finally:
        # 7) ë§ˆë¬´ë¦¬
        if 'vis' in locals():
            vis.close()
        db.close()