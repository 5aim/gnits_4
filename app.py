import datetime, pyodbc, os, subprocess, json, pathlib
import pandas as pd
import numpy as np

from decimal import Decimal
from collections import defaultdict
from flask import Flask, request, jsonify, render_template, redirect, url_for, Response
from flask_cors import CORS
from dotenv import load_dotenv
from apscheduler.schedulers.background import BackgroundScheduler
from windows import set_dpi_awareness


set_dpi_awareness()
app = Flask(__name__, static_url_path='/static', static_folder='static')
CORS(app)

# ========================================================= [ í˜„ì¬ì‹œê°„ ]

def get_current_time():
    return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

# ========================================================= [ VISUM ìë™í™” ì½”ë“œ ì‹¤í–‰ ]

def run_visum_script():
    script_path = os.path.join(os.path.dirname(__file__), 'auto simulation', 'auto_visum.py')
    print(f"âœ… [ {get_current_time()} ] Vissim ìë™í™” ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰")
    subprocess.Popen(['python', script_path])
    
# ========================================================= [ VISSIM ìë™í™” ì½”ë“œ ì‹¤í–‰ ]

def run_vissim_script():
    base_dir = os.path.abspath(os.path.dirname(__file__))
    script_path = os.path.join(base_dir, 'auto simulation', 'auto_vissim.py')
    
    if not os.path.exists(script_path):
        print(f"âŒ ê²½ë¡œ ì˜¤ë¥˜: {script_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… [ {get_current_time()} ] Vissim ìë™í™” ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰")
    subprocess.Popen(['python', script_path], cwd=os.path.dirname(script_path))

# ========================================================= [ ìë™í™” ì‹œë®¬ë ˆì´ì…˜ ìŠ¤ì¼€ì¥´ëŸ¬ ì„¤ì • ]

# nohup python app.py > server.log 2>&1 &
scheduler = BackgroundScheduler()
scheduler.add_job(run_visum_script, 'cron', hour=2, minute=0)
scheduler.add_job(run_vissim_script, 'cron', hour=1, minute=0)
scheduler.start()

# ========================================================= [ í‹°ë² ë¡œ ì—°ê²° ]

load_dotenv()
FLASK_ENV = os.getenv("FLASK_ENV", "production")

# ê°•ë¦‰ ì„¼í„°ìš©
DSNNAME = os.getenv("DSNNAME")
DBUSER = os.getenv("DBUSER")
DBPWD = os.getenv("DBPWD")

# ì—”ì œë¡œ í…ŒìŠ¤íŠ¸ìš©
ENZERO_SERVER = os.getenv("ENZERO_SERVER")
ENZERO_PORT = os.getenv("ENZERO_PORT")
ENZERO_DB = os.getenv("ENZERO_DB")
ENZERO_UID = os.getenv("ENZERO_UID")
ENZERO_PWD = os.getenv("ENZERO_PWD")

def get_connection():
    if FLASK_ENV == "test":
        print(f">>> [INFO] Flask í™˜ê²½ ì„¤ì •: {FLASK_ENV}")
        return pyodbc.connect(
            f"DRIVER=Tibero 5 ODBC Driver;"
            f"SERVER={ENZERO_SERVER};"
            f"PORT={ENZERO_PORT};"
            f"DB={ENZERO_DB};"
            f"UID={ENZERO_UID};"
            f"PWD={ENZERO_PWD};"
        )
    else:
        print(f">>> [INFO] Flask í™˜ê²½ ì„¤ì •: {FLASK_ENV}")
        return pyodbc.connect(
            f"DSN={DSNNAME};"
            f"UID={DBUSER};"
            f"PWD={DBPWD}"
        )

# Decimal ì²˜ë¦¬ í•¨ìˆ˜
def convert_decimal(obj):
    if isinstance(obj, Decimal):
        return float(obj)
    return obj

# ê¶Œì—­ë³„ ë§¤í•‘
district_mapping = {
    1: "êµë™ì§€êµ¬",
    2: "ì†¡ì •ë™",
    3: "ë„ì‹¬",
    4: "ì•„ë ˆë‚˜"
}

hourly_mapping = {
    "08": "ì˜¤ì „ì²¨ë‘ 08ì‹œ ~ 09ì‹œ",
    "11": "ì˜¤ì „ë¹„ì²¨ë‘ 11ì‹œ ~ 12ì‹œ",
    "14": "ì˜¤í›„ë¹„ì²¨ë‘ 14ì‹œ ~ 15ì‹œ",
    "17": "ì˜¤í›„ì²¨ë‘ 17ì‹œ ~ 18ì‹œ"
}

# ì§€ì²´ì‹œê°„ ê¸°ì¤€ LOS
def get_los(delay):
    if delay < 15: return "A"
    elif delay < 30: return "B"
    elif delay < 50: return "C"
    elif delay < 70: return "D"
    elif delay < 100: return "E"
    elif delay < 220: return "F"
    elif delay < 340: return "FF"
    else: return "FFF"

# ========================================================= [ ë¡œê·¸ì¸ ]

@app.route('/')
def index():
    return redirect(url_for('login'))

@app.route('/login')
def login():
    return render_template('login.html')

# ========================================================= [ íšŒì›ê°€ì… ]

@app.route('/sign-up')
def sign_up():
    return render_template('sign_up.html')

# ========================================================= [ ë©”ì¸í˜ì´ì§€ ]

@app.route('/home')
def home():
    return render_template('home.html')

# ========================================================= [ ë”¥ëŸ¬ë‹ í•™ìŠµ ]

# @app.route('/gndl-learn-start', methods=['GET'])
# def deeplearning_learn_start():
#     try:
#         logs = []
        
#         VENV_PYTHON = os.path.join(os.getcwd(), "venv", "Scripts", "python.exe")
        
#         base_dir = os.path.join(os.getcwd(), "gndl")
#         gnn_data_dir = os.path.join(base_dir, "gnn_data")

#         pkl_files = ["node_features.pkl", "edge_list.pkl", "node_index.pkl"]
#         pkl_paths = [os.path.join(gnn_data_dir, f) for f in pkl_files]

#         # STEP 1: ì „ì²˜ë¦¬ í•„ìš” ì—¬ë¶€ í™•ì¸
#         if not all(os.path.exists(p) for p in pkl_paths):
#             logs.append("ğŸŸ¡ ì „ì²˜ë¦¬ ë°ì´í„° ì—†ìŒ â†’ 1.preprocess.py ì‹¤í–‰")
#             result = subprocess.run([VENV_PYTHON, "gndl/1.preprocess.py"], capture_output=True, text=True)
#             logs.append(result.stdout or result.stderr)
#         else:
#             logs.append("âœ… ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì¡´ì¬í•˜ì—¬ ê±´ë„ˆëœ€")

#         # STEP 2: í•™ìŠµ ëª¨ë¸ íŒŒì¼ ì²´í¬
#         model_path = os.path.join(gnn_data_dir, "best_model.pt")
#         if not os.path.exists(model_path):
#             logs.append("ğŸŸ¡ ëª¨ë¸ ì—†ìŒ â†’ 2.gnn.py ì‹¤í–‰")
#             result = subprocess.run([VENV_PYTHON, "gndl/2.gnn.py"], capture_output=True, text=True)
#             logs.append(result.stdout or result.stderr)
#         else:
#             logs.append("âœ… í•™ìŠµëœ ëª¨ë¸ì´ ì¡´ì¬í•˜ì—¬ ê±´ë„ˆëœ€")

#         # STEP 3: ì‹œê°í™” ê²°ê³¼ í™•ì¸
#         visual_path = os.path.join(gnn_data_dir, "visual_result.png")
#         if not os.path.exists(visual_path):
#             logs.append("ğŸŸ¡ ì‹œê°í™” ì—†ìŒ â†’ 3.gnn_visual.py ì‹¤í–‰")
#             result = subprocess.run([VENV_PYTHON, "gndl/3.gnn_visual.py"], capture_output=True, text=True)
#             logs.append(result.stdout or result.stderr)
#         else:
#             logs.append("âœ… ì‹œê°í™” ê²°ê³¼ê°€ ì¡´ì¬í•˜ì—¬ ê±´ë„ˆëœ€")

#         # STEP 4: ë¯¸ë˜ ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸
#         pred_path = os.path.join(gnn_data_dir, "future_prediction.json")
#         if not os.path.exists(pred_path):
#             logs.append("ğŸŸ¡ ë¯¸ë˜ ì˜ˆì¸¡ ì—†ìŒ â†’ 4.future_prediction.py ì‹¤í–‰")
#             result = subprocess.run([VENV_PYTHON, "gndl/4.future_prediction.py"], capture_output=True, text=True)
#             logs.append(result.stdout or result.stderr)
#         else:
#             logs.append("âœ… ë¯¸ë˜ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì¡´ì¬í•˜ì—¬ ê±´ë„ˆëœ€")

#         return jsonify({
#             "status": "success",
#             "message": "GNN ë”¥ëŸ¬ë‹ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ",
#             "logs": logs
#         })

#     except Exception as e:
#         return jsonify({
#             "status": "fail",
#             "message": "âŒ GNN í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
#             "error": str(e),
#             "logs": logs
#         })




# ========================================================= [ ëª¨ë‹ˆí„°ë§ 1 - ì‹œê°„ëŒ€ë³„ êµí†µìˆ˜ìš” ë¶„ì„ì •ë³´ ]

@app.route('/monitoring/visum-hourly-vc', methods=['GET'])
def visum_hourly_vc():
    pass

# ========================================================= [ ëª¨ë‹ˆí„°ë§ 2 - êµí†µì¡´ê°„ í†µí–‰ì •ë³´ ]

@app.route('/monitoring/visum-zone-od', methods=['GET'])
def visum_zone_od():
    try:
        conn = get_connection()
        cursor = conn.cursor()

        # [1] VISUM_ZONE_INFO â†’ ZONE_ID, LAT, LON
        cursor.execute("SELECT ZONE_ID, LAT, LON FROM VISUM_ZONE_INFO")
        rows_info = cursor.fetchall()
        df_zone_info = pd.DataFrame(
            [[str(r[0]), float(r[1]), float(r[2])] for r in rows_info],
            columns=["ZONE_ID", "LAT", "LON"]
        )

        # [2] VISUM_ZONE_OD â†’ OD Matrix
        cursor.execute("""
            SELECT FROM_ZONE_ID, FROM_ZONE_NAME,
            TO_ZONE_ID, TO_ZONE_NAME,
            AUTO_MATRIX_VALUE, BUS_MATRIX_VALUE, HGV_MATRIX_VALUE
            FROM VISUM_ZONE_OD
        """)
        rows_od = cursor.fetchall()
        df_od = pd.DataFrame([
            [
                str(r[0]), str(r[1]), str(r[2]), str(r[3]),
                float(r[4] or 0), float(r[5] or 0), float(r[6] or 0)
            ] for r in rows_od
        ], columns=[
            "FROM_ZONE_ID", "FROM_ZONE_NAME",
            "TO_ZONE_ID", "TO_ZONE_NAME",
            "AUTO_MATRIX_VALUE", "BUS_MATRIX_VALUE", "HGV_MATRIX_VALUE"
        ])

        # [3] OD_MATRIX_VALUE ê³„ì‚°
        df_od["OD_MATRIX_VALUE"] = (
            df_od["AUTO_MATRIX_VALUE"] +
            df_od["BUS_MATRIX_VALUE"] +
            df_od["HGV_MATRIX_VALUE"]
        ).round(6)

        # [4] ìƒìœ„ 5ê°œ TO_ZONE ì¶”ì¶œ
        df_top5 = (
            df_od.sort_values(["FROM_ZONE_ID", "OD_MATRIX_VALUE"], ascending=[True, False])
            .groupby("FROM_ZONE_ID")
            .head(5)
            .reset_index(drop=True)
        )

        # [5] FROM ZONE ì¢Œí‘œ ë³‘í•©
        df_top5 = df_top5.merge(
            df_zone_info.rename(columns={
                "ZONE_ID": "FROM_ZONE_ID",
                "LAT": "FROM_LAT",
                "LON": "FROM_LON"
            }),
            on="FROM_ZONE_ID", how="left"
        )

        # [6] TO ZONE ì¢Œí‘œ ë³‘í•©
        df_top5 = df_top5.merge(
            df_zone_info.rename(columns={
                "ZONE_ID": "TO_ZONE_ID",
                "LAT": "TO_LAT",
                "LON": "TO_LON"
            }),
            on="TO_ZONE_ID", how="left"
        )

        # [7] ë³€í™˜ â†’ ì¤‘ì°¨ êµ¬ì¡°ë¡œ ê·¸ë£¹
        result = []
        grouped = df_top5.groupby(["FROM_ZONE_ID", "FROM_ZONE_NAME", "FROM_LAT", "FROM_LON"])
        for (from_id, from_name, from_lat, from_lon), group_df in grouped:
            destinations = []
            for _, row in group_df.iterrows():
                destinations.append({
                    "to_zone_name": row["TO_ZONE_NAME"],
                    "coordinates": [row["TO_LAT"], row["TO_LON"]],
                    "value": round(row["OD_MATRIX_VALUE"], 2)
                })
            result.append({
                "coordinates": [from_lat, from_lon],
                "from_zone_name": from_name,
                "destination": destinations
            })

        conn.close()
        print(f"âœ… [ {get_current_time()} ] OD Matrix ì‘ë‹µ {len(result)}ê°œ ê·¸ë£¹ ì™„ë£Œ")

        return jsonify(result), 200

    except Exception as e:
        print(f"âŒ [ {get_current_time()} ] OD Matrix ì²˜ë¦¬ ì—ëŸ¬: {str(e)}")
        return jsonify({
            "status": "fail",
            "message": "OD ë¶„ì„ ì¤‘ ì—ëŸ¬ ë°œìƒ",
            "error": str(e),
            "timestamp": get_current_time()
        }), 500

# ========================================================= [ ëª¨ë‹ˆí„°ë§ 3 - ë¶„ì„ì§€ì—­ë³„ êµí†µíë¦„ í†µê³„ì •ë³´ ] - 4k 

@app.route('/monitoring/statistics-traffic-flow/node-result', methods=['GET'])
def statistics_traffic_flow():
    try:
        conn = get_connection()
        cursor = conn.cursor()

        # ğŸ“Œ 1. ìµœì‹  STAT_HOUR ì¶”ì¶œ
        cursor.execute("""
            SELECT STAT_DATE FROM (
                SELECT SUBSTR(STAT_HOUR, 1, 8) AS STAT_DATE
                FROM NODE_RESULT
                GROUP BY SUBSTR(STAT_HOUR, 1, 8)
                ORDER BY STAT_DATE DESC
            )
            WHERE ROWNUM = 1
        """)
        latest_date_row = cursor.fetchone()
        if not latest_date_row:
            return jsonify({"status": "fail", "message": "STAT_HOUR ë‚ ì§œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}), 404

        latest_date = latest_date_row[0]

        # ğŸ“Œ 2. NODE_RESULT: delayë§Œ ê°„ë‹¨íˆ ì¡°íšŒ
        cursor.execute("""
            SELECT DISTRICT, STAT_HOUR, NODE_ID, DELAY, VEHS
            FROM NODE_RESULT
            WHERE SUBSTR(STAT_HOUR, 1, 8) = ?
        """, [latest_date])
        result_rows = [tuple(row) for row in cursor.fetchall()]

        if not result_rows:
            return jsonify({"status": "fail", "message": "NODE_RESULT ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}), 404

        df_result = pd.DataFrame(result_rows, columns=["DISTRICT", "STAT_HOUR", "NODE_ID", "DELAY", "VEHS"])
        df_result["DELAY"] = pd.to_numeric(df_result["DELAY"], errors='coerce')
        df_result["VEHS"] = pd.to_numeric(df_result["VEHS"], errors='coerce').fillna(0).astype(int)
        df_result["DISTRICT"] = df_result["DISTRICT"].apply(lambda x: int(x) if pd.notna(x) else None)

        # ğŸ“Œ 3. NODE_INFO: ìœ„ì¹˜ ì •ë³´ë§Œ ì¡°íšŒ
        cursor.execute("""
            SELECT NODE_ID, LAT, LON
            FROM NODE_INFO
        """)
        info_rows = [tuple(row) for row in cursor.fetchall()]

        if not info_rows:
            return jsonify({"status": "fail", "message": "NODE_INFO ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}), 404

        df_info = pd.DataFrame(info_rows, columns=["NODE_ID", "LAT", "LON"])

        # ğŸ“Œ 4. ë³‘í•©
        df_merged = pd.merge(df_result, df_info, on="NODE_ID", how="left")
        df_filtered = df_merged.dropna(subset=["LAT", "LON", "DISTRICT"])

        # ğŸ“Œ 5. DISTRICT ëª…ì¹­ ì ìš©
        df_filtered["DISTRICT_NAME"] = df_filtered["DISTRICT"].map(district_mapping)
        
        # ğŸ“Œ 6. stat_hourë¥¼ readable hourly labelë¡œ ë³€í™˜
        df_filtered["STAT_HOUR_LABEL"] = df_filtered["STAT_HOUR"].apply(
            lambda x: hourly_mapping.get(x[-2:], x)
        )
        df_filtered["TARGET_DATE"] = df_filtered["STAT_HOUR"].str[:8]

        # ğŸ“Œ 7. ê·¸ë£¹í™”: STAT_HOUR + DISTRICT_NAME + NODE_ID
        grouped = df_filtered.groupby(["TARGET_DATE", "STAT_HOUR_LABEL", "DISTRICT_NAME", "NODE_ID"]).agg({
            "DELAY": "mean",
            "LAT": "first",
            "LON": "first"
        }).reset_index()

        grouped["DELAY"] = grouped["DELAY"].round(2)
        grouped["LOS"] = grouped["DELAY"].apply(get_los)

        # ğŸ“Œ ğŸš€ ì¶”ê°€: VEH ì§‘ê³„
        vehs_df = df_filtered.groupby(["TARGET_DATE", "STAT_HOUR_LABEL", "DISTRICT_NAME"])["VEHS"].sum().reset_index()
        vehs_df["VEHS"] = vehs_df["VEHS"].astype(int)

        # ğŸ“Œ 7. GeoJSON + VEHS ì¡°í•©
        result_json = dict()

        for (hour_label, district), group_df in grouped.groupby(["STAT_HOUR_LABEL", "DISTRICT_NAME"]):
            features = []
            for idx, row in group_df.iterrows():
                feature = {
                    "type": "Feature",
                    "id": idx,
                    "geometry": {
                        "type": "Point",
                        "coordinates": [float(row["LAT"]), float(row["LON"])]
                    },
                    "properties": {
                        "los": row["LOS"]
                    }
                }
                features.append(feature)

            geojson = {
                "type": "FeatureCollection",
                "features": features
            }

            veh_row = vehs_df[
                (vehs_df["STAT_HOUR_LABEL"] == hour_label) &
                (vehs_df["DISTRICT_NAME"] == district)
            ]
            vehs_value = int(veh_row["VEHS"].values[0]) if not veh_row.empty else 0
            
            result_json.setdefault(hour_label, {})[district] = {
                "VEHS": vehs_value,
                "GEOJSON": geojson
            }

        # ğŸ“Œ 8. ì‘ë‹µ
        json_data = json.dumps({"status": "success", "target_date": latest_date, "data": result_json}, ensure_ascii=False, default=convert_decimal)
        return Response(json_data, content_type='application/json; charset=utf-8')

    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

# ========================================================= [ ëª¨ë‹ˆí„°ë§ 4 - ë„ë¡œêµ¬ê°„ë³„ í†µí–‰ëŸ‰ ì •ë³´ ]

@app.route('/monitoring/road-traffic-info', methods=['GET'])
def road_traffic_info():
    pass

# ========================================================= [ ëª¨ë‹ˆí„°ë§ 5 - êµì°¨ë¡œë³„ í†µí–‰ì •ë³´ ]

@app.route('/monitoring/node-result', methods=['GET'])
def node_result_summary():
    try:
        conn = get_connection()
        cursor = conn.cursor()

        # ğŸ“Œ 1. ê°€ì¥ ìµœì‹  ë‚ ì§œ(YYYYMMDD) ì¶”ì¶œ
        cursor.execute("""
            SELECT STAT_DATE FROM (
                SELECT SUBSTR(STAT_HOUR, 1, 8) AS STAT_DATE
                FROM NODE_RESULT
                GROUP BY SUBSTR(STAT_HOUR, 1, 8)
                ORDER BY STAT_DATE DESC
            )
            WHERE ROWNUM = 1
        """)
        latest_date_row = cursor.fetchone()
        if not latest_date_row:
            return jsonify({"status": "fail", "message": "STAT_HOUR ë‚ ì§œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}), 404

        latest_date = latest_date_row[0]

        # ğŸ“Œ 2. í•´ë‹¹ ë‚ ì§œ ë°ì´í„° ì¡°íšŒ
        cursor.execute("""
            SELECT STAT_HOUR, TIMEINT, NODE_ID, QLEN, VEHS, DELAY, STOPS
            FROM NODE_RESULT
            WHERE SUBSTR(STAT_HOUR, 1, 8) = ?
        """, [latest_date])
        rows = cursor.fetchall()
        rows = [tuple(row) for row in rows]

        if not rows:
            return jsonify({"status": "fail", "message": "í•´ë‹¹ ë‚ ì§œì— ëŒ€í•œ êµì°¨ë¡œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}), 404

        # ğŸ“Œ 3. DataFrame ìƒì„±
        df = pd.DataFrame(rows, columns=["STAT_HOUR", "TIMEINT", "NODE_ID", "QLEN", "VEHS", "DELAY", "STOPS"])
        df[["QLEN", "VEHS", "DELAY", "STOPS"]] = df[["QLEN", "VEHS", "DELAY", "STOPS"]].apply(pd.to_numeric, errors='coerce')

        # ğŸ“Œ 4. ë‚ ì§œì™€ ì‹œê°„ ë¶„ë¦¬
        df["DATE"] = df["STAT_HOUR"].str[:8]
        df["HOUR"] = df["STAT_HOUR"].str[8:10]

        # ğŸ“Œ 5. í‰ê· ê°’ ê³„ì‚°
        df_avg = df.groupby(["DATE", "HOUR", "NODE_ID"], as_index=False).agg({
            "QLEN": "mean",
            "VEHS": "mean",
            "DELAY": "mean",
            "STOPS": "mean"
        }).round({"QLEN": 2, "DELAY": 2, "STOPS": 2})

        # ğŸ“Œ 6. êµì°¨ë¡œ ì´ë¦„ ë³‘í•©
        cursor.execute("SELECT NODE_ID, CROSS_NAME FROM NODE_INFO")
        node_info = cursor.fetchall()
        node_info = [tuple(row) for row in node_info]
        df_node_info = pd.DataFrame(node_info, columns=["NODE_ID", "NODE_NAME"])
        df_node_info = df_node_info.drop_duplicates(subset="NODE_ID")

        df_merged = df_avg.merge(df_node_info, on="NODE_ID", how="left")

        # ğŸ“Œ 7. LOS ë“±ê¸‰ ê³„ì‚°
        def get_los(delay):
            if delay < 15: return "A"
            elif delay < 30: return "B"
            elif delay < 50: return "C"
            elif delay < 70: return "D"
            elif delay < 100: return "E"
            elif delay < 220: return "F"
            elif delay < 340: return "FF"
            else: return "FFF"

        df_merged["LOS"] = df_merged["DELAY"].apply(get_los)

        # ğŸ“Œ 8. ìµœì¢… ì»¬ëŸ¼ ì •ë¦¬
        df_merged = df_merged[[
            "DATE", "HOUR", "NODE_NAME", "QLEN", "VEHS", "DELAY", "STOPS", "LOS"
        ]]

        # ğŸ“Œ 9. ìœ íš¨í•œ êµì°¨ë¡œë§Œ í•„í„°ë§ + VEHS ì •ìˆ˜ ë³€í™˜
        df_merged = df_merged[df_merged["NODE_NAME"].notna()]
        df_merged["VEHS"] = df_merged["VEHS"].round(0).astype("Int64")

        # ğŸ“Œ 10. ì¤‘ë³µ ì œê±°
        df_merged = df_merged.drop_duplicates()

        # ğŸ“Œ 11. JSON ë³€í™˜ â†’ { "target_date": "YYYYMMDD", "data": { "í•œê¸€ì‹œê°„ë¼ë²¨": [ ... ] } }
        result_dict = {}
        target_date = None
        mapped_data = {}

        for (date, hour), group in df_merged.groupby(["DATE", "HOUR"]):
            records = (
                group.drop(columns=["DATE", "HOUR"])
                    .replace({np.nan: None})
                    .to_dict(orient="records")
            )

            # ì²« ë²ˆì§¸ date ê°’ì„ target_dateë¡œ ì„¤ì •
            if not target_date:
                target_date = date

            # hourly labelë¡œ ë³€í™˜
            hour_label = hourly_mapping.get(hour, hour)  # ë§¤í•‘ ì•ˆë˜ë©´ ê·¸ëŒ€ë¡œ hour ì‚¬ìš©
            mapped_data[hour_label] = records

        # âœ… ì‘ë‹µ ë°˜í™˜
        return app.response_class(
            response=json.dumps({
                "target_date": target_date,
                "data": mapped_data
            }, ensure_ascii=False, allow_nan=False),
            status=200,
            mimetype='application/json'
        )

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return jsonify({
            "status": "fail",
            "message": "êµì°¨ë¡œ ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
            "error": str(e),
            "timestamp": datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }), 500






# ========================================================= [ ì‹ í˜¸ìš´ì˜ 1 - ë„ë¡œì¶•ë³„ í†µê³„ì •ë³´ ]

@app.route('/signal/vttm-result', methods=['GET'])
def vttm_result_summary():
    try:
        conn = get_connection()
        cursor = conn.cursor()

        # ğŸ“Œ 1. ê°€ì¥ ìµœì‹  ë‚ ì§œ(YYYYMMDD) ì¶”ì¶œ
        cursor.execute("""
            SELECT STAT_DATE FROM (
                SELECT SUBSTR(STAT_HOUR, 1, 8) AS STAT_DATE
                FROM NODE_RESULT
                GROUP BY SUBSTR(STAT_HOUR, 1, 8)
                ORDER BY STAT_DATE DESC
            )
            WHERE ROWNUM = 1
        """)
        latest_date_row = cursor.fetchone()
        if not latest_date_row:
            return jsonify({"status": "fail", "message": "STAT_HOUR ë‚ ì§œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}), 404

        latest_date = latest_date_row[0]

        # ğŸ“Œ 2. í•´ë‹¹ ë‚ ì§œì˜ êµí†µ ê²°ê³¼ ë°ì´í„° ì¡°íšŒ
        cursor.execute("""
            SELECT DISTRICT, STAT_HOUR, FROM_NODE_NAME, TO_NODE_NAME, UPDOWN, DISTANCE, TRAVEL_TIME
            FROM VTTM_RESULT
            WHERE SUBSTR(STAT_HOUR, 1, 8) = ?
        """, [latest_date])
        rows = cursor.fetchall()

        columns = ['DISTRICT', 'STAT_HOUR', 'FROM_NODE_NAME', 'TO_NODE_NAME', 'UPDOWN', 'DISTANCE', 'TRAVEL_TIME']

        # ğŸ“Œ ë°ì´í„° ì „ì²˜ë¦¬ êµ¬ì¡° ì´ˆê¸°í™”
        grouped_data = defaultdict(lambda: defaultdict(list))
        pair_buffer = defaultdict(lambda: defaultdict(dict))  # (district, hour_label) => segment_key => {updown}

        for row in rows:
            record = dict(zip(columns, row))
            district_id = record['DISTRICT']
            stat_hour = record['STAT_HOUR']
            hour_code = stat_hour[-2:]
            hour_label = hourly_mapping.get(hour_code, hour_code)
            district_name = district_mapping.get(district_id, f"ê¸°íƒ€ì§€ì—­-{district_id}")

            from_node = record['FROM_NODE_NAME']
            to_node = record['TO_NODE_NAME']
            updown = str(record['UPDOWN'])
            distance = float(record['DISTANCE'] or 0)
            travel_time_val = float(record['TRAVEL_TIME'] or 0)

            travel_time = round(travel_time_val, 1) if travel_time_val > 0 else 0.0
            travel_speed = round((distance / travel_time_val) * 3.6, 1) if travel_time_val > 0 else 0.0
            travel_cost = 99.9

            segment_key = tuple(sorted([from_node, to_node]))

            pair_buffer[(district_name, hour_label)][segment_key][updown] = {
                "from_node": from_node,
                "to_node": to_node,
                "travel_time": travel_time,
                "travel_speed": travel_speed,
                "travel_cost": travel_cost
            }

        # ğŸ“Œ ì™„ì„±ëœ ìŒë§Œ ì •ë¦¬
        for (district_name, hour_label), segment_dict in pair_buffer.items():
            for segment_key, directions in segment_dict.items():
                if '0' in directions and '1' in directions:
                    from_node_data = directions['0']
                    to_node_data = directions['1']

                    record = {
                        from_node_data['from_node']: {
                            "travel_cost": from_node_data['travel_cost'],
                            "travel_speed": from_node_data['travel_speed'],
                            "travel_time": from_node_data['travel_time']
                        },
                        to_node_data['from_node']: {
                            "travel_cost": to_node_data['travel_cost'],
                            "travel_speed": to_node_data['travel_speed'],
                            "travel_time": to_node_data['travel_time']
                        }
                    }
                    grouped_data[district_name][hour_label].append(record)

        return jsonify({
            "status": "success",
            "target_date": latest_date,
            "data": grouped_data
        }), 200

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return jsonify({
            "status": "fail",
            "message": "êµì°¨ë¡œ ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
            "error": str(e),
            "timestamp": datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }), 500

# ========================================================= [ ì‹ í˜¸ìš´ì˜ 2 - ì§€ì ë³„ í†µí–‰ì •ë³´ ]

@app.route('/signal/vttm-traffic-info', methods=['GET'])
def vttm_traffic_info():
    pass

# ========================================================= [ ì‹ í˜¸ìš´ì˜ 3 - ì‹œê°„ëŒ€ë³„ êµí†µí˜¼ì¡ ì •ë³´ ]

@app.route('/signal/hourly-congested-info', methods=['GET'])
def hourly_congested_info_data():
    pass

def hourly_congested_info_map_data():
    pass

# ========================================================= [ ì‹ í˜¸ìš´ì˜ 4 - êµì°¨ë¡œë³„ íš¨ê³¼ì§€í‘œ ë¶„ì„ì •ë³´ ]

@app.route('/signal/node-approach-result', methods=['GET'])
def node_approach_result():
    hour_filter = request.args.get('hour')  # '08', '11', '14', '17' ì¤‘ í•˜ë‚˜
    label = hourly_mapping.get(hour_filter)
    if not label:
        return jsonify({
            "status": "fail",
            "message": f"ìœ íš¨í•˜ì§€ ì•Šì€ hour íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤: {hour_filter}",
            "timestamp": datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }), 400
            
    try:
        conn = get_connection()
        cursor = conn.cursor()

        # ğŸ“Œ 1. ìµœì‹  ì¼ì(STAT_HOUR â†’ YYYYMMDD) ì¡°íšŒ
        cursor.execute("""
            SELECT STAT_DATE FROM (
                SELECT SUBSTR(STAT_HOUR, 1, 8) AS STAT_DATE
                FROM NODE_DIR_RESULT
                GROUP BY SUBSTR(STAT_HOUR, 1, 8)
                ORDER BY STAT_DATE DESC
            )
            WHERE ROWNUM = 1
        """)
        latest_date_row = cursor.fetchone()
        if not latest_date_row:
            return jsonify({
                "status": "fail",
                "message": "NODE_DIR_RESULTì— STAT_HOUR ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
                "timestamp": datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }), 404

        latest_date = latest_date_row[0]

        # ğŸ“Œ 2. NODE_DIR_RESULT ì¡°íšŒ
        cursor.execute("""
            SELECT DISTRICT, STAT_HOUR, TIMEINT, NODE_ID, CROSS_ID, SA_NO,
                APPR_ID, DIRECTION, QLEN, VEHS, DELAY, STOPS
            FROM NODE_DIR_RESULT
            WHERE SUBSTR(STAT_HOUR, 1, 8) = ?
        """, [latest_date])
        node_dir_rows = [tuple(row) for row in cursor.fetchall()]
        node_dir_columns = ['DISTRICT', 'STAT_HOUR', 'TIMEINT', 'NODE_ID', 'CROSS_ID', 'SA_NO',
                            'APPR_ID', 'DIRECTION', 'QLEN', 'VEHS', 'DELAY', 'STOPS']
        df_node_dir = pd.DataFrame(node_dir_rows, columns=node_dir_columns)
        for col in df_node_dir.columns:
            df_node_dir[col] = df_node_dir[col].map(lambda x: float(x) if isinstance(x, Decimal) else x)

        # ğŸ“Œ 3. NODE_DIR_INFO ì¡°íšŒ
        cursor.execute("""
            SELECT CROSS_ID, DISTRICT, NODE_ID, NODE_NAME, CROSS_TYPE, INT_TYPE,
                APPR_ID, DIRECTION, APPR_NAME
            FROM NODE_DIR_INFO
        """)
        node_dir_info_rows = [tuple(row) for row in cursor.fetchall()]
        node_dir_info_columns = ['CROSS_ID', 'DISTRICT', 'NODE_ID', 'NODE_NAME',
                                'CROSS_TYPE', 'INT_TYPE', 'APPR_ID', 'DIRECTION', 'APPR_NAME']
        df_node_info = pd.DataFrame(node_dir_info_rows, columns=node_dir_info_columns)
        for col in df_node_info.columns:
            df_node_info[col] = df_node_info[col].map(lambda x: float(x) if isinstance(x, Decimal) else x)

        # ğŸ“Œ ë©”íƒ€ ì •ë³´ ì¶”ì¶œ
        df_node_meta = df_node_info.drop_duplicates(subset=['NODE_ID'])[
            ['NODE_ID', 'NODE_NAME', 'CROSS_TYPE', 'INT_TYPE']
        ].set_index('NODE_ID')

        df_appr_meta = df_node_info[[
            'NODE_ID', 'APPR_ID', 'DIRECTION', 'APPR_NAME'
        ]].dropna()

        # ğŸ“Œ 4. ê²°ê³¼ ê°€ê³µ
        grouped_result = {}

        for stat_hour, df_hour in df_node_dir.groupby('STAT_HOUR'):
            hh = stat_hour[-2:]
            if hh != hour_filter:
                continue

            grouped_result[label] = {}

            for node_id, df_node_alltime in df_hour.groupby('NODE_ID'):
                if node_id not in df_node_meta.index:
                    continue

                node_meta = df_node_meta.loc[node_id]
                node_name = node_meta['NODE_NAME']
                cross_id = df_node_alltime['CROSS_ID'].iloc[0]
                sa_no = df_node_alltime['SA_NO'].iloc[0]

                result_dict = {
                    "CROSS_ID": int(float(cross_id)),
                    "SA_NO": sa_no,
                    "CROSS_TYPE": int(float(node_meta['CROSS_TYPE'])),
                    "INT_TYPE": node_meta['INT_TYPE']
                }

                hourly_summary = {}
                all_vehs_total, all_delay_sum, all_delay_count = 0, 0.0, 0

                for appr_id, df_appr in df_node_alltime.groupby('APPR_ID'):
                    vehs = int(df_appr['VEHS'].sum(skipna=True) or 0)
                    delay_vals = df_appr['DELAY'].dropna().astype(float).tolist()
                    delay_avg = round(sum(delay_vals) / len(delay_vals), 1) if delay_vals else 0.0
                    los = get_los(delay_avg)

                    all_vehs_total += vehs
                    all_delay_sum += sum(delay_vals)
                    all_delay_count += len(delay_vals)

                    match = df_appr_meta[(df_appr_meta['NODE_ID'] == node_id) & (df_appr_meta['APPR_ID'] == appr_id)]
                    appr_name = match.iloc[0]['APPR_NAME'] if not match.empty else "ë¯¸ì§€ì •"

                    hourly_summary[appr_name] = {
                        "VEHS": int(vehs),
                        "DELAY": delay_avg,
                        "LOS": los
                    }

                result_dict["TOTAL_VEHS"] = all_vehs_total
                result_dict["TOTAL_DELAY"] = round(all_delay_sum / all_delay_count, 1) if all_delay_count > 0 else 0.0
                result_dict["TOTAL_LOS"] = get_los(result_dict["TOTAL_DELAY"])
                result_dict["hourly"] = hourly_summary

                for timeint, df_time in df_node_alltime.groupby('TIMEINT'):
                    timeint_str = str(timeint).zfill(2)
                    timeint_result = {}

                    for appr_id, df_appr_name in df_time.groupby('APPR_ID'):
                        match = df_appr_meta[
                            (df_appr_meta['NODE_ID'] == node_id) & 
                            (df_appr_meta['APPR_ID'] == appr_id)
                        ]
                        appr_name = match.iloc[0]['APPR_NAME'] if not match.empty else "ë¯¸ì§€ì •"

                        if appr_name not in timeint_result:
                            timeint_result[appr_name] = {}

                        for direction, df_dir in df_appr_name.groupby('DIRECTION'):
                            vehs = int(df_dir['VEHS'].sum(skipna=True) or 0)
                            delay_vals = df_dir['DELAY'].dropna().astype(float).tolist()
                            delay_avg = round(sum(delay_vals) / len(delay_vals), 1) if delay_vals else 0.0
                            los = get_los(delay_avg)

                            timeint_result[appr_name][str(int(direction))] = {
                                "VEHS": vehs,
                                "DELAY": delay_avg,
                                "LOS": los
                            }

                    result_dict[timeint_str] = timeint_result

                grouped_result[label][node_name] = result_dict
            
            if label not in grouped_result or not grouped_result[label]:
                return jsonify({
                    "status": "fail",
                    "message": f"{label}ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "timestamp": datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }), 404

        return app.response_class(
            response=json.dumps({
                "status": "success",
                "label": label,
                "latest_date": latest_date,
                "data": grouped_result[label],
            }, ensure_ascii=False),
            status=200,
            mimetype='application/json'
        )

    except Exception as e:
        return jsonify({
            "status": "fail",
            "message": "ë…¸ë“œ ì ‘ê·¼ ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
            "error": str(e)
        }), 500








# ========================================================= [ êµí†µê´€ë¦¬ 1 - êµí†µëŸ‰ íŒ¨í„´ë¹„êµ ë¶„ì„ì •ë³´ ]

@app.route('/management/compare-traffic-vol', methods=['GET'])
def compare_traffic_vol():
    pass

# ========================================================= [ êµí†µê´€ë¦¬ 2 - Deep Learning Progress Overview ]

@app.route('/management/deep-learning-overview', methods=['GET'])
def deep_learning_overview():
    pass

# ========================================================= [ êµí†µê´€ë¦¬ 3 - SA(Sub Area) ê·¸ë£¹ ê´€ë¦¬ì •ë³´ ]

@app.route('/management/sa-group-info', methods=['GET'])
def congested_info():
    pass

def sa_info():
    pass

# ========================================================= [ êµí†µê´€ë¦¬ 4 - í˜¼ì¡êµì°¨ë¡œ ì‹ í˜¸ìµœì í™” íš¨ê³¼ê²€ì¦ ]

@app.route('/management/cross-optimize', methods=['GET'])
def cross_optimize():
    pass










# ========================================================= [ ì„œë²„ì‹¤í–‰ ]

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5101, debug=False)