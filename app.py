import pyodbc, os, subprocess, pathlib, json, hashlib
import pandas as pd
import numpy as np
import pytz

from pprint import pprint
from datetime import datetime, timedelta, timezone
from decimal import Decimal
from collections import defaultdict
from flask import Flask, request, jsonify, render_template, redirect, url_for, Response, make_response
from flask_cors import CORS
from dotenv import load_dotenv
from apscheduler.schedulers.background import BackgroundScheduler
from windows import set_dpi_awareness









set_dpi_awareness()
app = Flask(__name__, static_url_path='/static', static_folder='static')
CORS(app)









# ========================================================= [ í˜„ì¬ì‹œê°„ ]

KST = pytz.timezone("Asia/Seoul")

def resolve_dataset_date(now_kst: datetime) -> str:
    """
    KST 06:00 ì´ì „  -> ì „ì „ë‚ ( D-2 )
    KST 06:00 ì´í›„  -> ì „ë‚   ( D-1 )
    """
    base = now_kst.date()
    if now_kst.hour < 6:
        target = base - timedelta(days=2)
    else:
        target = base - timedelta(days=1)
    return target.strftime("%Y%m%d")

def get_current_time():
    return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

# ========================================================= [ Etag ìƒì„± ]

def parse_hours_param(raw: str):
    """'01,02,03,04' -> {1,2,3,4}; ë¹„ê±°ë‚˜ ìœ íš¨ê°’ ì—†ìœ¼ë©´ None"""
    if not raw:
        return None
    hset = set()
    for tok in raw.split(','):
        tok = tok.strip()
        if not tok:
            continue
        try:
            h = int(tok)
            if 0 <= h <= 23:
                hset.add(h)
        except:
            pass
    return hset or None

def make_etag(dataset_date: str, hours_filter_set, total_rows: int) -> str:
    """ê°„ë‹¨ ETag: ë‚ ì§œ + ì‹œê°„ëŒ€ + ì´í–‰ìˆ˜ â†’ md5"""
    hours_key = ",".join(sorted(f"{h:02d}" for h in (hours_filter_set or [])))
    base = f"{dataset_date}|{hours_key}|{total_rows}"
    return hashlib.md5(base.encode("utf-8")).hexdigest()









# ========================================================= [ VISUM ìë™í™” ì½”ë“œ ì‹¤í–‰ ]

def run_visum_script():
    script_path = os.path.join(os.path.dirname(__file__), 'auto simulation', 'auto_visum.py')
    print(f"âœ… [ {get_current_time()} ] Vissim ìë™í™” ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰")
    subprocess.Popen(['python', script_path])
    
# ========================================================= [ VISSIM ìë™í™” ì½”ë“œ ì‹¤í–‰ ]

def run_vissim_script():
    base_dir = os.path.abspath(os.path.dirname(__file__))
    script_path = os.path.join(base_dir, 'auto simulation', 'auto_vissim.py')
    
    if not os.path.exists(script_path):
        print(f"âŒ ê²½ë¡œ ì˜¤ë¥˜: {script_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… [ {get_current_time()} ] Vissim ìë™í™” ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰")
    subprocess.Popen(['python', script_path], cwd=os.path.dirname(script_path))









# ========================================================= [ ìë™í™” ì‹œë®¬ë ˆì´ì…˜ ìŠ¤ì¼€ì¥´ëŸ¬ ì„¤ì • ]

# nohup python app.py > server.log 2>&1 &
scheduler = BackgroundScheduler()
scheduler.add_job(run_visum_script, 'cron', hour=2, minute=0)
scheduler.add_job(run_vissim_script, 'cron', hour=1, minute=0)
scheduler.start()

# ========================================================= [ í‹°ë² ë¡œ ì—°ê²° ]

load_dotenv()
FLASK_ENV = os.getenv("FLASK_ENV", "production")

# ê°•ë¦‰ ì„¼í„°ìš©
DSNNAME = os.getenv("DSNNAME")
DBUSER = os.getenv("DBUSER")
DBPWD = os.getenv("DBPWD")

# ì—”ì œë¡œ í…ŒìŠ¤íŠ¸ìš©
ENZERO_SERVER = os.getenv("ENZERO_SERVER")
ENZERO_PORT = os.getenv("ENZERO_PORT")
ENZERO_DB = os.getenv("ENZERO_DB")
ENZERO_UID = os.getenv("ENZERO_UID")
ENZERO_PWD = os.getenv("ENZERO_PWD")

def get_connection():
    if FLASK_ENV == "test":
        print(f">>> [INFO] Flask í™˜ê²½ ì„¤ì •: {FLASK_ENV}")
        return pyodbc.connect(
            f"DRIVER=Tibero 5 ODBC Driver;"
            f"SERVER={ENZERO_SERVER};"
            f"PORT={ENZERO_PORT};"
            f"DB={ENZERO_DB};"
            f"UID={ENZERO_UID};"
            f"PWD={ENZERO_PWD};"
        )
    else:
        print(f">>> [INFO] Flask í™˜ê²½ ì„¤ì •: {FLASK_ENV}")
        return pyodbc.connect(
            f"DSN={DSNNAME};"
            f"UID={DBUSER};"
            f"PWD={DBPWD}"
        )









# ========================================================= [ Decimal ì²˜ë¦¬ í•¨ìˆ˜ ]

def convert_decimal(obj):
    if isinstance(obj, Decimal):
        return float(obj)
    return obj

# ========================================================= [ ê¶Œì—­ë³„ ë§¤í•‘ ]

district_mapping = {
    1: "êµë™",
    2: "ì†¡ì •",
    3: "ë„ì‹¬",
    4: "ê²½í¬"
}
hourly_mapping = {
    "08": "ì˜¤ì „ì²¨ë‘ 08ì‹œ ~ 09ì‹œ",
    "11": "ì˜¤ì „ë¹„ì²¨ë‘ 11ì‹œ ~ 12ì‹œ",
    "14": "ì˜¤í›„ë¹„ì²¨ë‘ 14ì‹œ ~ 15ì‹œ",
    "17": "ì˜¤í›„ì²¨ë‘ 17ì‹œ ~ 18ì‹œ"
}

# ========================================================= [ ì§€ì²´ì‹œê°„ ê¸°ì¤€ LOS ]

def get_los(delay):
    if delay < 15: return "A"
    elif delay < 30: return "B"
    elif delay < 50: return "C"
    elif delay < 70: return "D"
    elif delay < 100: return "E"
    elif delay < 220: return "F"
    elif delay < 340: return "FF"
    else: return "FFF"









# ========================================================= [ ë¡œê·¸ì¸ ]

@app.route('/')
def index():
    return redirect(url_for('login'))

@app.route('/login')
def login():
    return render_template('login.html')

# ========================================================= [ íšŒì›ê°€ì… ]

@app.route('/sign-up')
def sign_up():
    return render_template('sign_up.html')

# ========================================================= [ ë©”ì¸í˜ì´ì§€ ]

@app.route('/home')
def home():
    return render_template('home.html')

# ========================================================= [ ë©”ì¸í˜ì´ì§€ ]

@app.route('/video_test', methods=['GET'])
def video_test():
    return render_template('video_test.html')

# ========================================================= [ ë”¥ëŸ¬ë‹ í•™ìŠµ ]

# @app.route('/gndl-learn-start', methods=['GET'])
# def deeplearning_learn_start():
#     try:
#         logs = []
        
#         VENV_PYTHON = os.path.join(os.getcwd(), "venv", "Scripts", "python.exe")
        
#         base_dir = os.path.join(os.getcwd(), "gndl")
#         gnn_data_dir = os.path.join(base_dir, "gnn_data")

#         pkl_files = ["node_features.pkl", "edge_list.pkl", "node_index.pkl"]
#         pkl_paths = [os.path.join(gnn_data_dir, f) for f in pkl_files]

#         # STEP 1: ì „ì²˜ë¦¬ í•„ìš” ì—¬ë¶€ í™•ì¸
#         if not all(os.path.exists(p) for p in pkl_paths):
#             logs.append("ğŸŸ¡ ì „ì²˜ë¦¬ ë°ì´í„° ì—†ìŒ â†’ 1.preprocess.py ì‹¤í–‰")
#             result = subprocess.run([VENV_PYTHON, "gndl/1.preprocess.py"], capture_output=True, text=True)
#             logs.append(result.stdout or result.stderr)
#         else:
#             logs.append("âœ… ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì¡´ì¬í•˜ì—¬ ê±´ë„ˆëœ€")

#         # STEP 2: í•™ìŠµ ëª¨ë¸ íŒŒì¼ ì²´í¬
#         model_path = os.path.join(gnn_data_dir, "best_model.pt")
#         if not os.path.exists(model_path):
#             logs.append("ğŸŸ¡ ëª¨ë¸ ì—†ìŒ â†’ 2.gnn.py ì‹¤í–‰")
#             result = subprocess.run([VENV_PYTHON, "gndl/2.gnn.py"], capture_output=True, text=True)
#             logs.append(result.stdout or result.stderr)
#         else:
#             logs.append("âœ… í•™ìŠµëœ ëª¨ë¸ì´ ì¡´ì¬í•˜ì—¬ ê±´ë„ˆëœ€")

#         # STEP 3: ì‹œê°í™” ê²°ê³¼ í™•ì¸
#         visual_path = os.path.join(gnn_data_dir, "visual_result.png")
#         if not os.path.exists(visual_path):
#             logs.append("ğŸŸ¡ ì‹œê°í™” ì—†ìŒ â†’ 3.gnn_visual.py ì‹¤í–‰")
#             result = subprocess.run([VENV_PYTHON, "gndl/3.gnn_visual.py"], capture_output=True, text=True)
#             logs.append(result.stdout or result.stderr)
#         else:
#             logs.append("âœ… ì‹œê°í™” ê²°ê³¼ê°€ ì¡´ì¬í•˜ì—¬ ê±´ë„ˆëœ€")

#         # STEP 4: ë¯¸ë˜ ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸
#         pred_path = os.path.join(gnn_data_dir, "future_prediction.json")
#         if not os.path.exists(pred_path):
#             logs.append("ğŸŸ¡ ë¯¸ë˜ ì˜ˆì¸¡ ì—†ìŒ â†’ 4.future_prediction.py ì‹¤í–‰")
#             result = subprocess.run([VENV_PYTHON, "gndl/4.future_prediction.py"], capture_output=True, text=True)
#             logs.append(result.stdout or result.stderr)
#         else:
#             logs.append("âœ… ë¯¸ë˜ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì¡´ì¬í•˜ì—¬ ê±´ë„ˆëœ€")

#         return jsonify({
#             "status": "success",
#             "message": "GNN ë”¥ëŸ¬ë‹ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ",
#             "logs": logs
#         })

#     except Exception as e:
#         return jsonify({
#             "status": "fail",
#             "message": "âŒ GNN í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
#             "error": str(e),
#             "logs": logs
#         })









# ========================================================= [ ëª¨ë‹ˆí„°ë§ 1 - ì‹œê°„ëŒ€ë³„ êµí†µìˆ˜ìš” ë¶„ì„ì •ë³´ ]

@app.route('/monitoring/visum-hourly-vc', methods=['GET'])
def visum_hourly_vc():
    now_kst = datetime.now(KST)  # ì°¸ê³ ìš©
    # rule_date = resolve_dataset_date(now_kst)  # â–¶ ë°°í¬ ì‹œ ë³µì›
    rule_date = "20250701"  # â–¶ í…ŒìŠ¤íŠ¸ìš©

    # ê³µí†µ í—¤ë” ê³„ì‚°(ë‹¤ìŒ ì—…ë°ì´íŠ¸ ì‹œê°: ë§¤ì¼ 06:00 KST)
    next_update = now_kst.replace(hour=6, minute=0, second=0, microsecond=0)
    if now_kst >= next_update:
        next_update += timedelta(days=1)
    x_next_update = next_update.isoformat()

    # 0) ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°(hour) ê²€ì¦ â€” 400 ì‘ë‹µ ë¶„ë¦¬
    ALLOWED_HOURS = {"08", "11", "14", "17", "24"}
    hour_param = request.args.get('hour', '').strip()
    if not hour_param:
        return jsonify({"error": "Missing 'hour' query parameter.",
                        "allowed": sorted(list(ALLOWED_HOURS))}), 400
    if hour_param not in ALLOWED_HOURS:
        return jsonify({"error": f"Invalid hour '{hour_param}'.",
                        "allowed": sorted(list(ALLOWED_HOURS))}), 400

    # âœ… make_etagê°€ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ, intë¡œ ë³€í™˜í•´ 1-íŠœí”Œë¡œ ì „ë‹¬
    try:
        hours_key_for_etag = (int(hour_param),)
    except ValueError:
        hours_key_for_etag = (24,) if hour_param == "24" else (0,)

    conn = get_connection()
    cursor = conn.cursor()

    try:
        # 1) ë¶„ê¸°: ì‹œê°„ëŒ€ vs ì¼(day)
        if hour_param == "24":
            target_key = rule_date  # yyyymmdd
            sql_main = """
                SELECT LINK_ID, VC
                FROM TOMMS.DAY_LINK_RESULT
                WHERE STAT_DAY = ?
                ORDER BY LINK_ID
            """
            main_param = (rule_date,)

            # â”€â”€ hour_label ê³„ì‚°: mmì›” ddì¼ 00ì‹œ ~ 24ì‹œ
            date_source = rule_date + "00"  # yyyymmddhh í˜•íƒœë¡œ mm, dd ì¶”ì¶œìš©
            mm = int(date_source[4:6])
            dd = int(date_source[6:8])
            hour_label = f"{mm}ì›” {dd}ì¼ ì „ì¼ í‰ê· "

        else:
            stat_hour = rule_date + hour_param  # ì˜ˆ: 2025070108
            target_key = stat_hour              # yyyymmddhh
            sql_main = """
                SELECT LINK_ID, VC
                FROM TOMMS.HOUR_LINK_RESULT
                WHERE STAT_HOUR = ?
                ORDER BY LINK_ID
            """
            main_param = (stat_hour,)

            # â”€â”€ hour_label ê³„ì‚°: mmì›” ddì¼ hhì‹œ ~ (hh+1)ì‹œ
            date_source = stat_hour             # yyyymmddhh
            mm = int(date_source[4:6])
            dd = int(date_source[6:8])
            start_h = int(hour_param)
            end_h = (start_h + 1) % 24
            hour_label = f"{mm}ì›” {dd}ì¼ {start_h:02d}ì‹œ ~ {end_h:02d}ì‹œ"

        cursor.execute(sql_main, main_param)
        rows = cursor.fetchall()

        # 2) ê·¸ë£¹/ID ìˆ˜ì§‘ + per-id VC ëˆ„ì 
        groups = []
        all_ids_in_order = []
        from collections import defaultdict
        vc_list_by_id = defaultdict(list)

        for raw_link, vc_val in rows:
            if not raw_link:
                continue
            ids = [x.strip() for x in str(raw_link).split(",") if x.strip()]
            if not ids:
                continue

            try:
                vc_num = None if vc_val is None else float(vc_val)
            except Exception:
                vc_num = None

            groups.append({"raw": ",".join(ids), "ids": ids})
            all_ids_in_order.extend(ids)

            if vc_num is not None:
                for lid in ids:
                    vc_list_by_id[lid].append(vc_num)

        # ë¹ˆ ê²°ê³¼ 404 (ì •ì±…ìƒ ETag ì—†ìŒ)
        if not groups:
            return jsonify({"status": "fail", "message": f"{rule_date} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}), 404

        # --- ETag / If-None-Match (ë¯¼ê°ë„: len(groups)ë§Œ ë°˜ì˜) ---
        etag = f'"{make_etag(rule_date, hours_key_for_etag, total_rows=len(groups))}"'
        inm_raw = request.headers.get("If-None-Match", "")
        inm = inm_raw.strip().strip('"').replace("W/","").strip()

        if inm == etag.strip('"'):
            payload = {
                "status": "not_modified",
                "message": "Resource not modified. Use cached response.",
                "target_date": rule_date
            }
            body = json.dumps(payload, ensure_ascii=False)
            resp = Response(body, content_type="application/json; charset=utf-8", status=304)
            resp.headers["ETag"] = etag
            resp.headers["X-Dataset-Date"] = rule_date
            resp.headers["X-Next-Update"] = x_next_update
            resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
            resp.headers["Pragma"] = "no-cache"
            resp.headers["Expires"] = "0"
            return resp
        # ---------------------------------------------------------

        # 3) LINK_VERTEX ì¼ê´„ ì¡°íšŒ(IN)
        unique_ids = list(dict.fromkeys(all_ids_in_order).keys())
        placeholders = ",".join(["?"] * len(unique_ids))
        sql_vertex = f"""
            SELECT LINK_ID, LINK_SEQ, WGS84_X, WGS84_Y
            FROM TOMMS.LINK_VERTEX
            WHERE LINK_ID IN ({placeholders})
            ORDER BY LINK_ID, LINK_SEQ
        """
        cursor.execute(sql_vertex, tuple(unique_ids))
        vrows = cursor.fetchall()
        if not vrows:
            return jsonify({"status": "fail", "message": "NODE_INFO ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}), 404

        # 4) ê°œë³„ link_id â†’ ì¢Œí‘œ ëª©ë¡ ë§¤í•‘
        coords_by_id = defaultdict(list)
        for link_id, link_seq, x, y in vrows:
            fx = float(x) if x is not None else None
            fy = float(y) if y is not None else None
            coords_by_id[str(link_id)].append([fx, fy])

        # 4-1) per-id VC í‰ê· 
        vc_by_id_avg = {}
        for lid, arr in vc_list_by_id.items():
            if arr:
                vc_by_id_avg[lid] = sum(arr) / len(arr)

        # 5) ë¬¸ì„œ êµ¬ì„±
        documents = []
        doc_vcs_for_avg = []
        for g in groups:
            merged_coords = []
            per_id_vcs = []
            for lid in g["ids"]:
                merged_coords.extend(coords_by_id.get(lid, []))
                if lid in vc_by_id_avg:
                    per_id_vcs.append(vc_by_id_avg[lid])

            if per_id_vcs:
                group_vc = round(sum(per_id_vcs) / len(per_id_vcs), 2)
                doc_vcs_for_avg.append(group_vc)
            else:
                group_vc = None

            documents.append({
                "link_id": g["raw"],
                "coordinates": merged_coords,
                "v/c": group_vc
            })

        # 6) ì „ì²´ í‰ê·  v/c
        avg_vc = round(sum(doc_vcs_for_avg) / len(doc_vcs_for_avg), 2) if doc_vcs_for_avg else 0.00

        final_json = {
            "v/c": avg_vc,
            "hour_label": hour_label,
            "traffic_vol": 9999,
            "documents": documents
        }

        # ë³¸ë¬¸ + í—¤ë”
        body = json.dumps(final_json, ensure_ascii=False)
        resp = Response(body, content_type="application/json; charset=utf-8", status=200)
        resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
        resp.headers["Pragma"] = "no-cache"
        resp.headers["Expires"] = "0"
        resp.headers["ETag"] = etag
        resp.headers["X-Dataset-Date"] = rule_date
        resp.headers["X-Next-Update"] = x_next_update
        return resp

    except Exception as e:
        return jsonify({"error": str(e)}), 500
    finally:
        try:
            cursor.close()
        finally:
            conn.close()

# ì™„ë£Œ ========================================================= [ ëª¨ë‹ˆí„°ë§ 2 - êµí†µì¡´ê°„ í†µí–‰ì •ë³´ ]

@app.route('/monitoring/visum-zone-od', methods=['GET'])
def visum_zone_od():
    try:
        # ---- query params ----
        from_filter = request.args.get('from')   # ex) '810011'
        top_param   = request.args.get('top', default='10')
        try:
            top_n = max(1, int(top_param))
        except:
            top_n = 10

        conn = get_connection()
        cursor = conn.cursor()

        # ---- ë™ì  WHERE (ë‹¨ì¼ from í•„í„°) + qmark(?) í”Œë ˆì´ìŠ¤í™€ë” ----
        inner_where = ""
        params = []
        if from_filter:
            inner_where = "WHERE o.FROM_ZONE_ID = ?"
            params.append(from_filter)

        query = f"""
            SELECT *
            FROM (
                SELECT
                    o.FROM_ZONE_ID,
                    o.FROM_ZONE_NAME,
                    o.TO_ZONE_ID,
                    o.TO_ZONE_NAME,
                    NVL(o.AUTO_MATRIX_VALUE,0)
                    + NVL(o.BUS_MATRIX_VALUE,0)
                    + NVL(o.HGV_MATRIX_VALUE,0)              AS OD_MATRIX_VALUE,
                    f.LON AS FROM_LON, f.LAT AS FROM_LAT,
                    t.LON AS TO_LON,   t.LAT AS TO_LAT,
                    ROW_NUMBER() OVER (
                        PARTITION BY o.FROM_ZONE_ID
                        ORDER BY NVL(o.AUTO_MATRIX_VALUE,0)
                               + NVL(o.BUS_MATRIX_VALUE,0)
                               + NVL(o.HGV_MATRIX_VALUE,0) DESC
                    ) AS RN
                FROM VISUM_ZONE_OD o
                LEFT JOIN VISUM_ZONE_INFO f ON f.ZONE_ID = o.FROM_ZONE_ID
                LEFT JOIN VISUM_ZONE_INFO t ON t.ZONE_ID = o.TO_ZONE_ID
                {inner_where}
            )
            WHERE RN <= ?
            ORDER BY FROM_ZONE_ID, RN
        """
        params.append(top_n)
        cursor.execute(query, tuple(params))
        rows = cursor.fetchall()
        cols = [d[0] for d in cursor.description]
        idx  = {c: i for i, c in enumerate(cols)}

        # ---- fromë³„ ê·¸ë£¹ ----
        from collections import defaultdict
        groups = defaultdict(lambda: {
            "from_zone_id": None,
            "from_zone_name": None,
            "from_lon": None, "from_lat": None,
            "items": []  # list of dicts
        })

        rank_counter = defaultdict(int)
        for r in rows:
            fid  = r[idx["FROM_ZONE_ID"]]
            fnm  = r[idx["FROM_ZONE_NAME"]]
            flon = r[idx["FROM_LON"]]; flat = r[idx["FROM_LAT"]]
            tid  = r[idx["TO_ZONE_ID"]]
            tnm  = r[idx["TO_ZONE_NAME"]]
            tlon = r[idx["TO_LON"]];   tlat = r[idx["TO_LAT"]]
            val  = r[idx["OD_MATRIX_VALUE"]]

            g = groups[fid]
            if g["from_zone_id"] is None:
                g["from_zone_id"]   = str(fid) if fid is not None else None
                g["from_zone_name"] = fnm
                g["from_lon"]       = float(flon) if flon is not None else None
                g["from_lat"]       = float(flat) if flat is not None else None

            rank_counter[fid] += 1
            g["items"].append({
                "to_zone_id": str(tid) if tid is not None else None,
                "to_zone_name": tnm,
                "to_lon": float(tlon) if tlon is not None else None,
                "to_lat": float(tlat) if tlat is not None else None,
                "value": round(float(val), 2) if val is not None else 0.0,
                "rank": rank_counter[fid]
            })

        # ---- í•­ìƒ GeoJSONë§Œ ë°˜í™˜ (ì¢Œí‘œëŠ” [lat, lon]) ----
        payload = []
        for _, g in groups.items():
            # Points FeatureCollection: from 1ê°œ + to Nê°œ
            points = {"type": "FeatureCollection", "features": []}

            # from point
            points["features"].append({
                "type": "Feature",
                "properties": {
                    "role": "from",
                    "zone_id": g["from_zone_id"],
                    "zone_name": g["from_zone_name"]
                },
                "geometry": {
                    "type": "Point",
                    "coordinates": [g["from_lat"], g["from_lon"]]   # [lat, lon]
                }
            })

            # to points
            for it in g["items"]:
                points["features"].append({
                    "type": "Feature",
                    "properties": {
                        "role": "to",
                        "zone_id": it["to_zone_id"],
                        "zone_name": it["to_zone_name"],
                        "value": it["value"],
                        "rank": it["rank"]
                    },
                    "geometry": {
                        "type": "Point",
                        "coordinates": [it["to_lat"], it["to_lon"]]   # [lat, lon]
                    }
                })

            # Lines FeatureCollection: fromâ†’to Nê°œ
            lines = {"type": "FeatureCollection", "features": []}
            for it in g["items"]:
                lines["features"].append({
                    "type": "Feature",
                    "properties": {
                        "from_zone_id": g["from_zone_id"],
                        "from_zone_name": g["from_zone_name"],
                        "to_zone_id": it["to_zone_id"],
                        "to_zone_name": it["to_zone_name"],
                        "value": it["value"],
                        "rank": it["rank"]
                    },
                    "geometry": {
                        "type": "LineString",
                        "coordinates": [
                            [g["from_lat"], g["from_lon"]],   # [lat, lon]
                            [it["to_lat"], it["to_lon"]]       # [lat, lon]
                        ]
                    }
                })

            # ---- â¬‡ï¸ from_zone ì¢Œí‘œ ë³´ì • ë¡œì§: points.features ì „ì²´ ê¸°ì¤€ bounding-boxì˜ ì¤‘ì‹¬ ----
            # None ê°’ ì œê±° í›„ lat/lon ê°ê°ì˜ min/max ê³„ì‚°
            all_lats = []
            all_lons = []
            for feat in points["features"]:
                coords = feat.get("geometry", {}).get("coordinates", [])
                if not coords or len(coords) < 2:
                    continue
                lat, lon = coords[0], coords[1]  # í˜„ì¬ êµ¬ì¡°: [lat, lon]
                if lat is not None and lon is not None:
                    all_lats.append(lat)
                    all_lons.append(lon)

            if all_lats and all_lons:
                min_lat, max_lat = min(all_lats), max(all_lats)
                min_lon, max_lon = min(all_lons), max(all_lons)
                mid_lat = round((min_lat + max_lat) / 2.0, 4)
                mid_lon = round((min_lon + max_lon) / 2.0, 4)
                from_coords = [mid_lat, mid_lon]     # [lat, lon]
            else:
                # ì•ˆì „ê°€ë“œ: ê³„ì‚° ë¶ˆê°€ ì‹œ ì›ë˜ ì¢Œí‘œ ì‚¬ìš©
                from_coords = [
                    g["from_lat"] if g["from_lat"] is not None else 0.0,
                    g["from_lon"] if g["from_lon"] is not None else 0.0
                ]

            payload.append({
                "from_zone": {
                    "id": g["from_zone_id"],
                    "name": g["from_zone_name"],
                    "coordinates": from_coords  # ìƒˆë¡œ ê³„ì‚°ëœ ì„¼í„° ì¢Œí‘œ [lat, lon]
                },
                "points": points,
                "lines": lines,
                "meta": {"top": top_n, "format": "geojson", "coord_order": "latlon"}
            })

        result = payload[0] if from_filter and len(payload) == 1 else payload
        conn.close()

        # ---- í—¤ë”: ETag / X-Dataset-Date / X-Next-Update ----
        body = json.dumps(result, ensure_ascii=False)
        etag = f'"{hashlib.md5(body.encode("utf-8")).hexdigest()}"'  # ë”°ì˜´í‘œ í¬í•¨

        # ë°ì´í„°ì…‹ ë‚ ì§œ: ì´ í…Œì´ë¸”ì—ëŠ” ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ìœ¼ë‹ˆ KST ì˜¤ëŠ˜(YYYYMMDD)ë¡œ í‘œê¸°
        dataset_date = datetime.now(KST).strftime("%Y%m%d")

        # ë‹¤ìŒ ì—…ë°ì´íŠ¸ ì‹œê°: ë§¤ì¼ 06:00 KST
        now_kst = datetime.now(KST)
        next_update = now_kst.replace(hour=6, minute=0, second=0, microsecond=0)
        if now_kst >= next_update:
            next_update += timedelta(days=1)
        x_next_update = next_update.isoformat()

        # If-None-Match ì²˜ë¦¬(ë”°ì˜´í‘œ ìœ ë¬´ ëª¨ë‘ í—ˆìš©)
        inm_raw = request.headers.get("If-None-Match", "")
        inm = inm_raw.strip().strip('"').replace("W/","").strip()
        if inm == etag.strip('"'):
            resp = make_response("", 304)
            resp.headers["ETag"] = etag
            resp.headers["X-Dataset-Date"] = dataset_date
            resp.headers["X-Next-Update"] = x_next_update
            resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
            resp.headers["Pragma"] = "no-cache"
            resp.headers["Expires"] = "0"
            return resp

        # 200 ì‘ë‹µ
        resp = make_response(body, 200)
        resp.headers["Content-Type"] = "application/json; charset=utf-8"
        resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
        resp.headers["Pragma"] = "no-cache"
        resp.headers["Expires"] = "0"
        resp.headers["ETag"] = etag
        resp.headers["X-Dataset-Date"] = dataset_date
        resp.headers["X-Next-Update"] = x_next_update
        return resp

    except Exception as e:
        resp = make_response(jsonify({
            "status": "fail",
            "message": "OD ë¶„ì„ ì¤‘ ì—ëŸ¬ ë°œìƒ",
            "error": str(e),
            "timestamp": get_current_time()
        }), 500)
        resp.headers["Cache-Control"] = "no-store"
        return resp

# ========================================================= [ ëª¨ë‹ˆí„°ë§ 4K - ë¶„ì„ì§€ì—­ë³„ êµí†µíë¦„ í†µê³„ì •ë³´ ]

@app.route('/monitoring/statistics-traffic-flow', methods=['GET'])
def statistics_traffic_flow():
    try:
        # --- 0) ë‚ ì§œ ê³ ì •(í…ŒìŠ¤íŠ¸) ---
        now_kst = datetime.now(KST)  # ì°¸ê³ ìš©
        # rule_date = resolve_dataset_date(now_kst)  # â–¶ ë°°í¬ ì‹œ ë³µì›
        rule_date = "20250701"  # â–¶ í…ŒìŠ¤íŠ¸ìš© (YYYYMMDD)

        # --- ê³µí†µ í—¤ë”: ë‹¤ìŒ ì—…ë°ì´íŠ¸ ì‹œê°(ë§¤ì¼ 06:00 KST) ---
        next_update = now_kst.replace(hour=6, minute=0, second=0, microsecond=0)
        if now_kst >= next_update:
            next_update += timedelta(days=1)
        x_next_update = next_update.isoformat()

        # --- 0-1) hour ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° íŒŒì‹±/ê²€ì¦ ---
        ALLOWED_HOURS = {"08", "11", "14", "17"}
        hours_raw = (request.args.get('hour') or '').strip()
        hours_filter = None
        if hours_raw:
            parts = [p.strip() for p in hours_raw.split(",") if p.strip()]
            invalid = [p for p in parts if p not in ALLOWED_HOURS]
            if invalid:
                return jsonify({
                    "status": "error",
                    "message": f"Invalid hour value(s): {', '.join(invalid)}",
                    "allowed": sorted(list(ALLOWED_HOURS))
                }), 400
            hours_filter = set(parts)  # {"08","11"} ë“±

        # âœ… ETagìš© í‚¤ë¥¼ ì •ìˆ˜ íŠœí”Œë¡œ ì •ê·œí™” (ì˜ˆ: {"08","11"} -> (8,11))
        if hours_filter:
            hours_key_for_etag = tuple(sorted(int(h) for h in hours_filter))
        else:
            hours_key_for_etag = None

        conn = get_connection()
        cursor = conn.cursor()

        # ============================================================
        # A) documents êµ¬ì„±: HOUR_LINK_RESULT + LINK_VERTEX (ì‹œê°„ í•„í„° ë°˜ì˜)
        # ============================================================
        cursor.execute("""
            SELECT STAT_HOUR, LINK_ID, DISTRICT, SA_NO, VC, VOLUME, SPEED
            FROM HOUR_LINK_RESULT
            WHERE SUBSTR(STAT_HOUR, 1, 8) = ?
        """, [rule_date])
        rows = cursor.fetchall()
        rows = [tuple(r) for r in rows] if rows else []
        if not rows:
            # ì›ìë£Œ ì—†ìŒ â†’ 404 (ì •ì±…ìƒ ETag ì—†ìŒ)
            return jsonify({"status": "fail", "message": f"{rule_date} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}), 404

        df_result = pd.DataFrame(
            rows,
            columns=["STAT_HOUR", "LINK_ID", "DISTRICT", "SA_NO", "VC", "VOLUME", "SPEED"]
        )

        # ì‹œê°„ í•„í„° ì ìš©(ìš”ì²­ ì‹œ)
        if hours_filter:
            # STAT_HOUR: yyyymmddHH â†’ ë’¤ 2ìë¦¬(HH) ê¸°ì¤€ í•„í„°
            df_result = df_result[df_result["STAT_HOUR"].str[-2:].isin(hours_filter)]

        # ë¹ˆ ê²°ê³¼ 204 + ETag
        if df_result.empty:
            etag = f'"{make_etag(rule_date, hours_key_for_etag, total_rows=0)}"'
            resp = Response(status=204)
            resp.headers["ETag"] = etag
            resp.headers["X-Dataset-Date"] = rule_date
            resp.headers["X-Next-Update"] = x_next_update
            resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
            resp.headers["Pragma"] = "no-cache"
            resp.headers["Expires"] = "0"
            return resp

        # ETag / If-None-Match
        etag = f'"{make_etag(rule_date, hours_key_for_etag, total_rows=len(df_result))}"'
        inm_raw = request.headers.get("If-None-Match", "")
        inm = inm_raw.strip().strip('"').replace("W/","").strip()
        if inm == etag.strip('"'):
            resp = Response(status=304)
            resp.headers["ETag"] = etag
            resp.headers["X-Dataset-Date"] = rule_date
            resp.headers["X-Next-Update"] = x_next_update
            resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
            resp.headers["Pragma"] = "no-cache"
            resp.headers["Expires"] = "0"
            return resp
        # -----------------------------------------------------------

        # ìˆ«ìí˜• ë³´ì •(ë¬¸ìì—´ì€ ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
        df_result["SPEED"] = pd.to_numeric(df_result["SPEED"], errors="coerce")
        df_result["VC"] = pd.to_numeric(df_result["VC"], errors="coerce")
        df_result["VOLUME"] = pd.to_numeric(df_result["VOLUME"], errors="coerce")

        # LINK_ID ëª©ë¡ ì¶”ì¶œ(ìˆœì„œ ë³´ì¡´ ì¤‘ë³µì œê±°)
        all_ids_in_order = [str(x).strip() for x in df_result["LINK_ID"].tolist() if str(x).strip()]
        unique_ids = list(dict.fromkeys(all_ids_in_order).keys())

        # LINK_VERTEX ì¼ê´„ ì¡°íšŒ(IN)
        placeholders = ",".join(["?"] * len(unique_ids))
        sql_vertex = f"""
            SELECT LINK_ID, LINK_SEQ, WGS84_X, WGS84_Y
            FROM LINK_VERTEX
            WHERE LINK_ID IN ({placeholders})
            ORDER BY LINK_ID, LINK_SEQ
        """
        cursor.execute(sql_vertex, tuple(unique_ids))
        vrows = cursor.fetchall()
        if not vrows:
            # ë³´ì¡° í…Œì´ë¸” ì—†ìŒ â†’ 404 (ì •ì±…ìƒ ETag ì—†ìŒ)
            return jsonify({"status": "fail", "message": "LINK_VERTEX ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}), 404

        # ì¢Œí‘œ ë§¤í•‘(link_id â†’ [[x,y], ...])  [x=lon, y=lat]
        from collections import defaultdict
        coords_by_id = defaultdict(list)
        for link_id, link_seq, x, y in vrows:
            fx = float(x) if x is not None else None
            fy = float(y) if y is not None else None
            coords_by_id[str(link_id)].append([fx, fy])

        # --- ë§í¬ë³„ ìš”ì•½ ---
        agg_df = (
            df_result.groupby("LINK_ID").agg(
                vc_avg=("VC", "mean"),
                volume_sum=("VOLUME", "sum"),
                speed_avg=("SPEED", "mean")
            ).reset_index()
        )
        vc_by_id = {str(r["LINK_ID"]): (None if pd.isna(r["vc_avg"]) else round(float(r["vc_avg"]), 2))
                    for _, r in agg_df.iterrows()}
        vol_by_id = {str(r["LINK_ID"]): (None if pd.isna(r["volume_sum"]) else int(r["volume_sum"]))
                     for _, r in agg_df.iterrows()}
        spd_by_id = {str(r["LINK_ID"]): (None if pd.isna(r["speed_avg"]) else round(float(r["speed_avg"]), 2))
                     for _, r in agg_df.iterrows()}

        # --- LINK_ID â†’ DISTRICT ë§¤í•‘ (ì²« ë“±ì¥ê°’ ê¸°ì¤€) ---
        df_link_dist = df_result[["LINK_ID", "DISTRICT"]].copy()
        df_link_dist["DISTRICT"] = pd.to_numeric(df_link_dist["DISTRICT"], errors="coerce").astype("Int64")
        df_link_dist = df_link_dist.dropna(subset=["DISTRICT"])
        df_link_dist = df_link_dist.drop_duplicates(subset=["LINK_ID"], keep="first")
        link_to_district = {str(row["LINK_ID"]): int(row["DISTRICT"]) for _, row in df_link_dist.iterrows()}

        # --- êµ¬ì—­ë³„ ë°”ìŠ¤ì¼“ 4ê°œ ìƒì„± ---
        order_codes = [1, 2, 3, 4]
        documents_grouped = [
            {
                "district_no": code,
                "district_name": district_mapping.get(code, str(code)),
                "data": []
            }
            for code in order_codes
        ]
        idx_by_code = {d["district_no"]: i for i, d in enumerate(documents_grouped)}

        # --- ë§í¬ ìš”ì•½ì„ í•´ë‹¹ DISTRICT ë°”ìŠ¤ì¼“ì— ì±„ìš°ê¸° ---
        for lid in unique_ids:
            d_no = link_to_district.get(str(lid))
            if d_no not in idx_by_code:  # DISTRICTê°€ 1~4ê°€ ì•„ë‹ˆë©´ ìŠ¤í‚µ
                continue
            item = {
                "link_id": str(lid),
                "coordinates": coords_by_id.get(str(lid), []),
                "v/c": vc_by_id.get(str(lid)),
                "volume": vol_by_id.get(str(lid)),
                "speed": spd_by_id.get(str(lid))
            }
            documents_grouped[idx_by_code[d_no]]["data"].append(item)

        # ============================================================
        # B) ì¼ í‰ê·  ì†ë„ - daily_average_speed: DAY_LINK_RESULTì—ì„œ ê³„ì‚° (íŒŒë¼ë¯¸í„° ë¬´ê´€)
        # ============================================================
        cursor.execute("""
            SELECT DISTRICT, SA_NO, SPEED
            FROM DAY_LINK_RESULT
            WHERE STAT_DAY = ?
        """, [rule_date])
        day_rows = cursor.fetchall()
        day_rows = [tuple(r) for r in day_rows] if day_rows else []

        daily_average_speed = []
        if day_rows:
            df_day = pd.DataFrame(day_rows, columns=["DISTRICT", "SA_NO", "SPEED"])
            df_day["SPEED"] = pd.to_numeric(df_day["SPEED"], errors="coerce")
            df_day["DISTRICT"] = pd.to_numeric(df_day["DISTRICT"], errors="coerce").astype("Int64")

            overall_avg_by_dist = df_day.groupby("DISTRICT")["SPEED"].mean()

            sa_present_mask = df_day["SA_NO"].notna() & (df_day["SA_NO"].astype(str).str.strip() != "")
            df_day_sa = df_day[sa_present_mask]
            sa_included_by_dist = (
                df_day_sa.groupby("DISTRICT")["SPEED"].mean()
                if not df_day_sa.empty else pd.Series(dtype=float)
            )

            for code in order_codes:
                name = district_mapping.get(code)
                overall = None
                if code in overall_avg_by_dist.index:
                    o = overall_avg_by_dist.loc[code]
                    overall = None if pd.isna(o) else round(float(o), 2)

                sa_included = None
                if code in sa_included_by_dist.index:
                    s = sa_included_by_dist.loc[code]
                    sa_included = None if pd.isna(s) else round(float(s), 2)

                daily_average_speed.append({
                    "district": name,
                    "daily_average_speed": {
                        "overall": overall,
                        "sa_included": sa_included
                    }
                })
        else:
            for code in order_codes:
                daily_average_speed.append({
                    "district": district_mapping.get(code),
                    "daily_average_speed": {"overall": None, "sa_included": None}
                })

        # ============================================================
        # C) hour_label ìƒì„±: 'mmì›” ddì¼ hhì‹œ ~ hhì‹œ'
        #    - ë‹¨ì¼ ì‹œê°„: ë¬¸ìì—´ 1ê°œ
        #    - ë‹¤ì¤‘/ë¯¸ì§€ì •: í•´ë‹¹ ì‹œê°„ëŒ€ ì „ì²´ì˜ ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
        # ============================================================
        mm = int(rule_date[4:6])
        dd = int(rule_date[6:8])

        def make_label(hh_str: str) -> str:
            start_h = int(hh_str)
            end_h = (start_h + 1) % 24
            return f"{mm}ì›” {dd}ì¼ {start_h:02d}ì‹œ ~ {end_h:02d}ì‹œ"

        if hours_filter and len(hours_filter) == 1:
            single_hour = sorted(list(hours_filter))[0]
            hour_label_value = make_label(single_hour)  # ë¬¸ìì—´
        else:
            # íŒŒë¼ë¯¸í„° ì—†ê±°ë‚˜ ë‹¤ì¤‘ ì‹œê°„ì˜ ê²½ìš°: ë°ì´í„°ì— ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì‹œê°„ëŒ€ ê¸°ì¤€ìœ¼ë¡œ êµ¬ì„±
            if hours_filter:
                candidate_hours = sorted(hours_filter)
            else:
                # df_resultì—ì„œ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì‹œê°„ëŒ€ ì¶”ì¶œ (ALLOWED_HOURSì™€ êµì§‘í•©)
                candidate_hours = sorted(set(df_result["STAT_HOUR"].str[-2:]).intersection(ALLOWED_HOURS))
            hour_label_value = [make_label(h) for h in candidate_hours]  # ë¦¬ìŠ¤íŠ¸

        # ============================================================
        # D) ë³¸ë¬¸ + í—¤ë”(ì •ìƒ 200ì—ì„œë„ ETag í¬í•¨)
        # ============================================================
        payload = {
            "status": "success",
            "hour_label": hour_label_value,
            "row_count": int(len(df_result)),
            "documents": documents_grouped,              # âœ… DISTRICT-ê·¸ë£¹ í˜•íƒœ
            "daily_average_speed": daily_average_speed,  # ê·¸ëŒ€ë¡œ ìœ ì§€
            "hourly_total_traffic_volume": [
                {"district": "êµë™", "traffic_volume": 8888},
                {"district": "ì†¡ì •", "traffic_volume": 7777},
                {"district": "ë„ì‹¬", "traffic_volume": 9999},
                {"district": "ê²½í¬", "traffic_volume": 6666}
            ],
            "map_center_coordinates": [
                {"district": "êµë™", "coordinates": [128.874273, 37.765208]},
                {"district": "ì†¡ì •", "coordinates": [128.924538, 37.771808]},
                {"district": "ë„ì‹¬", "coordinates": [128.897176, 37.755575]},
                {"district": "ê²½í¬", "coordinates": [128.891529, 37.787484]}
            ]
        }

        body = json.dumps(payload, ensure_ascii=False)
        resp = Response(body, content_type="application/json; charset=utf-8", status=200)
        resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
        resp.headers["Pragma"] = "no-cache"
        resp.headers["Expires"] = "0"
        resp.headers["ETag"] = etag
        resp.headers["X-Dataset-Date"] = rule_date
        resp.headers["X-Next-Update"] = x_next_update
        return resp

    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500
    finally:
        try:
            cursor.close()
        finally:
            conn.close()

# ========================================================= [ ëª¨ë‹ˆí„°ë§ 4 - êµì°¨ë¡œë³„ í†µí–‰ì •ë³´ ]

@app.route('/monitoring/node-result', methods=['GET'])
def node_result_summary():
    now_kst = datetime.now(KST)  # ì°¸ê³ ìš©
    # rule_date = resolve_dataset_date(now_kst)  # â–¶ ë°°í¬ ì‹œ ë³µì›
    rule_date = "20250701"  # â–¶ í…ŒìŠ¤íŠ¸ìš© (YYYYMMDD)

    # ê³µí†µ í—¤ë” ê³„ì‚°(ë‹¤ìŒ ì—…ë°ì´íŠ¸ ì‹œê°: ë§¤ì¼ 06:00 KST)
    next_update = now_kst.replace(hour=6, minute=0, second=0, microsecond=0)
    if now_kst >= next_update:
        next_update += timedelta(days=1)
    x_next_update = next_update.isoformat()

    try:
        conn = get_connection()
        cursor = conn.cursor()

        # 0) í…Œì´ë¸”ì— ì¡´ì¬í•˜ëŠ” ìµœì‹  ë‚ ì§œ(YYYYMMDD)
        cursor.execute("""
            SELECT STAT_DATE FROM (
                SELECT SUBSTR(STAT_HOUR, 1, 8) AS STAT_DATE
                FROM NODE_RESULT
                GROUP BY SUBSTR(STAT_HOUR, 1, 8)
                ORDER BY STAT_DATE DESC
            )
            WHERE ROWNUM = 1
        """)
        latest_date_row = cursor.fetchone()
        if not latest_date_row:
            return jsonify({"status": "fail", "message": "STAT_HOUR ë‚ ì§œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}), 404
        latest_date = latest_date_row[0]

        # 1) rule_date ë°ì´í„° ìœ ë¬´ í™•ì¸ â†’ ìˆìœ¼ë©´ rule_date ì‚¬ìš©, ì—†ìœ¼ë©´ latest_dateë¡œ í´ë°±
        cursor.execute("""
            SELECT 1
            FROM NODE_RESULT
            WHERE SUBSTR(STAT_HOUR, 1, 8) = ?
              AND ROWNUM = 1
        """, [rule_date])
        has_rule_date = cursor.fetchone() is not None

        active_date = rule_date if has_rule_date else latest_date
        fallback_used = (active_date != rule_date)

        mm, dd = int(active_date[4:6]), int(active_date[6:8])

        # 2) ë°ì´í„° ì¡°íšŒ (active_date ê¸°ì¤€)
        cursor.execute("""
            SELECT STAT_HOUR, TIMEINT, NODE_ID, QLEN, VEHS, DELAY, STOPS
            FROM NODE_RESULT
            WHERE SUBSTR(STAT_HOUR, 1, 8) = ?
        """, [active_date])
        rows = cursor.fetchall()
        rows = [tuple(r) for r in rows]
        if not rows:
            return jsonify({"status": "fail", "message": "í•´ë‹¹ ë‚ ì§œì— ëŒ€í•œ êµì°¨ë¡œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}), 404

        # 3) DataFrame ìƒì„±
        df = pd.DataFrame(rows, columns=["STAT_HOUR", "TIMEINT", "NODE_ID", "QLEN", "VEHS", "DELAY", "STOPS"])
        df[["QLEN", "VEHS", "DELAY", "STOPS"]] = df[["QLEN", "VEHS", "DELAY", "STOPS"]].apply(pd.to_numeric, errors="coerce")
        df["DATE"] = df["STAT_HOUR"].str[:8]
        df["HOUR"] = df["STAT_HOUR"].str[8:10]

        # 4) í‰ê· ê°’ ê³„ì‚°
        df_avg = df.groupby(["DATE", "HOUR", "NODE_ID"], as_index=False).agg({
            "QLEN": "mean",
            "VEHS": "mean",
            "DELAY": "mean",
            "STOPS": "mean"
        }).round(2)

        # 5) êµì°¨ë¡œ ì´ë¦„ ë§¤í•‘
        cursor.execute("SELECT NODE_ID, CROSS_NAME FROM NODE_INFO")
        node_info_rows = cursor.fetchall()
        node_info = [tuple(r) for r in node_info_rows]
        df_node_info = pd.DataFrame(node_info, columns=["NODE_ID", "NODE_NAME"]).drop_duplicates(subset="NODE_ID")

        df_merged = df_avg.merge(df_node_info, on="NODE_ID", how="left")
        df_merged = df_merged[df_merged["NODE_NAME"].notna()].copy()

        # 6) LOS ê³„ì‚°
        def los_alpha(delay):
            if delay < 15: return "A"
            elif delay < 30: return "B"
            elif delay < 50: return "C"
            elif delay < 70: return "D"
            elif delay < 100: return "E"
            elif delay < 220: return "F"
            elif delay < 340: return "FF"
            else: return "FFF"

        los_map = {"A":1,"B":2,"C":3,"D":4,"E":5,"F":6,"FF":6,"FFF":6}
        df_merged["LOS_NUM"] = df_merged["DELAY"].apply(lambda d: los_map[los_alpha(d)])

        # 7) ì»¬ëŸ¼ ì†Œë¬¸ì
        df_merged.rename(columns={
            "NODE_NAME": "node_name",
            "QLEN": "qlen",
            "VEHS": "vehs",
            "DELAY": "delay",
            "STOPS": "stops"
        }, inplace=True)

        # 8) ì‹œê°„ ë¸”ë¡ ë°ì´í„° êµ¬ì„± (active_dateì˜ mm/ddë¡œ ë¼ë²¨ ìƒì„±)
        data_blocks = []
        for hour, group in df_merged.groupby("HOUR"):
            h = int(hour); h_next = (h + 1) % 24
            hour_label = f"{mm}ì›” {dd}ì¼ {h:02d}ì‹œ ~ {h_next:02d}ì‹œ"

            items = []
            for _, r in group.iterrows():
                qlen  = float(r["qlen"])  if pd.notna(r["qlen"])  else 0.0
                vehs  = int(r["vehs"])    if pd.notna(r["vehs"])  else 0.0
                delay = float(r["delay"]) if pd.notna(r["delay"]) else 0.0
                stops = float(r["stops"]) if pd.notna(r["stops"]) else 0.0
                los   = int(r["LOS_NUM"]) if pd.notna(r["LOS_NUM"]) else None

                items.append({
                    "node_name": str(r["node_name"]),
                    "qlen": f"{qlen:.1f}",
                    "vehs": f"{int(vehs)}",
                    "delay": f"{delay:.1f}",
                    "stops": f"{stops:.1f}",
                    "los": f"{los}",
                    "max_qlen": f"{qlen * 1.5:.1f}",
                    "max_vehs": f"{int(vehs * 1.5)}",
                    "max_delay": f"{delay * 1.5:.1f}",
                    "max_stops": "5",
                    "max_los": "6"
                })

            data_blocks.append({"hour_label": hour_label, "items": items})

        # 9) Payload & ETag
        payload = {
            # "requested_date": rule_date,     # ìš”ì²­ ì˜ë„
            # "active_date": active_date,      # ì‹¤ì œ ì¡°íšŒì— ì‚¬ìš©ëœ ë‚ ì§œ
            # "fallback_used": fallback_used,  # rule_date ë°ì´í„° ì—†ì–´ì„œ ìµœì‹ ìœ¼ë¡œ ëŒ€ì²´í–ˆëŠ”ì§€ ì—¬ë¶€
            "target_date": latest_date,
            "data": data_blocks
        }
        body = json.dumps(payload, ensure_ascii=False)
        etag = f'"{hashlib.md5(body.encode("utf-8")).hexdigest()}"'

        # 10) ë‹¤ìŒ ì—…ë°ì´íŠ¸ ì‹œê°„ (ë§¤ì¼ 06:00 KST)
        now_kst = datetime.now(KST)
        next_update_kst = now_kst.replace(hour=6, minute=0, second=0, microsecond=0)
        if now_kst >= next_update_kst:
            next_update_kst += timedelta(days=1)

        x_next_update_str = next_update_kst.isoformat()

        # 11) If-None-Match ì²˜ë¦¬
        inm_raw = request.headers.get("If-None-Match", "")
        inm = inm_raw.strip().strip('"').replace("W/", "").strip()
        if inm == etag.strip('"'):
            resp = Response(status=304)
            resp.headers["ETag"] = etag
            resp.headers["X-Dataset-Date"] = active_date
            resp.headers["X-Next-Update"] = x_next_update_str
            return resp

        # 12) ì •ìƒ ì‘ë‹µ
        resp = Response(body, content_type="application/json; charset=utf-8", status=200)
        resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
        resp.headers["Pragma"] = "no-cache"
        resp.headers["Expires"] = "0"
        resp.headers["ETag"] = etag
        resp.headers["X-Dataset-Date"] = active_date
        resp.headers["X-Next-Update"] = x_next_update_str
        return resp

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return jsonify({
            "status": "fail",
            "message": "êµì°¨ë¡œ ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
            "error": str(e),
            "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }), 500

# ========================================================= [ ëª¨ë‹ˆí„°ë§ 5 - ë„ë¡œêµ¬ê°„ë³„ í†µí–‰ëŸ‰ ì •ë³´ ]

@app.route('/monitoring/road-traffic-info', methods=['GET'])
def road_traffic_info():
    pass









# ========================================================= [ ì‹ í˜¸ìš´ì˜ 1 - ë„ë¡œì¶•ë³„ í†µê³„ì •ë³´ ]

@app.route('/signal/vttm-result', methods=['GET'])
def vttm_result_summary():
    try:
        conn = get_connection()
        cursor = conn.cursor()

        # 1) ê°€ì¥ ìµœì‹  ë‚ ì§œ(YYYYMMDD)
        cursor.execute("""
            SELECT STAT_DATE FROM (
                SELECT SUBSTR(STAT_HOUR, 1, 8) AS STAT_DATE
                FROM NODE_RESULT
                GROUP BY SUBSTR(STAT_HOUR, 1, 8)
                ORDER BY STAT_DATE DESC
            )
            WHERE ROWNUM = 1
        """)
        latest_date_row = cursor.fetchone()
        if not latest_date_row:
            return jsonify({"status": "fail", "message": "STAT_HOUR ë‚ ì§œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}), 404
        latest_date = latest_date_row[0]

        # 2) í•´ë‹¹ ë‚ ì§œ VTTM ê²°ê³¼
        cursor.execute("""
            SELECT DISTRICT, STAT_HOUR, FROM_NODE_NAME, TO_NODE_NAME, UPDOWN, DISTANCE, TRAVEL_TIME
            FROM VTTM_RESULT
            WHERE SUBSTR(STAT_HOUR, 1, 8) = ?
        """, [latest_date])
        rows = cursor.fetchall()
        rows = [tuple(r) for r in rows] if rows else []
        if not rows:
            return jsonify({"status": "fail", "message": "í•´ë‹¹ ë‚ ì§œì— ëŒ€í•œ VTTM ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}), 404

        columns = ['DISTRICT', 'STAT_HOUR', 'FROM_NODE_NAME', 'TO_NODE_NAME', 'UPDOWN', 'DISTANCE', 'TRAVEL_TIME']

        # (district_name, hour_label, segment_key) -> {'0': {...}, '1': {...}}
        pair_buffer = defaultdict(dict)

        for row in rows:
            record = dict(zip(columns, row))
            district_id = record['DISTRICT']
            stat_hour   = record['STAT_HOUR']

            # "HHì‹œ ~ HH+1ì‹œ"
            hour_val   = int(stat_hour[-2:])
            hour_label = f"{hour_val}ì‹œ ~ {(hour_val + 1) % 24}ì‹œ"

            district_name = district_mapping.get(district_id, f"ê¸°íƒ€ì§€ì—­-{district_id}")

            from_node = str(record['FROM_NODE_NAME'])
            to_node   = str(record['TO_NODE_NAME'])
            updown    = str(record['UPDOWN'])
            distance  = float(record['DISTANCE'] or 0)
            ttime_val = float(record['TRAVEL_TIME'] or 0)
            tcost_val = float((record.get('TRAVEL_COST') or 0))  # SELECTì— ì—†ìœ¼ë©´ 0

            travel_time  = round(ttime_val, 1) if ttime_val > 0 else 0.0
            travel_speed = round((distance / ttime_val) * 3.6, 1) if ttime_val > 0 else 0.0
            travel_cost  = round(tcost_val, 1) if tcost_val > 0 else 0.0

            segment_key = tuple(sorted([from_node, to_node]))
            key = (district_name, hour_label, segment_key)

            pair_buffer[key][updown] = {
                "from_node": from_node,
                "to_node": to_node,
                "travel_time": travel_time,
                "travel_speed": travel_speed,
                "travel_cost": travel_cost
            }

        # hour_label -> district_name -> items[]
        hour_district_map = defaultdict(lambda: defaultdict(list))

        for (district_name, hour_label, segment_key), directions in pair_buffer.items():
            if '0' in directions and '1' in directions:
                from_node_data = directions['0']
                to_node_data   = directions['1']

                dir_list = [
                    {
                        "updown": 0,
                        "travel_time": from_node_data['travel_time'],
                        "travel_speed": from_node_data['travel_speed'],
                        "travel_cost": from_node_data['travel_cost']
                    },
                    {
                        "updown": 1,
                        "travel_time": to_node_data['travel_time'],
                        "travel_speed": to_node_data['travel_speed'],
                        "travel_cost": to_node_data['travel_cost']
                    }
                ]

                data_collection = {
                    "traffic_vol": 0,  # TODO: ì¿¼ë¦¬ ì—°ë™ ì‹œ ì‹¤ì œ ê°’ìœ¼ë¡œ ëŒ€ì²´
                    "travel_speed": from_node_data['travel_speed'],
                    "travel_time": from_node_data['travel_time']
                }

                hour_district_map[hour_label][district_name].append({
                    "from_node": segment_key[0],
                    "to_node": segment_key[1],
                    "directions": dir_list,
                    "data_collection": data_collection
                })

        # ë°°ì—´ë¡œ ë³€í™˜ (value ì¤‘ì‹¬)
        data_blocks = []
        for hour_label, districts in hour_district_map.items():
            for district_name, items in districts.items():
                data_blocks.append({
                    "hour_label": hour_label,
                    "district": district_name,
                    "items": items
                })

        payload = {
            "status": "success",
            "target_date": latest_date,
            "data": data_blocks
        }

        # ---- ETag / X-Next-Update / X-Dataset-Date ----
        body = json.dumps(payload, ensure_ascii=False)
        etag = f'"{hashlib.md5(body.encode("utf-8")).hexdigest()}"'  # ìŒë”°ì˜´í‘œ í¬í•¨

        now_kst = datetime.now(KST)
        next_update = now_kst.replace(hour=6, minute=0, second=0, microsecond=0)
        if now_kst >= next_update:
            next_update += timedelta(days=1)
        x_next_update = next_update.isoformat()

        # If-None-Match ì²˜ë¦¬(ë”°ì˜´í‘œ/Weak í—ˆìš©)
        inm_raw = request.headers.get("If-None-Match", "")
        inm = inm_raw.strip().strip('"').replace("W/", "").strip()
        if inm == etag.strip('"'):
            resp = Response(status=304)
            resp.headers["ETag"] = etag
            resp.headers["X-Dataset-Date"] = latest_date
            resp.headers["X-Next-Update"] = x_next_update
            resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
            resp.headers["Pragma"] = "no-cache"
            resp.headers["Expires"] = "0"
            return resp

        # 200 ì‘ë‹µ
        resp = Response(body, content_type="application/json; charset=utf-8", status=200)
        resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
        resp.headers["Pragma"] = "no-cache"
        resp.headers["Expires"] = "0"
        resp.headers["ETag"] = etag
        resp.headers["X-Dataset-Date"] = latest_date
        resp.headers["X-Next-Update"] = x_next_update
        return resp

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return jsonify({
            "status": "fail",
            "message": "êµì°¨ë¡œ ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
            "error": str(e),
            "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }), 500

# ========================================================= [ ì‹ í˜¸ìš´ì˜ 4K - ê¶Œì—­ë³„ ì‹œê°„ëŒ€ë³„ êµí†µí˜¼ì¡ ì •ë³´ ]

@app.route('/signal/district-hourly-congested-info', methods=['GET'])
def hourly_congested_info_data():
    try:
        # --- 0) ë‚ ì§œ ê³ ì •(í…ŒìŠ¤íŠ¸) ---
        now_kst = datetime.now(KST)  # ì°¸ê³ ìš©
        # rule_date = resolve_dataset_date(now_kst)  # â–¶ ë°°í¬ ì‹œ ë³µì›
        rule_date = "20250701"  # â–¶ í…ŒìŠ¤íŠ¸ìš©

        # --- ê³µí†µ í—¤ë”: ë‹¤ìŒ ì—…ë°ì´íŠ¸ ì‹œê°(ë§¤ì¼ 06:00 KST) ---
        next_update = now_kst.replace(hour=6, minute=0, second=0, microsecond=0)
        if now_kst >= next_update:
            next_update += timedelta(days=1)
        x_next_update = next_update.isoformat()

        # --- 0-1) hour ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° íŒŒì‹±/ê²€ì¦ ---
        ALLOWED_HOURS = {"08", "11", "14", "17"}
        hours_raw = (request.args.get('hour') or '').strip()
        hours_filter = None
        if hours_raw:
            parts = [p.strip() for p in hours_raw.split(",") if p.strip()]
            invalid = [p for p in parts if p not in ALLOWED_HOURS]
            if invalid:
                return jsonify({
                    "status": "error",
                    "message": f"Invalid hour value(s): {', '.join(invalid)}",
                    "allowed": sorted(list(ALLOWED_HOURS))
                }), 400
            hours_filter = set(parts)  # {"08","11"} ë“±

        # If-None-Match ìˆ˜ì‹  (ETag ë¹„êµëŠ” ì‘ë‹µ ì§ì „ì— ìˆ˜í–‰)
        inm_raw = request.headers.get("If-None-Match", "")
        inm = inm_raw.strip().strip('"').replace("W/","").strip()

        # âœ… ETagìš© í‚¤ ì •ê·œí™”
        hours_key_for_etag = tuple(sorted(int(h) for h in hours_filter)) if hours_filter else None

        conn = get_connection()
        cursor = conn.cursor()

        # ============================================================
        # A) ê¶Œì—­ë³„ ë¶„ì„ì •ë³´ : NP_RESULT (ì‹œê°„ í•„í„° ë° ê¶Œì—­ë³„ COST í•©ê³„)
        # ============================================================
        cursor.execute("""
            SELECT DISTRICT, STAT_HOUR, VEHS, COST
            FROM TOMMS.NP_RESULT
            WHERE SUBSTR(TO_CHAR(STAT_HOUR), 1, 8) = ?
        """, [rule_date])
        rows = cursor.fetchall()
        rows = [tuple(r) for r in rows] if rows else []
        if not rows:
            return jsonify({"status": "fail", "message": f"{rule_date} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}), 404

        df_np_result = pd.DataFrame(rows, columns=["DISTRICT", "STAT_HOUR", "VEHS", "COST"])

        # ì‹œê°„ í•„í„° ì ìš©(ìš”ì²­ ì‹œ)
        if hours_filter:
            df_np_result = df_np_result[df_np_result["STAT_HOUR"].astype(str).str[-2:].isin(hours_filter)]

        # ë¹ˆ ê²°ê³¼ 204
        if df_np_result.empty:
            etag = f'"{make_etag(rule_date, hours_key_for_etag, total_rows=0)}"'
            resp = Response(status=204)
            resp.headers["ETag"] = etag
            resp.headers["X-Dataset-Date"] = rule_date
            resp.headers["X-Next-Update"] = x_next_update
            resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
            resp.headers["Pragma"] = "no-cache"
            resp.headers["Expires"] = "0"
            return resp

        # ---- íƒ€ì… ë³´ì • (A)
        df_np_result["DISTRICT"] = pd.to_numeric(df_np_result["DISTRICT"], errors="coerce")
        df_np_result["COST"] = pd.to_numeric(df_np_result["COST"], errors="coerce")
        df_np_result["STAT_HOUR"] = df_np_result["STAT_HOUR"].astype(str).str[:10]  # 'YYYYMMDDHH'
        df_np_result["HH"] = df_np_result["STAT_HOUR"].str[-2:]
        df_np_result["MM"] = df_np_result["STAT_HOUR"].str[4:6]
        df_np_result["DD"] = df_np_result["STAT_HOUR"].str[6:8]

        # ë™ì¼ í‚¤ ì¤‘ë³µ ë°©ì§€: ì‹œê°„/ê¶Œì—­ë³„ COST í•©
        agg_a = (
            df_np_result
            .groupby(["STAT_HOUR", "HH", "MM", "DD", "DISTRICT"], dropna=False, as_index=False)["COST"]
            .sum()
        )

        def build_hours_order(present_hours, hours_filter):
            if hours_filter:
                return [h for h in ["08", "11", "14", "17"] if h in hours_filter]
            return [h for h in ["08", "11", "14", "17"] if h in present_hours]

        hours_order_a = build_hours_order(set(agg_a["HH"].unique().tolist()), hours_filter)

        def make_hour_label(mm: str, dd: str, hh: str) -> str:
            hs = int(hh)
            he = (hs + 1) % 24
            return f"{mm}ì›” {dd}ì¼ {hs:02d}ì‹œ ~ {he:02d}ì‹œ"

        # ---- documents ê¸°ë³¸ ê³¨ê²© ìƒì„±(A: district_data)
        documents = []
        doc_by_hh = {}
        for hh in hours_order_a:
            sub = agg_a[agg_a["HH"] == hh]
            if sub.empty:
                continue
            mm = sub.iloc[0]["MM"]
            dd = sub.iloc[0]["DD"]
            hour_label = make_hour_label(mm, dd, hh)

            district_data = []
            for dno in [1, 2, 3, 4]:
                row = sub[sub["DISTRICT"] == dno]
                cost_val = float(row.iloc[0]["COST"]) if not row.empty and pd.notna(row.iloc[0]["COST"]) else None
                district_data.append({
                    "district_no": dno,
                    "district": district_mapping.get(dno, str(dno)),
                    "cost": cost_val
                })

            doc = {"hour_label": hour_label, "district_data": district_data}
            documents.append(doc)
            doc_by_hh[hh] = doc  # ì´í›„ B ì„¹ì…˜ì—ì„œ ì‹œê°„ëŒ€ ë§¤ì¹­ ì‹œ í™œìš©

        # ============================================================
        # B-1) ë„ë¡œ(ROAD_NAME) ê¸°ì¤€ í‰ê· : HOUR_LINK_RESULT
        #      - AVG(VOLUME), AVG(VC)
        #      - UPDOWN ë¬´ì‹œ
        # ============================================================
        cursor.execute("""
            SELECT DISTRICT, STAT_HOUR, ROAD_NAME,
                   AVG(VOLUME) AS VOLUME_AVG,
                   AVG(VC)     AS VC_AVG
            FROM TOMMS.HOUR_LINK_RESULT
            WHERE SUBSTR(TO_CHAR(STAT_HOUR), 1, 8) = ?
            GROUP BY DISTRICT, STAT_HOUR, ROAD_NAME
        """, [rule_date])
        link_rows = cursor.fetchall()
        link_rows = [tuple(r) for r in link_rows] if link_rows else []
        df_link = pd.DataFrame(
            link_rows,
            columns=["DISTRICT", "STAT_HOUR", "ROAD_NAME", "VOLUME_AVG", "VC_AVG"]
        ) if link_rows else pd.DataFrame(columns=["DISTRICT","STAT_HOUR","ROAD_NAME","VOLUME_AVG","VC_AVG"])

        if not df_link.empty:
            if hours_filter:
                df_link = df_link[df_link["STAT_HOUR"].astype(str).str[-2:].isin(hours_filter)]
            df_link["DISTRICT"]   = pd.to_numeric(df_link["DISTRICT"], errors="coerce")
            df_link["VOLUME_AVG"] = pd.to_numeric(df_link["VOLUME_AVG"], errors="coerce")
            df_link["VC_AVG"]     = pd.to_numeric(df_link["VC_AVG"], errors="coerce")
            df_link["STAT_HOUR"]  = df_link["STAT_HOUR"].astype(str).str[:10]
            df_link["HH"] = df_link["STAT_HOUR"].str[-2:]
            df_link["MM"] = df_link["STAT_HOUR"].str[4:6]
            df_link["DD"] = df_link["STAT_HOUR"].str[6:8]

            hours_order_b1 = build_hours_order(set(df_link["HH"].unique().tolist()), hours_filter)
            for hh in hours_order_b1:
                if hh not in doc_by_hh:
                    sub_any = df_link[df_link["HH"] == hh]
                    if sub_any.empty:
                        continue
                    mm = sub_any.iloc[0]["MM"]; dd = sub_any.iloc[0]["DD"]
                    doc = {"hour_label": make_hour_label(mm, dd, hh), "district_data": []}
                    documents.append(doc)
                    doc_by_hh[hh] = doc

            # ì‹œê°„ëŒ€ë³„ road_data ìƒì„±/ë¶€ì°©
            for hh, doc in doc_by_hh.items():
                sub = df_link[df_link["HH"] == hh]
                if sub.empty:
                    doc["road_data"] = []
                    continue
                road_data = []
                for dno in [1, 2, 3, 4]:
                    part = sub[sub["DISTRICT"] == dno].copy()
                    if part.empty:
                        road_data.append({
                            "district_no": dno,
                            "district": district_mapping.get(dno, str(dno)),
                            "roads": []
                        })
                        continue
                    part = part.sort_values(["ROAD_NAME"]).reset_index(drop=True)
                    roads = []
                    for _, r in part.iterrows():
                        roads.append({
                            "road_name": (str(r["ROAD_NAME"]) if pd.notna(r["ROAD_NAME"]) else None),
                            "volume_avg": (float(r["VOLUME_AVG"]) if pd.notna(r["VOLUME_AVG"]) else None),
                            "vc_avg":     (float(r["VC_AVG"]) if pd.notna(r["VC_AVG"]) else None),
                        })
                    road_data.append({
                        "district_no": dno,
                        "district": district_mapping.get(dno, str(dno)),
                        "roads": roads
                    })
                doc["road_data"] = road_data

        # ============================================================
        # B-2) ì¶•(SA_NO) ê¸°ì¤€ í‰ê· : HOUR_LINK_RESULT
        #      - AVG(VC) -> ì†Œìˆ˜ì  2ìë¦¬
        #      - AVG(SPEED) -> ì†Œìˆ˜ì  1ìë¦¬
        #      - UPDOWN ë¬´ì‹œ
        # ============================================================
        cursor.execute("""
            SELECT DISTRICT, STAT_HOUR, SA_NO,
                   AVG(VC)    AS VC_AVG,
                   AVG(SPEED) AS SPEED_AVG
            FROM TOMMS.HOUR_LINK_RESULT
            WHERE SUBSTR(TO_CHAR(STAT_HOUR), 1, 8) = ?
            GROUP BY DISTRICT, STAT_HOUR, SA_NO
        """, [rule_date])
        sa_rows = cursor.fetchall()
        sa_rows = [tuple(r) for r in sa_rows] if sa_rows else []
        df_sa = pd.DataFrame(
            sa_rows,
            columns=["DISTRICT", "STAT_HOUR", "SA_NO", "VC_AVG", "SPEED_AVG"]
        ) if sa_rows else pd.DataFrame(columns=["DISTRICT","STAT_HOUR","SA_NO","VC_AVG","SPEED_AVG"])

        if not df_sa.empty:
            if hours_filter:
                df_sa = df_sa[df_sa["STAT_HOUR"].astype(str).str[-2:].isin(hours_filter)]
            df_sa["DISTRICT"]  = pd.to_numeric(df_sa["DISTRICT"], errors="coerce")
            df_sa["VC_AVG"]    = pd.to_numeric(df_sa["VC_AVG"], errors="coerce")
            df_sa["SPEED_AVG"] = pd.to_numeric(df_sa["SPEED_AVG"], errors="coerce")
            df_sa["STAT_HOUR"] = df_sa["STAT_HOUR"].astype(str).str[:10]
            df_sa["HH"] = df_sa["STAT_HOUR"].str[-2:]
            df_sa["MM"] = df_sa["STAT_HOUR"].str[4:6]
            df_sa["DD"] = df_sa["STAT_HOUR"].str[6:8]

            hours_order_b2 = build_hours_order(set(df_sa["HH"].unique().tolist()), hours_filter)
            for hh in hours_order_b2:
                if hh not in doc_by_hh:
                    sub_any = df_sa[df_sa["HH"] == hh]
                    if sub_any.empty:
                        continue
                    mm = sub_any.iloc[0]["MM"]; dd = sub_any.iloc[0]["DD"]
                    doc = {"hour_label": make_hour_label(mm, dd, hh), "district_data": []}
                    documents.append(doc)
                    doc_by_hh[hh] = doc

            # ë°˜ì˜¬ë¦¼ ìœ í‹¸
            def round_or_none(x, nd):
                return round(float(x), nd) if (x is not None and pd.notna(x)) else None

            # ì‹œê°„ëŒ€ë³„ sa_data ìƒì„±/ë¶€ì°©
            for hh, doc in doc_by_hh.items():
                sub = df_sa[df_sa["HH"] == hh]
                if sub.empty:
                    doc["sa_data"] = []
                    continue
                sa_data = []
                for dno in [1, 2, 3, 4]:
                    part = sub[sub["DISTRICT"] == dno].copy()
                    if part.empty:
                        sa_data.append({
                            "district_no": dno,
                            "district": district_mapping.get(dno, str(dno)),
                            "segments": []
                        })
                        continue
                    # SA_NO ì •ë ¬
                    part = part.sort_values(["SA_NO"]).reset_index(drop=True)
                    segments = []
                    for _, r in part.iterrows():
                        segments.append({
                            "sa_no": (str(r["SA_NO"]) if pd.notna(r["SA_NO"]) else None),
                            "vc_avg":   round_or_none(r["VC_AVG"], 2),  # ì†Œìˆ˜ì  2ìë¦¬
                            "speed_avg": round_or_none(r["SPEED_AVG"], 1)  # ì†Œìˆ˜ì  1ìë¦¬
                        })
                    sa_data.append({
                        "district_no": dno,
                        "district": district_mapping.get(dno, str(dno)),
                        "segments": segments
                    })
                doc["sa_data"] = sa_data

        # ---- ETag ê³„ì‚°(A+B1+B2 í•©ì‚°)
        total_rows = len(agg_a) + (len(df_link) if not df_link.empty else 0) + (len(df_sa) if not df_sa.empty else 0)
        etag = f'"{make_etag(rule_date, hours_key_for_etag, total_rows=total_rows)}"'
        if inm == etag.strip('"'):
            resp = Response(status=304)
            resp.headers["ETag"] = etag
            resp.headers["X-Dataset-Date"] = rule_date
            resp.headers["X-Next-Update"] = x_next_update
            resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
            resp.headers["Pragma"] = "no-cache"
            resp.headers["Expires"] = "0"
            return resp

        payload = {
            "status": "success",
            "rule_date": rule_date,     # 'YYYYMMDD'
            # documents[*] = {
            #   hour_label,
            #   district_data: [{district_no, district, cost} Ã—4],
            #   road_data: [{district_no, district, roads: [{road_name, volume_avg, vc_avg}...]} Ã—4],
            #   sa_data:   [{district_no, district, segments: [{sa_no, vc_avg(2), speed_avg(1)}...]} Ã—4]
            # }
            "documents": documents
        }

        resp = jsonify(payload)
        resp.headers["ETag"] = etag
        resp.headers["X-Dataset-Date"] = rule_date
        resp.headers["X-Next-Update"] = x_next_update
        resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
        resp.headers["Pragma"] = "no-cache"
        resp.headers["Expires"] = "0"
        return resp, 200

    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500
    finally:
        try:
            cursor.close()
        finally:
            conn.close()

# ========================================================= [ ì‹ í˜¸ìš´ì˜ 4 - êµì°¨ë¡œë³„ íš¨ê³¼ì§€í‘œ ë¶„ì„ì •ë³´ ]

def _run_sql(cursor, sql, params=None, step=""):
    params = params or []
    try:
        cursor.execute(sql, params)
        return cursor.fetchall()
    except Exception as e:
        # ì„œë²„ ë¡œê·¸ì— ìƒì„¸ ë‚¨ê¸°ê¸°
        print(f"[DB-ERROR] step={step}\nSQL=\n{sql}\nparams={params}\nexc={repr(e)}")
        # í˜¸ì¶œìì—ê²Œ ì—ëŸ¬ ì „ë‹¬
        raise

@app.route('/signal/node-approach-result', methods=['GET'])
def node_approach_result():
    hour_filter = (request.args.get('hour') or '').strip()  # '08','11','14','17' ë“±
    if not (len(hour_filter) == 2 and hour_filter.isdigit() and 0 <= int(hour_filter) <= 23):
        return jsonify({
            "status": "fail",
            "message": f"ìœ íš¨í•˜ì§€ ì•Šì€ hour íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤: {hour_filter}",
            "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }), 400

    conn = None
    try:
        conn = get_connection()
        cursor = conn.cursor()

        # -------------------------------------------- 1) ìµœì‹  ì¼ì (ë‹¨ìˆœí™”: MAX ì‚¬ìš©)
        sql_latest = """
            SELECT MAX(SUBSTR(STAT_HOUR, 1, 8)) AS STAT_DATE
            FROM NODE_DIR_RESULT
        """
        latest_rows = _run_sql(cursor, sql_latest, step="latest_date")
        latest_date_row = latest_rows[0] if latest_rows else None
        if not latest_date_row or not latest_date_row[0]:
            return jsonify({
                "status": "fail",
                "message": "NODE_DIR_RESULTì— STAT_HOUR ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
                "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }), 404

        latest_date = latest_date_row[0]  # 'YYYYMMDD'
        mm, dd = int(latest_date[4:6]), int(latest_date[6:8])
        h = int(hour_filter); h_next = (h + 1) % 24
        label = f"{mm}ì›” {dd}ì¼ {h:02d}ì‹œ ~ {h_next:02d}ì‹œ"

        # -------------------------------------------- 2) ìµœì‹  ì¼ì + í•´ë‹¹ ì‹œ(hour)ë§Œ DBì—ì„œ ë°”ë¡œ í•„í„°
        sql_rows = """
            SELECT STAT_HOUR, TIMEINT, NODE_ID, SA_NO,
                APPR_ID, DIRECTION, QLEN, VEHS, DELAY, STOPS
            FROM TOMMS.NODE_DIR_RESULT
            WHERE SUBSTR(STAT_HOUR, 1, 8) = ?
            AND SUBSTR(STAT_HOUR, -2, 2) = ?
        """
        rows = [tuple(r) for r in cursor.execute(sql_rows, [latest_date, hour_filter]).fetchall()]
        cols = ['STAT_HOUR','TIMEINT','NODE_ID','SA_NO','APPR_ID','DIRECTION','QLEN','VEHS','DELAY','STOPS']
        df = pd.DataFrame(rows, columns=cols)
        if df.empty:
            return jsonify({
                "status": "fail",
                "message": f"{label}ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
                "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }), 404

        # ìˆ«ì ë³€í™˜
        for col in ["APPR_ID","DIRECTION","QLEN","VEHS","DELAY","STOPS"]:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors="coerce")

        ORDERED_SLICES = ['00-15','15-30','30-45','45-00']
        df = df[df['TIMEINT'].isin(ORDERED_SLICES)].copy()

        # -------------------------------------------- 3) NODE_DIR_INFO ì¡°íšŒ
        sql_info = """
            SELECT CROSS_ID, DISTRICT, NODE_ID, NODE_NAME, CROSS_TYPE, INT_TYPE,
                APPR_ID, DIRECTION, APPR_NAME
            FROM TOMMS.NODE_DIR_INFO
        """
        info_rows = [tuple(r) for r in cursor.execute(sql_info).fetchall()]
        info_cols = ['CROSS_ID','DISTRICT','NODE_ID','NODE_NAME','CROSS_TYPE','INT_TYPE',
                    'APPR_ID','DIRECTION','APPR_NAME']
        df_info = pd.DataFrame(info_rows, columns=info_cols)
        for col in ["CROSS_ID","APPR_ID","DIRECTION","CROSS_TYPE"]:
            if col in df_info.columns:
                df_info[col] = pd.to_numeric(df_info[col], errors="coerce")

        # âœ… ì—¬ê¸°ì„œ mergeí•˜ì—¬ DISTRICT, CROSS_IDë¥¼ ë¶€ì—¬
        df = df.merge(df_info[['NODE_ID','CROSS_ID','DISTRICT']], on='NODE_ID', how='left')

        # ë©”íƒ€ í”„ë ˆì„(ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        df_node_meta = df_info.drop_duplicates(subset=['NODE_ID'])[['NODE_ID','NODE_NAME','CROSS_TYPE','INT_TYPE']].set_index('NODE_ID')
        df_appr_meta = df_info[['NODE_ID','APPR_ID','DIRECTION','APPR_NAME']].dropna()
        if 'APPR_ID' in df_appr_meta.columns:
            df_appr_meta['APPR_ID'] = pd.to_numeric(df_appr_meta['APPR_ID'], errors="coerce")

        # -------------------------------------------- 4) ì¼ì¼ ì´ êµí†µëŸ‰ ë§µ
        
        unique_cross_ids = sorted({int(c) for c in df['CROSS_ID'].dropna().astype(int).tolist()})
        daily_volume_map = {}
        if unique_cross_ids:
            placeholders = ",".join(["?"] * len(unique_cross_ids))
            query = f"""
                SELECT CROSS_ID, VOL
                FROM TOMMS.STAT_DAY_CROSS
                WHERE STAT_DAY = ?
                AND INFRA_TYPE = 'SMT'
                AND CROSS_ID IN ({placeholders})
            """
            params = [latest_date] + unique_cross_ids
            cursor.execute(query, params)
            for cross_id_val, vol_val in cursor.fetchall():
                c = int(cross_id_val)
                v = 0
                if vol_val is not None:
                    try:
                        v = int(vol_val)
                    except Exception:
                        try:
                            v = int(float(vol_val))
                        except Exception:
                            v = 0
                daily_volume_map[c] = v
        
        # -------------------------------------------- 5) ìµœì‹  ì‹ í˜¸ì£¼ê¸° ë§¤í•‘

        signal_cycle_map = {}

        # dfì—ëŠ” ì´ë¯¸ NODE_DIR_RESULT + NODE_DIR_INFO mergeë¡œ CROSS_IDê°€ ìˆìŒ
        try:
            unique_cross_ids = sorted({int(c) for c in df['CROSS_ID'].dropna().astype(int).tolist()})
        except Exception:
            unique_cross_ids = []

        if unique_cross_ids:
            placeholders = ",".join(["?"] * len(unique_cross_ids))
            # ROW_NUMBER()ë¡œ cross_idë§ˆë‹¤ ìµœì‹ (INT_CREDATE DESC) 1ê±´ë§Œ ì„ íƒ
            sql_cycle = f"""
                SELECT INT_LCNO, INT_CYCLE
                FROM (
                    SELECT INT_LCNO, INT_CYCLE, INT_CREDATE,
                        ROW_NUMBER() OVER (PARTITION BY INT_LCNO ORDER BY INT_CREDATE DESC) AS RN
                    FROM ITS_SCS_L_OPER
                    WHERE INT_LCNO IN ({placeholders})
                ) T
                WHERE T.RN = 1
            """
            params = unique_cross_ids
            cursor.execute(sql_cycle, params)
            for lcno, cycle in cursor.fetchall():
                # í‚¤: cross_id(INT_LCNO), ê°’: ì‹ í˜¸ì£¼ê¸°(INT_CYCLE), ì•ˆì „ ë³€í™˜
                try:
                    key = int(lcno)
                except Exception:
                    continue
                val = None
                if cycle is not None:
                    try:
                        val = int(cycle)
                    except Exception:
                        try:
                            val = int(float(cycle))
                        except Exception:
                            val = None
                if val is not None and val > 0:
                    signal_cycle_map[key] = val

        # -------------------------------------------- 6) ê²°ê³¼ ê°€ê³µ
        
        nodes = []

        for node_id, df_node in df.groupby('NODE_ID'):
            if node_id not in df_node_meta.index:
                continue

            node_meta = df_node_meta.loc[node_id]
            node_name = node_meta['NODE_NAME']
            cross_id  = df_node['CROSS_ID'].dropna().iloc[0] if not df_node['CROSS_ID'].dropna().empty else None
            sa_no     = df_node['SA_NO'].dropna().iloc[0] if 'SA_NO' in df_node.columns and not df_node['SA_NO'].dropna().empty else None

            # --- ì‹œê°„ëŒ€ ì „ì²´ ì ‘ê·¼ë¡œ ìš”ì•½ (hourly) ---
            hourly_items = []
            all_vehs_total, all_delay_sum, all_delay_count = 0, 0.0, 0

            for appr_id, df_appr in df_node.groupby('APPR_ID'):
                vehs = int(df_appr['VEHS'].sum(skipna=True) or 0)
                delay_vals = df_appr['DELAY'].dropna().astype(float).tolist()
                delay_avg = round(sum(delay_vals) / len(delay_vals), 1) if delay_vals else 0.0
                los = get_los(delay_avg)

                all_vehs_total += vehs
                all_delay_sum  += sum(delay_vals)
                all_delay_count += len(delay_vals)

                match = df_appr_meta[(df_appr_meta['NODE_ID'] == node_id) & (df_appr_meta['APPR_ID'] == appr_id)]
                appr_name = match.iloc[0]['APPR_NAME'] if not match.empty else "ë¯¸ì§€ì •"

                hourly_items.append({
                    "appr_name": appr_name,
                    "vehs": vehs,
                    "delay": delay_avg,
                    "los": los
                })

            total_delay = round(all_delay_sum / all_delay_count, 1) if all_delay_count > 0 else 0.0
            total_los = get_los(total_delay)

            # [ë³€ê²½] daily_total_vehs í• ë‹¹
            
            daily_total_val = 0
            if pd.notna(cross_id):
                try:
                    daily_total_val = daily_volume_map.get(int(cross_id), 0)
                except Exception:
                    daily_total_val = 0
            
            # -------------------------------------------- ìµœì‹  ì‹ í˜¸ì£¼ê¸° ë°˜ì˜
            
            signal_cycle_val = 150
            if pd.notna(cross_id):
                try:
                    signal_cycle_val = signal_cycle_map.get(int(cross_id), 150)
                except Exception:
                    signal_cycle_val = 150

            result_obj = {
                "node_name": node_name,
                "cross_id": int(cross_id) if pd.notna(cross_id) else None,
                "sa_no": sa_no,
                "cross_type": int(node_meta['CROSS_TYPE']) if pd.notna(node_meta['CROSS_TYPE']) else None,
                "int_type": node_meta['INT_TYPE'],
                "daily_total_vehs": daily_total_val,
                "total_vehs": all_vehs_total,
                "total_delay": total_delay,
                "total_los": total_los,
                "signal_circle": signal_cycle_val,
                "hourly": hourly_items,
                "time_slices": []
            }

            # -------------------------------------------- êµ¬ê°„ë³„ time_slices(4ê°œ ê³ ì •)
            for slice_label in ORDERED_SLICES:
                df_time = df_node[df_node['TIMEINT'] == slice_label].copy()

                # -------------------------------------------- (1) ì›ì‹œ items: APPR_ID Ã— DIRECTION
                
                items = []
                if not df_time.empty:
                    for (appr_id, direction), df_pair in df_time.groupby(['APPR_ID', 'DIRECTION']):
                        match = df_appr_meta[
                            (df_appr_meta['NODE_ID'] == node_id) &
                            (df_appr_meta['APPR_ID'] == appr_id)
                        ]
                        appr_name = match.iloc[0]['APPR_NAME'] if not match.empty else "ë¯¸ì§€ì •"

                        vehs = int(df_pair['VEHS'].sum(skipna=True) or 0)
                        delay_vals = df_pair['DELAY'].dropna().astype(float).tolist()
                        delay_avg = round(sum(delay_vals) / len(delay_vals), 1) if delay_vals else 0.0
                        los = get_los(delay_avg)

                        items.append({
                            "appr_id": int(appr_id) if pd.notna(appr_id) else None,
                            "appr_name": appr_name,
                            "direction": int(direction) if pd.notna(direction) else None,
                            "vehs": vehs,
                            "delay": delay_avg,
                            "los": los
                        })

                # -------------------------------------------- (2) ì ‘ê·¼ë¡œ í•©ì„±
                
                appr_summary = []
                if not df_time.empty:
                    for appr_id, df_ap in df_time.groupby('APPR_ID'):
                        match = df_appr_meta[
                            (df_appr_meta['NODE_ID'] == node_id) &
                            (df_appr_meta['APPR_ID'] == appr_id)
                        ]
                        appr_name = match.iloc[0]['APPR_NAME'] if not match.empty else "ë¯¸ì§€ì •"

                        vehs_sum = int(df_ap['VEHS'].sum(skipna=True) or 0)
                        dvals = df_ap['DELAY'].dropna().astype(float).tolist()
                        delay_avg3 = round(sum(dvals) / len(dvals), 1) if dvals else 0.0
                        los3 = get_los(delay_avg3)

                        appr_summary.append({
                            "appr_id": int(appr_id) if pd.notna(appr_id) else None,
                            "appr_name": appr_name,
                            "vehs_sum": vehs_sum,
                            "delay_avg": delay_avg3,
                            "los": los3
                        })

                # -------------------------------------------- (3) êµ¬ê°„ ì´ê´„
                
                if not df_time.empty:
                    total_vehs_slice = int(df_time['VEHS'].sum(skipna=True) or 0)
                    all_d = df_time['DELAY'].dropna().astype(float).tolist()
                    avg_delay_slice = round(sum(all_d) / len(all_d), 1) if all_d else 0.0
                    los_slice = get_los(avg_delay_slice)
                else:
                    total_vehs_slice = 0
                    avg_delay_slice = 0.0
                    los_slice = get_los(avg_delay_slice)

                result_obj["time_slices"].append({
                    "timeint": slice_label,
                    "items": items,
                    "appr_summary": appr_summary,
                    "slice_summary": {
                        "total_vehs": total_vehs_slice,
                        "avg_delay": avg_delay_slice,
                        "los": los_slice
                    }
                })

            nodes.append(result_obj)

        if not nodes:
            return jsonify({
                "status": "fail",
                "message": f"{label}ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
                "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }), 404

        # -------------------------------------------- ìµœì¢… payload
        
        body = json.dumps({
            "status": "success",
            "label": label,
            "target_date": latest_date,
            "data": nodes
        }, ensure_ascii=False)
        etag = f'"{hashlib.md5(body.encode("utf-8")).hexdigest()}"'
        now_kst = datetime.now(KST)
        next_update = now_kst.replace(hour=6, minute=0, second=0, microsecond=0)
        if now_kst >= next_update:
            next_update += timedelta(days=1)
        x_next_update = next_update.isoformat()

        inm_raw = request.headers.get("If-None-Match", "")
        inm = inm_raw.strip().strip('"').replace("W/","").strip()
        if inm == etag.strip('"'):
            resp = Response(status=304)
            resp.headers["ETag"] = etag
            resp.headers["X-Dataset-Date"] = latest_date
            resp.headers["X-Next-Update"] = x_next_update
            resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
            resp.headers["Pragma"] = "no-cache"
            resp.headers["Expires"] = "0"
            return resp

        resp = Response(body, content_type="application/json; charset=utf-8", status=200)
        resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
        resp.headers["Pragma"] = "no-cache"
        resp.headers["Expires"] = "0"
        resp.headers["ETag"] = etag
        resp.headers["X-Dataset-Date"] = latest_date
        resp.headers["X-Next-Update"] = x_next_update
        return resp

    # -------------------------------------------- ì¢…ë£Œ

    except Exception as e:
        # í´ë¼ì´ì–¸íŠ¸ì—ëŠ” ê³ ì • ë©”ì‹œì§€ + ê°„ë‹¨í•œ ì—ëŸ¬ ë¬¸ìì—´ë§Œ
        return jsonify({
            "status": "fail",
            "message": "ë…¸ë“œ ì ‘ê·¼ ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
            "error": str(e)
        }), 500
    finally:
        try:
            if conn:
                conn.close()
        except:
            pass









# ========================================================= [ êµí†µê´€ë¦¬ 1 - êµí†µëŸ‰ íŒ¨í„´ë¹„êµ ë¶„ì„ì •ë³´ ]

@app.route('/management/compare-traffic-vol', methods=['GET'])
def compare_traffic_vol():
    pass

# ========================================================= [ êµí†µê´€ë¦¬ 2 - Deep Learning Progress Overview ]

@app.route('/management/deep-learning-overview', methods=['GET'])
def deep_learning_overview():
    pass

# ========================================================= [ êµí†µê´€ë¦¬ 3 - SA(Sub Area) ê·¸ë£¹ ê´€ë¦¬ì •ë³´ ]

@app.route('/management/sa-group-info', methods=['GET'])
def congested_info():
    pass

def sa_info():
    pass

# ========================================================= [ êµí†µê´€ë¦¬ 4 - í˜¼ì¡êµì°¨ë¡œ ì‹ í˜¸ìµœì í™” íš¨ê³¼ê²€ì¦ ]

@app.route('/management/signal-optimize', methods=['GET'])
def cross_optimize():
    pass









# ========================================================= [ ì„œë²„ì‹¤í–‰ ]

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5101, debug=False)